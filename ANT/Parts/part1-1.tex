\chapter{Preliminaries}\label{ch:Preliminaries}
  A good selection of topics that need some discussion before studying number theory are the following:
  \begin{itemize}
    \item Asymptotics,
    \item Arithmetic Functions
    \item Dirichlet Characters,
    \item Exponential Sums,
    \item Integral Lattices,
    \item Summation Formulas,
    \item Integration Techniques and Transforms,
    \item The Gamma Function.
  \end{itemize}
  In the interest of keeping this text almost completely self-contained, this chapter is dedicated to the basics of these topics as they are essential tools that we will require in our investigations. The concepts presented in this chapter are tools in a toolbox rather than pure number theory. In order to improve the readability of the remainder of the text we will use the results presented here without reference unless it is a matter of clarity. As for standard knowledge, we assume familiarity with basic number theory, complex analysis, real analysis, functional analysis, topology, and algebra. We have also outsourced specific subtopics to the appendix and we will reference them when necessary.
  \section{Notational Conventions}
    Here we make some notational conventions that will be used throughout the rest of the text unless specified otherwise:
    \begin{itemize}
      \item The symbols $\subset$ and $\supset$ denote strict containment.
      \item Any ring is understood to be a commutative ring with $1$ and a subring is understood to contain $1$.
      \item The finite field with $p$ elements $\F_{p}$ stands for $\Z/p\Z$.
      \item Primes and divisors of integers are taken to be positive.
      \item For any $x \in \R$, $\lfloor x \rfloor$, $\lceil x \rceil$, and $\{x\}$ denotes the floor, ceiling, and fractional part of $x$ respectively.
      \item The symbol $\e$ denotes a small positive constant ($\e > 0$) that is not necessarily the same from line to line.
      \item If $a \in (\Z/m\Z)^{\ast}$, we will always let $\conj{a}$ denote the multiplicative inverse. That is, $a\conj{a} \equiv 1 \tmod{m}$.
      \item By analytic we mean real analytic or complex analytic accordingly.
      \item For the complex variables $z$, $s$, and $u$, we write
      \[
        z = x+iy, \quad s = \s+it, \quad \text{and} \quad u = \tau+ir,
      \]
      for the real and imaginary parts of these variables respectively unless specified otherwise. Moreover, in certain expressions we often write $\Im(z)$ for clarity.
      \item For $\mathbf{z} \in \C^{n}$, we write
      \[
        \mathbf{z} = \mathbf{x}+i\mathbf{y},
      \]
      for the real and imaginary parts of $\mathbf{z}$ respectively unless specified otherwise. Moreover,
      \[
        \mathbf{z}^{-1} = (z_{1}^{-1},\ldots,z_{n}^{-1}) = \frac{1}{\mathbf{z}},
      \]
      provided $z_{i} \neq 0$ for $1 \le i \le n$. Also,
      \[
        \a\mathbf{z} = (\a z_{1},\ldots,\a z_{n}) \quad \text{and} \quad \mathbf{z}^{\a} = (z_{1}^{\a},\ldots,z_{n}^{\a}),
      \]
      for all $\a,\b \in \C$. Lastly,
      \[
        \mathbf{z}^{\mathbf{w}} = z_{1}^{w_{1}} \cdots z_{n}^{w_{n}}, \quad \text{and} \quad \mathbf{z}\mathbf{w} = (z_{1}w_{1},\ldots,z_{n}w_{n}),
      \]
      for all $\mathbf{w} = (w_{1},\ldots,w_{n}) \in \C^{n}$.
      \item If $r \in \Z$ denotes the order of a possible pole of a complex function, $r \ge 0$ if it is a pole and $r \le 0$ if it is a zero.
      \item The nontrivial zeros of an $L$-function will be denoted by $\rho = \b+i\g$ unless specified otherwise.
      \item We will always take the principal branch of the logarithm.
      \item For a sum $\sum$ over integers satisfying a congruence condition, $\sum^{'}$ will denote the sum restricted to relatively prime integers satisfying the same congruence.
      \item All integrals are taken with positive orientation.
      \item We will write $\int_{(a)}$ for the complex integral over the line whose real part is $a$ and with positive orientation.
      \item $\d_{a,b}$ will denote the indicator function for $a = b$. That is, $\d_{a,b} = 1,0$ according to if $a = b$ or not.
    \end{itemize}
  \section{Asymptotics}
    Throughout we assume $f,g:\R^{n} \to \C$. Much of the language of number theory is given in terms of asymptotics (or estimates or bounds) as they allows us to discuss approximate growth and dispense with superfluous constants. For this reason, asymptotics will be the first material that we will present. The asymptotics that we will cover are listed in the following table:
    \begin{center}
      \begin{stabular}[1.5]{|c|c|c|}
        \hline
        Asymptotics & Notation \\
        \hline
        Big O & $f(\mathbf{x}) = O(g(\mathbf{x}))$ \\
        \hline
        Vinogradov's symbol & $f(\mathbf{x}) \ll g(\mathbf{x})$ \\
        \hline
        Order of magnitude symbol & $f(\mathbf{x}) \asymp g(\mathbf{x})$ \\
        \hline
        Little o & $f(\mathbf{x}) = o(g(\mathbf{x}))$ \\
        \hline
        Asymptotic equivalence & $f(\mathbf{x}) \sim g(\mathbf{x})$ \\
        \hline
        Omega symbol & $f(\mathbf{x}) = \W(g(\mathbf{x}))$ \\
        \hline
      \end{stabular}
    \end{center}
    Implicit in all of these asymptotics is some limiting process $\mathbf{x} \to \mathbf{x}_{0}$ where $\mathbf{x}_{0}$ is finite or $\infty$. If $\mathbf{x}_{0}$ is finite then it is understood that the asymptotic is assumed to hold for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. If $\mathbf{x}_{0}$ is infinite then the asymptotic is assumed to hold for sufficiently large $\mathbf{x}$. If the limiting process is not explicitly mentioned, it is assumed to be as $\mathbf{x} \to \infty$. Often times, asymptotics will hold for all admissible values of $\mathbf{x}$ and this will be clear from context although we still might suppress the specific limiting process.

    \begin{remark}
      If $f,g: \C^{n} \to \C$ then the following theory still holds by identifying $\C^{n} \cong \R^{2n}$. Moreover, if $f,g:\Z_{+}^{n} \to \C$ then by extending $f(\mathbf{n})$ and $g(\mathbf{n})$ to $\R^{n}$ by making them piecewise linear, so that they are piecewise continuous, the following theory still holds with $\mathbf{n}$ in place of $\mathbf{x}$. In particular, we may take $f(\mathbf{x})$ or $g(\mathbf{x})$ to be a constant function.
    \end{remark}

    Implicit in some asymptotics will be a constant (such constants are in general not unique and any sufficiently large constant will do). Any such constant is called the \textbf{implicit constant}\index{implicit constant} of the asymptotic. The implicit constant may depend on one or more parameters, $\e$, $\s$, etc. If we wish to make these dependencies known, we use subscripts. If it is possible to choose the implicit constant independent of a certain parameter then we say that the asymptotic is \textbf{uniform}\index{uniform} with respect to that parameter. Moreover, we say that an implicit constant is \textbf{effective}\index{effective} if the constant is numerically computable and \textbf{ineffective}\index{ineffective} otherwise. Moreover, if we are interested in the dependence of an asymptotic on a certain parameter, say $p$, we will refer to the \textbf{$p$-aspect}\index{$p$-aspect} to mean the part of the asymptotic that is dependent upon $p$.
    \subsection*{\texorpdfstring{$O$}{O}-estimates and Symbols}
      We say $f(\mathbf{x})$ \textbf{is of order}\index{is of order} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = O(g(\mathbf{x}))$
      if there is some positive constant $c$ such that
      \[
        |f(\mathbf{x})| \le c|g(\mathbf{x})|,
      \]
      as $\mathbf{x} \to \mathbf{x}_{0}$. We call this an \textbf{$O$-estimate}\index{$O$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth at most}\index{growth at most} $g(\mathbf{x})$. The $O$-estimate says that for $\mathbf{x}$ close to $\mathbf{x}_{0}$, the size of $f(\mathbf{x})$ grows like $g(\mathbf{x})$.
      
      \begin{remark}
        Many authors assume that $g(\mathbf{x})$ is a nonnegative function so that the absolute value on $g(\mathbf{x})$ can be dropped. As we require asymptotics that will be used more generally, we do not make this assumption since one could very well replace $O(g(\mathbf{x}))$ with $O(|g(\mathbf{x})|)$. In practice this deviation causes no issue.
      \end{remark}
      
      As a symbol, let $O(g(\mathbf{x}))$ stand for a function $f(\mathbf{x})$ that is $O(g(\mathbf{x}))$. Then we may use the $O$-estimates in algebraic equations and inequalities. Note that this extends the definition of the symbol because $f(\mathbf{x}) = O(g(\mathbf{x}))$ means $f(\mathbf{x})$ is $O(g(\mathbf{x}))$. The symbol $\ll$ is known as \textbf{Vinogradov's symbol}\index{Vinogradov's symbol} and it is an alternative way to express $O$-estimates. We write $f(\mathbf{x}) \ll g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ if $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$. We also write $f(\mathbf{x}) \gg g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ to mean $g(\mathbf{x}) \ll f(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$. If there is a dependence of the implicit constant on parameters, we use subscripts to denote dependence on these parameters. If both $f(\mathbf{x}) \ll g(\mathbf{x})$ and $g(\mathbf{x}) \ll f(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ then we say $f(\mathbf{x})$ and $g(\mathbf{x})$ have the \textbf{same order of magnitude}\index{same order of magnitude} and write $f(\mathbf{x}) \asymp g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$. We also say $f(\mathbf{x})$ has \textbf{growth}\index{growth} $g(\mathbf{x})$. If there is a dependence of the implicit constant on parameters, we use subscripts to denote dependence on these parameters. From the definition of the $O$-estimate, this is equivalent to the existence of positive constants $c_{1}$ and $c_{2}$ such that
      \[
        c_{1}|g(\mathbf{x})| \le |f(\mathbf{x})| \le c_{2}|g(\mathbf{x})|.
      \]
      Equivalently, we can interchange $f(\mathbf{x})$ and $g(\mathbf{x})$ in the above equation.
    \subsection*{\texorpdfstring{$o$}{o}-estimates and Symbols}
      We say $f(\mathbf{x})$ \textbf{is of smaller order than}\index{is of smaller order than} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = o(g(\mathbf{x}))$ if
      \[
        \lim_{\mathbf{x} \to \mathbf{x}_{0}}\left|\frac{f(\mathbf{x})}{g(\mathbf{x})}\right| = 0,
      \]
      provided $g(\mathbf{x}) \neq 0$ for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{$o$-estimate}\index{$o$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth less than}\index{growth less than} $g(\mathbf{x})$. The $o$-estimate says that for $\mathbf{x}$ close to $\mathbf{x}_{0}$, $g(\mathbf{x})$ dominates $f(\mathbf{x})$. If $f(\mathbf{x}) = o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ where the implicit constant can be taken arbitrarily small by definition of the $o$-estimate. Therefore, $o$-estimates are stronger than $O$-estimates. As a symbol, let $o(g(\mathbf{x}))$ stand for a function $f(\mathbf{x})$ that is $o(g(\mathbf{x}))$. Then we may use the $o$-estimates in algebraic equations and inequalities. Note that this extends the definition of the symbol because $f(\mathbf{x}) = o(g(\mathbf{x}))$ means $f(\mathbf{x})$ is $o(g(\mathbf{x}))$. Moreover, a \textbf{asymptotic formula}\index{asymptotic formula} for $f(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ is an equation of shape
      \[
        f(\mathbf{x}) = M(\mathbf{x})+O(E(\mathbf{x})),
      \]
      as $\mathbf{x} \to \mathbf{x}_{0}$ such that $E(\mathbf{x}) = o(M(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ too. Accordingly, we call $M(\mathbf{x})$ the \textbf{main term}\index{main term} and $E(\mathbf{x})$ the \textbf{error term}\index{error term} respectively.

      \begin{remark}
        The asymptotic formula
        \[
          f(\mathbf{x}) = M(\mathbf{x})+O(E(\mathbf{x})),
        \]
        as $\mathbf{x} \to \mathbf{x}_{0}$ is equivalent to
        \[
          f(\mathbf{x})-M(\mathbf{x}) = O(E(\mathbf{x})),
        \]
        as $\mathbf{x} \to \mathbf{x}_{0}$ which shows that the absolute error between $f(\mathbf{x})$ and $M(\mathbf{x})$ is $O(E(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$.
      \end{remark}
      
      We say $f(\mathbf{x})$ \textbf{is asymptotic to}\index{is asymptotic to} $g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) \sim g(\mathbf{x})$ if
      \[
        \lim_{\mathbf{x} \to \mathbf{x}_{0}}\frac{f(\mathbf{x})}{g(\mathbf{x})} = 1,
      \]
      provided $g(\mathbf{x}) \neq 0$ for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{asymptotic equivalence}\index{asymptotic equivalence} and say that $f(\mathbf{x})$ and $g(\mathbf{x})$ are \textbf{asymptotically equivalent}\index{asymptotically equivalent}. It is useful to think of asymptotic equivalence as $f(\mathbf{x})$ and $g(\mathbf{x})$ being the same size in the limit as $\mathbf{x} \to \mathbf{x}_{0}$. Immediately from the definition, we see that this is an equivalence relation on functions. In particular, if $f(\mathbf{x}) \sim g(\mathbf{x})$ and $g(\mathbf{x}) \sim h(\mathbf{x})$ then $f(\mathbf{x}) \sim h(\mathbf{x})$. Also, if $f(\mathbf{x}) \sim g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $f(\mathbf{x}) \asymp g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ with $c_{1} \le 1 \le c_{2}$. So asymptotic equivalence is stronger than being of the same order of magnitude. In addition, if we have an asymptotic formula for $f(\mathbf{x})$ with main term $M(\mathbf{x})$ and $M(\mathbf{x}) \neq 0$ for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm then $f(\mathbf{x}) \sim M(\mathbf{x})$. Therefore an asymptotic formula is stronger than asymptotic equality. Also note that $f(\mathbf{x}) \sim g(\mathbf{x})$ is equivalent to $f(\mathbf{x}) = g(\mathbf{x})(1+o(1))$ and hence implies $f(\mathbf{x}) = g(\mathbf{x})(1+O(1))$. We say $f(\mathbf{x})$ \textbf{is of larger order than}\index{is of larger order than} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $\W(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = \W(g(\mathbf{x}))$ if
      \[
        \limsup_{\mathbf{x} \to \mathbf{x}_{0}}\left|\frac{f(\mathbf{x})}{g(\mathbf{x})}\right| > 0.
      \]
      provided $g(\mathbf{x}) \neq 0$ for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{$\W$-estimate}\index{$\W$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth at least}\index{growth at least} $g(\mathbf{x})$. Observe that $f(\mathbf{x}) = \W(g(\mathbf{x}))$ is precisely the negation of $f(\mathbf{x}) = o(g(\mathbf{x}))$, so that $f(\mathbf{x}) = \W(g(\mathbf{x}))$ means $f(\mathbf{x}) = o(g(\mathbf{x}))$ is false. This is weaker than $f(\mathbf{x}) \gg g(\mathbf{x})$ because $f(\mathbf{x}) = \W(g(\mathbf{x}))$ means $|f(\mathbf{x})| \ge c|g(\mathbf{x})|$ for some values of $\mathbf{x}$ arbitrarily close to $\mathbf{x}_{0}$ whereas $f(\mathbf{x}) \gg g(\mathbf{x})$ means $|f(\mathbf{x})| \ge c|g(\mathbf{x})|$ for all values of $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm.
    \subsection*{Algebraic Manipulation for \texorpdfstring{$O$}{O}-estimates and \texorpdfstring{$o$}{o}-estimates}
      Asymptotics become increasingly more useful when we can use them in equations to represent approximations. We catalogue some of the most useful algebraic manipulations for $O$-estimates and $o$-estimates. Most importantly, if an algebraic equation involves a $O$-estimate or $o$-estimate then it is understood that the equation is not symmetric and is interpreted to be read from left to right. That is, any function of the form satisfying the estimate on the left-hand side also satisfies the estimate on the right-hand side too. We begin with $O$-estimates. The trivial algebraic manipulations are collected in the proposition below:

      \begin{proposition}\label{prop:Big_Oh_manipulations}
          The following $O$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
          \begin{enumerate}[label*=(\roman*)]
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ and $g(\mathbf{x}) = O(h(\mathbf{x}))$ then $f(\mathbf{x}) = O(h(\mathbf{x}))$. Equivalently, $O(O(h(\mathbf{x}))) = O(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = O(g_{i}(\mathbf{x}))$ for $i = 1,2$ then $f_{1}(\mathbf{x})f_{2}(\mathbf{x}) = O(g_{1}(\mathbf{x})g_{2}(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x})h(\mathbf{x}))$ then $f(\mathbf{x}) = g(\mathbf{x})O(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = O(g_{i}(\mathbf{x}))$ for $i = 1,2,\ldots,n$ then $\sum_{1 \le i \le n}f_{i}(\mathbf{x}) = O\left(\sum_{1 \le i \le n}|g_{i}(\mathbf{x})|\right)$.
            \item If $f_{n}(\mathbf{x}) = O(g_{n}(\mathbf{x}))$ for $n \ge 1$ then $\sum_{n \ge 1}f_{n}(\mathbf{x}) = O\left(\sum_{n \ge 1}|g_{n}(\mathbf{x})|\right)$ provided both $\sum_{n \ge 1}f_{n}(\mathbf{x})$ and $\sum_{n \ge 1}|g_{n}(\mathbf{x})|$ converge.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and $T(\mathbf{x})$ is such that $T(\mathbf{x}) \to \mathbf{x}_{0}$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $(f \circ T)(\mathbf{x}) = O((g \circ T)(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ then $\Re(f(\mathbf{x})) = O(g(\mathbf{x}))$ and $\Im(f(\mathbf{x})) = O(g(\mathbf{x}))$.
          \end{enumerate}
      \end{proposition}
      \begin{proof}
        Statements (i)-(iii) and (vi) follow immediately from the definition of the $O$-estimate. Statement (iv) follows from the definition and the triangle inequality. Statement (v) follows in the same way as (iv) given that both sums converge. Statement (vii) follows from the definition the $O$-estimate and the bounds $|x| \le |z|$ and $|y| \le |z|$ for any complex $z$.
      \end{proof}

      The most common application of \cref{prop:Big_Oh_manipulations} (vi) will be in the single variable case when $z \asymp w$ or $|z| \sim |w|$ (the latter case implying the former) where $w$ is a function of $z$ (usually one that is more simple than $x$ itself). Taking $h(z) = w$, \cref{prop:Big_Oh_manipulations} (vi) says that if $f(z) = O(g(z))$ then $f(w) = O(g(w))$. In terms of Vinogradov's symbol, $f(z) \ll g(z)$ implies $f(w) \ll g(w)$. $O$-estimates also behave well with respect to integrals provided the functions are continuous and we are integrating over a compact region:

      \begin{proposition}
        Suppose $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \infty$ and $f(\mathbf{x})$ and $g(\mathbf{x})$ are continuous on a compact region $D$ where this estimate holds. Then
        \[
          \int_{D}f(\mathbf{x})\,d\mathbf{x} = O\left(\int_{D}|g(\mathbf{x})|\,d\mathbf{x}\right).
        \]
      \end{proposition}
      \begin{proof}
        This follows immediately from the definition of the $O$-estimate and that continuous functions are bounded on compact regions.
      \end{proof}

      The next proposition is a collection of some useful expressions for simplifying equations involving $O$-estimates:

      \begin{proposition}
        Let $f(\mathbf{x})$ be a function such that $f(\mathbf{x}) \to 0$ as $\mathbf{x} \to \mathbf{x}_{0}$. The following $O$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
        \begin{enumerate}[label*=(\roman*)]
          \item $\frac{1}{1+O(f(\mathbf{x}))} = 1+O(f(\mathbf{x}))$.
          \item $(1+O(f(\mathbf{x})))^{w} = 1+O(f(\mathbf{x}))$ for any complex $w$.
          \item $\log(1+O(f(\mathbf{x}))) = O(f(\mathbf{x}))$.
          \item $e^{1+O(f(\mathbf{x}))} = 1+O(f(\mathbf{x}))$.
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        Taking the Taylor series truncated after the first term and applying Taylor's theorem gives the following $O$-estimates as $z \to 0$:
        \begin{enumerate}[label*=(\roman*)]
          \item $\frac{1}{1+z} = 1+O(z)$.
          \item $(1+z)^{z} = 1+O(z)$.
          \item $\log(1+z) = O(z)$.
          \item $e^{z} = 1+O(z)$.
        \end{enumerate}
        Now apply \cref{prop:Big_Oh_manipulations} (v) to each of these $O$-estimates, and use \cref{prop:Big_Oh_manipulations} (i).
      \end{proof}

      For $o$-estimates, the following properties are useful:

      \begin{proposition}\label{prop:Little_Oh_manipulations}
          The following $o$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
          \begin{enumerate}[label*=(\roman*)]
            \item If $f(\mathbf{x}) = o(g(\mathbf{x}))$ and $g(\mathbf{x}) = o(h(\mathbf{x}))$ then $f(\mathbf{x}) = o(h(\mathbf{x}))$. Equivalently, $o(o(h(\mathbf{x}))) = o(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = o(g_{i}(\mathbf{x}))$ for $i = 1,2$ then $f_{1}(\mathbf{x})f_{2}(\mathbf{x}) = o(g_{1}(\mathbf{x})g_{2}(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = o(g(\mathbf{x})h(\mathbf{x}))$ then $f(\mathbf{x}) = g(\mathbf{x})o(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = o(g_{i}(\mathbf{x}))$ for $i = 1,2,\ldots,n$ then $\sum_{1 \le i \le n}f_{i}(\mathbf{x}) = o\left(\sum_{1 \le i \le n}|g_{i}(\mathbf{x})|\right)$.
            \item If $f(\mathbf{x}) = o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and $h(\mathbf{x})$ is such that $h(\mathbf{x}) \to \mathbf{x}_{0}$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $(f \circ h)(\mathbf{x}) = o((g \circ h)(\mathbf{x}))$.
          \end{enumerate}
      \end{proposition}
      \begin{proof}
        Statements (i)-(iii) and (v) follow immediately from the definition of the $o$-estimate. Statement (iv) follows from the definition and that $\sum_{1 \le i \le n}|g_{i}(\mathbf{x})| \ge |g_{i}(\mathbf{x})|$.
      \end{proof}
    \subsection*{Growth and Decay of Functions}
      We will also be interested in the growth rate of functions. There are many types of growth rates, but we will only recall the ones that are standard. Throughout let $c \ge 1$. First suppose $\mathbf{x} \to \mathbf{x}_{0}$. If $f(\mathbf{x}) \asymp \log^{c}||\mathbf{x}||_{\infty}$, we say that $f(\mathbf{x})$ is of \textbf{logarithmic growth}\index{logarithmic growth}. If $f(\mathbf{x}) \asymp ||\mathbf{x}||_{\infty}^{c}$, we say that $f(\mathbf{x})$ is of \textbf{polynomial growth}\index{polynomial growth}. If $f(\mathbf{x}) \asymp e^{c||\mathbf{x}||_{\infty}}$, we say that $f(\mathbf{x})$ is of \textbf{exponential growth}\index{exponential growth}. Now suppose $\mathbf{x} \to \infty$. If $f(\mathbf{x}) \asymp \log^{-c}||\mathbf{x}||_{\infty}$, we say that $f(\mathbf{x})$ is of \textbf{logarithmic decay}\index{logarithmic decay}. If $f(\mathbf{x}) \asymp ||\mathbf{x}||_{\infty}^{-c}$ for some $c \ge 1$, we say that $f(\mathbf{x})$ is of \textbf{polynomial decay}\index{polynomial decay}. If $f(\mathbf{x}) \asymp e^{-c||\mathbf{x}||_{\infty}}$, we say that $f(\mathbf{x})$ is of \textbf{exponential decay}\index{exponential decay}. In all of these cases, we refer to the constant $c$ as the \textbf{order}\index{order} of growth or decay respectively. If $f(\mathbf{x}) = \W(||\mathbf{x}||_{\infty}^{n})$ for all $n \ge 0$ then we say $f(\mathbf{x})$ is of \textbf{rapid growth}\index{rapid growth}. Alternatively, if $f(\mathbf{x}) = o(||\mathbf{x}||_{\infty}^{-n})$ for all $n \ge 0$ then we say $f(\mathbf{x})$ is of \textbf{rapid decay}\index{rapid decay}.
  \section{Arithmetic Functions}
    An arithmetic function $f$ is a function $f:\N \to \C$. That is, it takes the positive integers into the complex numbers. We say that $f$ is \textbf{additive}\index{additive} if $f(nm) = f(n)+f(m)$ for all positive integers $n$ and $m$ such that $(n,m) = 1$. If this condition simply holds for all $n$ and $m$ then we say $f$ is \textbf{completely additive}\index{completely additive}. Similarly, we say that $f$ is \textbf{multiplicative}\index{multiplicative} if $f(nm) = f(n)f(m)$ for all positive integers $n$ and $m$ such that $(n,m) = 1$. If this condition simply holds for all $n$ and $m$ then we say $f$ is \textbf{completely multiplicative}\index{completely multiplicative}. Many important arithmetic functions are either additive, completely additive, multiplicative, or completely multiplicative. Note that if a $f$ is additive or multiplicative then $f$ is uniquely determined by its values on prime powers and if $f$ is completely additive or completely multiplicative then it is uniquely determined by its values on primes. Moreover, if $f$ is additive or completely additive then $f(1) = 0$ and if $f$ is multiplicative or completely multiplicative then $f(1) = 1$. Below is a list defining the most important arithmetic functions (some of these functions are restrictions of common functions but we define them here as arithmetic functions because their domain being $\N$ is important):
    \begin{enumerate}[label*=(\roman*)]
      \item The \textbf{constant function}\index{constant function}: The function $\mathbf{1}(n)$ restricted to all $n \ge 1$. This function is neither additive or multiplicative.
      \item The \textbf{indicator function}\index{indicator function}: The function $\d(n)$ defined by
      \[
        \d(n) = \begin{cases} 1 & \text{if $n = 1$}, \\ 0 & \text{if $n \ge 2$}. \end{cases}
      \]
      This function is completely multiplicative.
      \item The \textbf{identity function}\index{identity function}: The function $\id(n)$ restricted to all $n \ge 1$. This function is completely multiplicative.
      \item The \textbf{logarithm}\index{logarithm function}: The function $\log(n)$ restricted to all $n \ge 1$. This function is completely additive.
      \item The \textbf{M\"obius function}\index{M\"obius function}: The function $\mu(n)$ defined by
      \[
        \mu(n) = \begin{cases} 1 & \text{if $n$ is square-free with an even number of prime factors}, \\ -1 & \text{if $n$ is square-free with an odd number of prime factors}, \\ 0 & \text{if $n$ is not square-free}, \end{cases}
      \]
      for all $n \ge 1$. This function is multiplicative.
      \item The \textbf{characteristic function of square-free integers}\index{characteristic function of square-free integers}: The square of the M\"obius function $\mu^{2}(n)$ for all $n \ge 1$. This function is multiplicative.
      \item \textbf{Liouville's function}\index{Liouville's function}: The function $\l(n)$ defined by
      \[
        \l(n) = \begin{cases} 1 & \text{if $n = 1$}, \\ (-1)^{k} & \text{if $n$ is composed of $k$ not necessarily distinct prime factors}, \end{cases}
      \]
      for all $n \ge 1$. This function is completely multiplicative.
      \item \textbf{Euler's totient function}\index{Euler's totient function}: The function $\vphi(n)$ defined by
      \[
        \vphi(n) = \psum_{m \tmod{n}}1,
      \]
      for all $n \ge 1$. This function is multiplicative.
      \item The \textbf{divisor function}\index{divisor function}: The function $\s_{0}(n)$ defined by
      \[
        \s_{0}(n) = \sum_{d \mid n}1,
      \]
      for all $n \ge 1$. This function is multiplicative.
      \item The \textbf{sum of divisors function}\index{sum of divisors function}: The function $\s_{1}(n)$ defined by
      \[
        \s_{1}(n) = \sum_{d \mid n}d,
      \]
      for all $n \ge 1$. This function is multiplicative.
      \item The \textbf{generalized sum of divisors function}\index{generalized sum of divisors function}: The function $\s_{s}(n)$ defined by
      \[
        \s_{s}(n) = \sum_{d \mid n}d^{s},
      \]
      for all $n \ge 1$ and any $s \in \C$. This function is multiplicative.
      \item The \textbf{number of distinct prime factors function}\index{number of distinct prime factors function}: The function $\w(n)$ defined by
      \[
        \w(n) = \sum_{p \mid n}1,
      \]
      for all $n \ge 1$. This function is additive.
      \item The \textbf{total number of prime divisors function}\index{total number of prime divisors function}: The function $\W(n)$ defined by
      \[
        \W(n) = \sum_{p ^{m} \mid n}1,
      \]
      for all $n \ge 1$ and where $m \ge 1$. This function is completely additive.
      \item The \textbf{von Mangoldt function}\index{von Mangoldt function}: The function $\L(n)$ defined by
      \[
        \L(n) = \begin{cases} 0 & \text{if $n$ is not a prime power}, \\ \log(p) & \text{if $n = p^{m}$ for some prime $p$ and integer $m \ge 1$}, \end{cases}
      \]
      for all $n \ge 1$. This function is neither additive or multiplicative.
    \end{enumerate}
    If $f$ and $g$ are two arithmetic functions then we can define a new arithmetic function $f \ast g$ called the \textbf{Dirichlet convolution}\index{Dirichlet convolution} of $f$ and $g$ defined by
    \[
      (f \ast g)(n) = \sum_{d \mid n}f(d)g\left(\frac{n}{d}\right),
    \]
    for all $n \ge 1$. Note that $f \ast g = g \ast f$. This is especially useful when $f$ and $g$ are multiplicative:

    \begin{proposition}\label{prop:Dirichlet_convolution_of_multiplicative_functions}
      If $f$ and $g$ are multiplicative arithmetic functions then so is their Dirichlet convolution $f \ast g$.
    \end{proposition}
    \begin{proof}
      Let $n,m \ge 1$ be such that $(n,m) = 1$. Then every $d \mid nm$ is of the form $d = d'd''$ with $d' \mid n$, $d'' \mid m$, and $(d',d'') = 1$. Then
      \begin{align*}
        (f \ast g)(nm) &= \sum_{d \mid nm}f(d)g\left(\frac{nm}{d}\right) \\
        &= \sum_{\substack{d' \mid n \\ d'' \mid m \\ (d',d'') = 1}}f(d'd'')g\left(\frac{nm}{d'd''}\right) \\
        &= \sum_{\substack{d' \mid n \\ d'' \mid m \\ (d',d'') = 1}}f(d')f(d'')g\left(\frac{n}{d'}\right)g\left(\frac{m}{d''}\right) \\
        &= \left(\sum_{d' \mid n}f(d')g\left(\frac{n}{d'}\right)\right)\left(\sum_{d'' \mid m}f(d'')g\left(\frac{m}{d''}\right)\right) \\
        &= (f \ast g)(n)(f \ast g)(m).
      \end{align*}
      This completes the proof.
    \end{proof}

    From \cref{prop:Dirichlet_convolution_of_multiplicative_functions} we see that Dirichlet convolution makes the set of multiplicative functions into a semigroup. In fact, it is a monoid since the indicator function $\d$ acts as an identity. Indeed, for any multiplicative function $f$, we have
    \[
      (f \ast \d)(n) = \sum_{d \mid n}f(d)\d\left(\frac{n}{d}\right) = f(n).
    \]
    Therefore
    \[
      f \ast \d = f.
    \]
    A certain case of interest for Dirichlet convolution is when the M\"obius function is convolved with the constant function:

    \begin{proposition}\label{prop:Mobius_indicator}
    We have
    \[
      \sum_{d \mid n}\mu(d) = \sum_{d \mid n}\mu\left(\frac{n}{d}\right) = \begin{cases} 1 & \text{if $n = 1$}, \\ 0 & \text{if $n \ge 2$}. \end{cases}
    \]
    In particular,
    \[
      \mu \ast \mathbf{1} = \mathbf{1} \ast \mu = \d.
    \]
    \end{proposition}
    \begin{proof}
      In terms of Dirichlet convolutions, the second statement is equivalent to the first (actually just $\mu \ast \mathbf{1} = \d$ or $\d = \mathbf{1} \ast \mu$ since Dirichlet convolution is associative). So it suffices to prove the first statement only. By \cref{prop:Dirichlet_convolution_of_multiplicative_functions}, $\sum_{d \mid n}\mu(d)$ is multiplicative so we may assume that $n = p^{r}$ for some prime $p$ and $r \ge 0$. When $r = 0$, $d = 1$ and so the sum is $1$. For $r \ge 1$, $d$ runs over $1,p,\ldots,p^{r}$ and the only $d$ for which $\mu(d) \neq 0$ is $d = 1$ and $d = p$. But $\mu(1)+\mu(p) = 0$ so that the sum is zero in this case. This completes the proof. 
    \end{proof}

    With \cref{prop:Mobius_indicator}, we can prove the infamous \textbf{M\"obius inversion formula}\index{M\"obius inversion formula}:

    \begin{theorem*}[M\"obius inversion formula]
      Suppose $f$ and $g$ are arithmetic functions. Then
      \[
        g(n) = \sum_{d \mid n}f(d),
      \]
      for all $n \ge 1$, if and only if
      \[
        f(n) = \sum_{d \mid n}g(d)\mu\left(\frac{n}{d}\right),
      \]
      for all $n \ge 1$. In particular,
      \[
        g = f \ast \mathbf{1},
      \]
      if and only if
      \[
        f = g \ast \mu.
      \]
    \end{theorem*}
    \begin{proof}
      In terms of Dirichlet convolutions, the second statement is equivalent to the first. So it suffices to prove the second statement.
      Convolving the first formula with $\mu$ gives
      \[
        g \ast \mu = f \ast \mathbf{1} \ast \mu = f \ast \d = f,
      \]
      with the last two equalities following from \cref{prop:Mobius_indicator} and that $\d$ is the identity for Dirichlet convolution. This proves the forward implication. The reverse implication follows by convolving the second formula with $\mathbf{1}$ and arguing analogously. 
    \end{proof}

    Let $f$ be multiplicative. We associate to $f$ the completely multiplicative function $f_{A}$ defined on primes $p$ by
    \[
      f_{A}(p) = f(p^{2})-f(p)^{2}.
    \]
    We say that $f$ is \textbf{specially multiplicative}\index{specially multiplicative} if
    \[
        f(n)f(m) = \sum_{d \mid (n,m)}f_{A}(d)f\left(\frac{nm}{d^{2}}\right),
    \]
    for all $n,m \ge 1$. In fact, this is equivalent to the identity
    \[
        f(nm) = \sum_{d \mid (n,m)}\mu(d)f_{A}(d)f\left(\frac{n}{d}\right)f\left(\frac{m}{d}\right),
    \]
    for all $n,m \ge 1$ as the following proposition shows:

    \begin{proposition}\label{prop:specially_multiplicative_functions}
      Let $f$ be a multiplicative function. Then
      \[
        f(n)f(m) = \sum_{d \mid (n,m)}f_{A}(d)f\left(\frac{nm}{d^{2}}\right),
      \]
      for all $n,m \ge 1$, if and only if
      \[
        f(nm) = \sum_{d \mid (n,m)}\mu(d)f_{A}(d)f\left(\frac{n}{d}\right)f\left(\frac{m}{d}\right),
      \]
      for all $n,m \ge 1$.
    \end{proposition}
    \begin{proof}
      Suppose the first identity holds. Then by \cref{prop:Mobius_indicator}, we have
      \begin{align*}
        f(nm) &= \sum_{d \mid (n,m)}f_{A}(d)f\left(\frac{nm}{d^{2}}\right)\sum_{e \mid d}\mu\left(\frac{d}{e}\right) \\
        &= \sum_{\substack{d \mid (n,m) \\ e \mid d}}\mu\left(\frac{d}{e}\right)f_{A}(d)f\left(\frac{nm}{d^{2}}\right) \\
        &= \sum_{\substack{d \mid (n,m) \\ e \mid \left(\frac{n}{d},\frac{m}{d}\right)}}\mu(d)f_{A}(de)f\left(\frac{nm}{(de)^{2}}\right) && \text{$d \to de$} \\
        &= \sum_{d \mid (n,m)}\mu(d)f_{A}(d)\sum_{e \mid \left(\frac{n}{d},\frac{m}{d}\right)}f_{A}(e)f\left(\frac{\frac{n}{d}\frac{m}{d}}{e^{2}}\right) \\
        &= \sum_{d \mid (n,m)}\mu(d)f_{A}(d)f\left(\frac{n}{d}\right)f\left(\frac{m}{d}\right),
      \end{align*}
      where the last equality holds by the first identity. This proves the forward implication. For the reverse implication, suppose the second identity holds. By \cref{prop:Mobius_indicator} again, we have
      \begin{align*}
        f(n)f(m) &= \sum_{d \mid (n,m)}f_{A}(d)f\left(\frac{n}{d}\right)f\left(\frac{m}{d}\right)\sum_{e \mid d}\mu(e) \\
        &= \sum_{\substack{d \mid (n,m) \\ e \mid d}}\mu(e)f_{A}(d)f\left(\frac{n}{d}\right)f\left(\frac{m}{d}\right) \\
        &= \sum_{\substack{d \mid (n,m) \\ e \mid \left(\frac{n}{d},\frac{m}{d}\right)}}\mu(e)f_{A}(de)f\left(\frac{n}{de}\right)f\left(\frac{m}{de}\right) && \text{$d \to de$} \\
        &= \sum_{d \mid (n,m)}f_{A}(d)\sum_{e \mid \left(\frac{n}{d},\frac{m}{d}\right)}\mu(e)f_{A}(e)f\left(\frac{\frac{n}{d}}{e}\right)f\left(\frac{\frac{m}{d}}{e}\right) \\
        &= \sum_{d \mid (n,m)}f_{A}(d)\sum_{e \mid \left(\frac{n}{d},\frac{m}{d}\right)}f\left(\frac{nm}{d^{2}}\right),
      \end{align*}
      where the last equality holds by the second identity. This proves the reverse implication completing the proof.
    \end{proof}

    Lastly, we generalize $\s_{0}(n)$, $\s_{1}(n)$, and $\s_{s}(n)$ to all nonzero $n \in \Z$ by setting
    \[
      \s_{0}(n) = \s_{0}(|n|), \quad \s_{1}(n) = \s_{1}(|n|), \quad \text{and} \quad \s_{s}(n) = \s_{s}(|n|),
    \]
    for all $s \in \C$. It is very useful to know that $\s_{0}(n)$ grows slowly:

    \begin{proposition}\label{prop:sum_of_divisors_growth_rate}
      \phantom{ }
      \[
        \s_{0}(n) \ll_{\e} n^{\e}.
      \]
    \end{proposition}
    \begin{proof}
      Let $n = p_{1}^{r_{1}} \cdots p_{k}^{r_{k}}$ be the prime factorization of $n$. As $\s_{0}(p^{r_{i}}) = r_{i}+1$ for $1 \le i \le k$, multiplicativity implies $\s_{0}(n) = (r_{1}+1) \cdots (r_{k}+1)$. Then
      \[
        \frac{\s_{0}(n)}{n^{\e}} = \prod_{1 \le i \le r}\frac{r_{i}+1}{p_{i}^{r_{i}\e}}.
      \]
      It suffices to show that the right-hand side is bounded by a positive constant $c(\e)$. For a prime $p$, consider the nonnegative continuous function $f_{p,\e}(x)$ defined by
      \[
        f_{p,\e}(x) = \frac{x+1}{p^{x\e}},
      \]
      for $x \ge -1$. The derivative is given by
      \[
        f_{p,\e}'(x) = \frac{1-\log(p^{\e})(x+1)}{p^{x\e}},
      \]
      which is negative for $1 < \log(p^{\e})(x+1)$ or equivalently $\frac{1}{\log(p^{\e})}-1 < x$. Therefore $f_{p,\e}(x)$ is eventually decreasing and so attains a maximum positive value. In particular, it attains a maximal positive integral value for some $a_{p} \ge 0$ because $f_{p,\e}(-1) = 0$ and $f_{p,\e}(x)$ is positive for $x > -1$. Then the inequalities $f_{p,\e}(a_{p}-1) \le f_{p,\e}(a_{p})$ and $f_{p,\e}(a_{p}+1) \le f_{p,\e}(a_{p})$ are
      \[
        \frac{a_{p}}{p^{(a_{p}-1)\e}} \le \frac{a_{p}+1}{p^{a_{p}\e}} \quad \text{and} \quad \frac{a_{p}+2}{p^{(a_{p}+1)\e}} \le \frac{a_{p}+1}{p^{a_{p}\e}},
      \]
      respectively. Upon isolating $a_{p}$ to obtain an upper bound in the first inequality and a lower bound in the second inequality, we find that
      \[
        \frac{1}{p^{\e}-1}-1 \le a_{p} \le \frac{1}{p^{\e}-1}.
      \]
      Therefore we may take $a_{p} = \left\lfloor \frac{1}{p^{\e}-1}-1 \right\rfloor$. Moreover,
      \[
        \frac{\frac{1}{p^{\e}-1}}{p^{\e\left(\frac{1}{p^{\e}-1}-1\right)}} \le f_{p}(a_{p}) \le \frac{\frac{1}{p^{\e}-1}+1}{p^{\e\left(\frac{1}{p^{\e}-1}\right)}},
      \]
      and taking the logarithm yields
      \[
        \log\left(\frac{1}{p^{\e}-1}\right)-\log(p^{\e})\left(\frac{1}{p^{\e}-1}-1\right) \le \log(f_{p}(a_{p})) \le \log\left(\frac{1}{p^{\e}-1}+1\right)-\log(p^{\e})\left(\frac{1}{p^{\e}-1}\right),
      \]
      which can be further expressed as
      \[
        \log(p^{\e})-\log(p^{\e}-1)-\log(p^{\e})\left(\frac{1}{p^{\e}-1}\right) \le \log(f_{p}(a_{p})) \le \log(p^{\e})-\log(p^{\e}-1)-\log(p^{\e})\left(\frac{1}{p^{\e}-1}\right),
      \]
      Taking the limit as $p \to \infty$ and using L'H\^opital's rule for the $\log(p^{\e})\left(\frac{1}{p^{\e}-1}\right)$ terms, the left-hand and right-hand sides both tend to zero. Therefore $\log(f_{p}(a_{p})) \to 0$ and hence $f_{p}(a_{p}) \to 1$. This guarantees that the infinite product $\prod_{p}f_{p,\e}(a_{p})$ converges to some positive value $c(\e)$. But then
      \[
        \prod_{1 \le i \le r}\frac{r_{i}+1}{p_{i}^{r_{i}\e}} \le c(\e),
      \]
      from which the claim follows.
    \end{proof}
  \section{Dirichlet Characters}
    The most important multiplicative periodic functions for an analytic number theorist are the Dirichlet characters. A \textbf{Dirichlet character}\index{Dirichlet character} $\chi$ modulo $m \ge 1$ is an $m$-periodic homomorphism $\chi:\Z \to \C$ such that $\chi(a) = 0$ if and only if $(a,m) > 1$. Note that any Dirichlet character is necessarily a completely multiplicative arithmetic function when restricted $\N$. We call $m$ the \textbf{modulus}\index{modulus} of $\chi$. Sometimes we will also write $\chi_{m}$ to denote a Dirichlet character modulo $m$ if we need to express the dependence upon the modulus. For any $m \ge 1$, there is always the \textbf{principal Dirichlet character}\index{principal Dirichlet character} modulo $m$ which we denote by $\chi_{m,0}$ (sometimes also seen as $\chi_{0,m}$ or the ever more confusing $\chi_{0}$) and is defined by
    \[
      \chi_{m,0}(a) = \begin{cases} 1 & (a,m) = 1, \\ 0 & (a,m) > 1. \end{cases}
    \]
    When $m = 1$, the principal Dirichlet character is identically $1$ and we call this the \textbf{trivial Dirichlet character}\index{trivial Dirichlet character}. This is also the only Dirichlet character modulo $1$, so $\chi_{1} = \chi_{1,0}$. In general, we say a Dirichlet character $\chi$ is \textbf{principal}\index{principal} if it only takes values $0$ or $1$. We now discuss some basic facts of Dirichlet characters. By Euler's little, $a^{\vphi(m)} \equiv 1 \tmod{m}$ provided $(a,m) = 1$ and so the multiplicativity of $\chi$ implies that $\chi(a)^{\vphi(m)} = 1$. Therefore the nonzero values of $\chi_{m}$ are $\vphi(m)$-th roots of unity. In particular, there are only finitely many Dirichlet characters of any fixed modulus $m$. Given two Dirichlet character $\chi$ and $\psi$ modulo $m$, we define $\chi\psi$ by $\chi\psi(a) = \chi(a)\psi(a)$. This is also a Dirichlet character modulo $m$, so the Dirichlet characters modulo $m$ form an abelian group denoted by $X_{m}$. If we have a Dirichlet character $\chi$ modulo $m$ then $\cchi$ defined by $\cchi(a) = \conj{\chi(a)}$ is also a Dirichlet character modulo $m$ and is called the \textbf{conjugate Dirichlet character}\index{conjugate Dirichlet character} of $\chi$. Since the nonzero values of $\chi$ are roots of unity, if $(a,m) = 1$ then $\cchi(a) = \chi(a)^{-1}$. So $\cchi$ is the inverse of $\chi$. This is all strikingly similar to characters of $(\Z/m\Z)^{\ast}$ (see \cref{append:Character_Groups}) and there is indeed a connection. By the periodicity of $\chi$, the nonzero values are uniquely determined by $(\Z/m\Z)^{\ast}$. As $\chi$ is multiplicative, it descends to a character $\chi$ of $(\Z/m\Z)^{\ast}$. Conversely, if we are given a character $\chi$ of $(\Z/m\Z)^{\ast}$ we can extend it to a Dirichlet character by defining it to be $m$-periodic and declaring $\chi(a) = 0$ if $(a,m) > 1$. We call this extension the \textbf{zero extension}\index{zero extension}. So in other words, Dirichlet characters modulo $m$ are the zero extensions of the character group of $(\Z/m\Z)^{\ast}$. As groups are isomorphic to their character groups (see \cref{prop:character_group_isomorphim}), we deduce that the group of Dirichlet characters modulo $m$ is isomorphic to $(\Z/m\Z)^{\ast}$. That is, $X_{m} \cong \what{(\Z/m\Z)^{\ast}} \cong (\Z/m\Z)^{\ast}$. In particular, there are $\vphi(m)$ Dirichlet characters modulo $m$ and we identify them with the character group of $(\Z/m\Z)^{\ast}$. We now state two very useful relations called \textbf{Dirichlet orthogonality relations}\index{Dirichlet orthogonality relations} for Dirichlet characters (this follows from the more general orthogonality relations in \cref{append:Character_Groups} but we wish to give a direct proof):

    \begin{proposition*}[Dirichlet orthogonality relations]
    \phantom{ }
      \begin{enumerate}[label*=(\roman*)]
        \item For any two Dirichlet characters $\chi$ and $\psi$ modulo $m$,
        \[
          \frac{1}{\vphi(m)}\psum_{a \tmod{m}}\chi(a)\conj{\psi}(a) = \d_{\chi,\psi}.
        \]
        \item For any $a,b \in (\Z/m\Z)^{\ast}$,
        \[
          \frac{1}{\vphi(m)}\sum_{\chi \tmod{m}}\chi(a)\cchi(b) = \d_{a,b}.
        \]
      \end{enumerate}
    \end{proposition*}
    \begin{proof}
      We will prove the statements separately.
      \begin{enumerate}[label*=(\roman*)]
        \item Denote the left-hand side by $S$ and let $b$ be such that $(b,m) = 1$. Then
        \begin{align*}
          \chi(b)\conj{\psi}(b)S &= \frac{\chi(b)\conj{\psi}(b)}{\vphi(m)}\psum_{a \tmod{m}}\chi(a)\conj{\psi}(a) \\
          &= \frac{1}{\vphi(m)}\psum_{a \tmod{m}}\chi(ab)\conj{\psi}(ab) \\
          &= \frac{1}{\vphi(m)}\psum_{a \tmod{m}}\chi(a)\conj{\psi}(a) && \text{$a \mapsto a\conj{b}$} \\
          &= S.
        \end{align*}
        Consequently $S = 0$ unless $\chi(b)\conj{\psi}(b) = 1$ for all $b$ such that $(b,m) = 1$. This happens if and only if $\psi = \chi$ in which case $S = 1$ proving (i).
        \item Denote the left-hand side by $S$. Let $\psi$ be any Dirichlet character modulo $m$. Then
        \begin{align*}
          \psi(a)\conj{\psi}(b)S &= \frac{\psi(a)\conj{\psi}(b)}{\vphi(m)}\sum_{\chi \tmod{m}}\chi(a)\cchi(b) \\
          &= \frac{1}{\vphi(m)}\sum_{\chi \tmod{m}}\chi\psi(a)\conj{\chi\psi}(b) \\
          &= \frac{1}{\vphi(m)}\sum_{\chi \tmod{m}}\chi(a)\cchi(b) && \text{$\chi \mapsto \chi\conj{\psi}$} \\
          &= S.
        \end{align*}
        Thus $S = 0$ unless $\psi(a)\conj{\psi}(b) = \psi(a\conj{b}) = 1$ for all Dirichlet characters $\psi$ modulo $m$. If this happens then $a\conj{b} = 1 \tmod{m}$. To see this, let $m = p_{1}^{r_{1}} \cdots p_{k}^{r_{k}}$ be the prime factorization of $m$. By the structure theorem for finite abelian groups, we have
        \[
          (\Z/m\Z)^{\ast} \cong (\Z/p_{1}^{r_{1}}\Z)^{\ast} \x \cdots \x (\Z/p_{k}^{r_{k}}\Z)^{\ast}.
        \]
        Under this isomorphism, any $n$ taken modulo $m$ with $(n,m) = 1$ may be written uniquely as $n = n_{1} \cdots n_{k}$ where $n_{i}$ is taken modulo $p_{i}^{r_{i}}$ with $(n_{i},p_{i}^{r_{i}}) = 1$ for $1 \le i \le k$. Let $\w_{i}$ be a primitive $p_{i}^{r_{i}}$-th root of unity for all $i$ and set
        \[
          \psi(n) = \w_{1}^{n_{1}} \cdots \w_{k}^{n_{k}}.
        \]
        Clearly $\psi$ is a character of $(\Z/m\Z)^{\ast}$ and is therefore a Dirichlet character modulo $m$. Writing $a = e_{1} \cdots e_{k}$ and $b = f_{1} \cdots f_{k}$ under this isomorphism, it follows that
        \[
          1 = \w_{1}^{e_{1}\conj{f_{1}}} \cdots \w_{k}^{e_{k}\conj{f_{k}}},
        \]
        since $\psi(a\conj{b}) = 1$. As $\w_{i}$ has order $p_{i}^{r_{i}}$ and $1 \le e_{i},f_{i} \le p_{i}^{r_{i}}-1$ for all $i$, the only way the above identity holds is if $e_{i} \equiv f_{i} \tmod{p_{i}^{r_{i}}}$ for all $i$. This implies $a\conj{b} = 1 \tmod{m}$. But then $S = 1$ and (ii) follows.
      \end{enumerate}
    \end{proof}

    In many practical settings, the Dirichlet orthogonality relations are often used in the following form:

    \begin{corollary}\label{cor:Dirichlet_orthogonality_relations}
    \phantom{ }
      \begin{enumerate}[label*=(\roman*)]
        \item For any Dirichlet character $\chi$ modulo $m$,
        \[
          \frac{1}{\vphi(m)}\psum_{a \tmod{m}}\chi(a) = \d_{\chi,\chi_{m,0}}.
        \]
        \item For any $a \in (\Z/m\Z)^{\ast}$,
        \[
          \frac{1}{\vphi(m)}\sum_{\chi \tmod{m}}\chi(a) = \d_{a,1}.
        \]
      \end{enumerate}
    \end{corollary}
    \begin{proof}
      For (i), take $\psi = \chi_{m,0}$ in the Dirichlet orthogonality relations (namely (i)). For (ii), take $b \equiv 1 \tmod{m}$ in the Dirichlet orthogonality relations (namely (ii)).
    \end{proof}

    We will now describe how Dirichlet characters of a fixed modulus arise from Dirichlet characters of a smaller modulus. Let $\chi_{m_{1}}$ and $\chi_{m_{2}}$ be Dirichlet characters modulo $m_{1}$ and $m_{2}$. If $m_{1} \mid m_{2}$ then $(a,m_{2}) = 1$ implies $(a,m_{1}) = 1$. Accordingly, we say $\chi_{m_{2}}$ is \textbf{induced}\index{induced} from $\chi_{m_{1}}$ (or that $\chi_{m_{1}}$ \textbf{lifts} to $\chi_{m_{2}}$) if
    \[
      \chi_{m_{2}}(a) = \begin{cases} \chi_{m_{1}}(a) & \text{if $(a,m_{2}) = 1$}, \\ 0 & \text{if $(a,m_{2}) > 1$}. \end{cases}
    \]
    All this means is that $\chi_{m_{2}}$ is a Dirichlet character modulo $m_{2}$ whose values are given by those of $\chi_{m_{1}}$. Clearly every Dirichlet character is induced from itself. On the other hand, if there is a prime $p$ dividing $m_{2}$ and not $m_{1}$ (so $m_{2}$ is a larger modulus), $\chi_{m_{2}}$ will be different from $\chi_{m_{1}}$ since $\chi_{m_{2}}(p) = 0$ but $\chi_{m_{1}}(p) \neq 0$. In general, we say a Dirichlet character is \textbf{primitive}\index{primitive} if it is not induced by any character other than itself and \textbf{imprimitive}\index{imprimitive} otherwise. Notice that the principal Dirichlet characters are precisely those Dirichlet characters induced from the trivial Dirichlet character, and the only primitive one is the trivial Dirichlet character. It is not a hard matter to determine when Dirichlet characters are induced:

    \begin{proposition}\label{prop:Dirichlet_character_induction_classification}
      A Dirichlet character $\chi_{m_{2}}$ is induced from a Dirichlet character $\chi_{m_{1}}$ if and only if $\chi_{m_{2}}$ is constant on the residue classes in $(\Z/m_{2}\Z)^{\ast}$ that are congruent modulo $m_{1}$. When this happens, $\chi_{m_{1}}$ is uniquely determined.
    \end{proposition}
    \begin{proof}
      For the forward implication, if $\chi_{m_{2}}$ is induced from $\chi_{m_{1}}$ then $\chi_{m_{2}}$ is constant on the residue classes in $(\Z/m_{2}\Z)^{\ast}$ that are congruent modulo $m_{1}$ because $\chi_{m_{1}}$ is. For the reverse implication, first note that the surjective homomorphism $\Z/m_{2}\Z \to \Z/m_{1}\Z$ given by reduction modulo $m_{1}$ induces a surjective homomorphism $(\Z/m_{2}\Z)^{\ast} \to (\Z/m_{1}\Z)^{\ast}$ (because reduction modulo $m_{1}$ preserve inverses). Now suppose $\chi_{m_{2}}$ is constant on the residue classes in $(\Z/m_{2}\Z)^{\ast}$ that are congruent modulo $m_{1}$. Surjectivity of the previously mentioned map implies that $\chi_{m_{2}}$ induces a unique character on $(\Z/m_{1}\Z)^{\ast}$ and hence a unique Dirichlet character modulo $m_{1}$. By construction $\chi_{m_{2}}$ is induced from $\chi_{m_{1}}$.
    \end{proof}

    We are interested in primitive Dirichlet characters because they are the building blocks for all Dirichlet characters:

    \begin{theorem}\label{thm:Dirichlet_character_conductor_existance}
      Every Dirichlet character $\chi$ is induced from a primitive Dirichlet character $\wtilde{\chi}$ that is uniquely determined by $\chi$.
    \end{theorem}
    \begin{proof}
      Let the modulus of $\chi$ be $m$. Define a partial ordering on the set of Dirichlet characters where $\psi \le \chi$ if $\chi$ is induced from $\psi$. This ordering is clearly reflexive, and it is transitive by \cref{prop:Dirichlet_character_induction_classification}. Set
      \[
        X = \left\{\psi \in \bigcup_{d \mid m}X_{d}:\psi \le \chi\right\}
      \]
      This set is nonempty and finite by \cref{prop:Dirichlet_character_induction_classification}. Now suppose $\chi_{m_{1}},\chi_{m_{2}} \in X$. Set $m_{3} = (m_{1},m_{2})$. Also from \cref{prop:Dirichlet_character_induction_classification}, $\chi$ is constant on the residue classes of $(\Z/m\Z)^{\ast}$ that are congruent modulo $m_{1}$ or $m_{2}$ and hence also $m_{3}$. Therefore \cref{prop:Dirichlet_character_induction_classification} implies there is a unique Dirichlet character $\chi_{m_{3}}$ modulo $m_{3}$ that lifts to $\chi_{m_{1}}$ and $\chi_{m_{2}}$. We have now shown that every pair $\chi_{m_{1}},\chi_{m_{2}} \in X$ has a lower bound $\chi_{m_{3}}$. Hence $X$ contains a primitive Dirichlet character $\wtilde{\chi}$ that is minimal with respect to this partial ordering. There is only one such element. Indeed, since $m_{3} \le m_{1},m_{2}$ the partial ordering is compatible with the total ordering by modulus. Thus $\wtilde{\chi}$ is unique.
    \end{proof}

    In light of \cref{thm:Dirichlet_character_conductor_existance}, we define \textbf{conductor}\index{conductor} $q$ of a Dirichlet character $\chi$ modulo $m$ to be the modulus of the unique primitive character $\wtilde{\chi}$ that induces $\chi$. This is the most important data of a Dirichlet character since it tells us how $\chi$ is built. Note that $\chi$ is primitive if and only if its conductor and modulus are equal. Also observe that if $\chi$ has conductor $q$ then $\chi$ is actually $q$-periodic, we must have $q \mid m$, and the nonzero values of $\chi$ are all $q$-th roots of unity because those are the nonzero values of $\wtilde{\chi}$. Note that $\chi = \wtilde{\chi}\chi_{\frac{m}{q},0}$ by the definition of induced Dirichlet characters. Moreover, we have the formula
    \[
      \phi(m) = \sum_{d \mid m}N(d),
    \]
    where $N(d)$ is the number of primitive Dirichlet characters modulo $d$. Indeed, the right-hand side counts the number of Dirichlet characters modulo $m$ since every such Dirichlet character is induced from a unique primitive Dirichlet character by \cref{thm:Dirichlet_character_conductor_existance} whose modulus must divide $m$ as we have already mentioned. The right-hand side also counts the number of Dirichlet characters modulo $m$ since we have already seen that there are $\phi(m)$ of them (because the group of Dirichlet characters is isomorphic to the character group of $(\Z/m\Z)^{\ast}$). Primitive Dirichlet characters also behave well with respect to multiplication if the conductors are relatively prime as the following proposition shows:

    \begin{proposition}\label{prop:primitive_characters_multiplicative_relatively_prime}
      Suppose $\chi_{1}$ and $\chi_{2}$ are Dirichlet characters modulo $q_{1}$ and $q_{2}$ respectively with $q_{1}$ and $q_{2}$ relatively prime. Set $\chi = \chi_{1}\chi_{2}$ so that $\chi$ is a Dirichlet character modulo $q_{1}q_{2}$. Then $\chi$ is a primitive if and only if $\chi_{1}$ and $\chi_{2}$ are both primitive.
    \end{proposition}
    \begin{proof}
      First suppose $\chi$ is primitive of conductor $q$. If $d_{1}$ and $d_{2}$ are the conductors of $\chi_{1}$ and $\chi_{2}$ respectively then $\chi$ is $d_{1}d_{2}$-periodic and primitivity further implies that $q \mid d_{1}d_{2}$. But as $d_{1} \mid q_{1}$, $d_{2} \mid q_{2}$, and $q = q_{1}q_{2}$, we must have $q = d_{1}d_{2}$ and hence $d_{1} = q_{1}$ and $d_{2} = q_{2}$. It follows that $\chi_{1}$ and $\chi_{2}$ are both primitive. Conversely, suppose $\chi_{1}$ and $\chi_{2}$ are both primitive. If $d$ is the conductor of $\chi$, set $d_{1} = (d,q_{1})$ and $d_{2} = (d,q_{2})$. As $(q_{1},q_{2}) = 1$ and $q = q_{1}q_{2}$, we must have $(d_{1},d_{2}) = 1$ and $d_{1}d_{2} = q$. But then $d_{1} = q_{1}$ and $d_{2} = q_{2}$. Hence $d = q_{1}q_{2}$ which implies that $\chi$ is primitive.
    \end{proof}
    
    We would now like to distinguish Dirichlet characters whose nonzero values are either real or imaginary. We say $\chi$ is \textbf{real}\index{real} if it is real-valued. Hence the nonzero values of $\chi$ are $1$ or $-1$ since they must be roots of unity. We say $\chi$ is an \textbf{complex}\index{complex} if it is not real. More commonly, we distinguish Dirichlet characters modulo $m$ by their order as an element of $(\Z/m\Z)^{\ast}$. If $\chi$ is of order $2$, $3$, etc.\ in $(\Z/m\Z)^{\ast}$ then we say it is \textbf{quadratic}\index{quadratic}, \textbf{cubic}\index{cubic}, etc. In particular, a Dirichlet character is quadratic if and only if it is real. For any Dirichlet character $\chi$, $\chi(-1) = \pm 1$ because $\chi(-1)^{2} = 1$. We would like to distinguish this parity. Accordingly, we say $\chi$ is \textbf{even}\index{even} if $\chi(-1) = 1$ and \textbf{odd}\index{odd} if $\chi(-1) = -1$. Clearly even Dirichlet characters are even functions and odd Dirichlet characters are odd functions. Moreover, $\chi$ and $\cchi$ have the same parity and any lift of $\chi$ has the same parity as $\chi$. Also note that
    \[
      \frac{\chi(1)-\chi(-1)}{2} = \begin{cases} 0 & \text{if $\chi$ is even}, \\ 1 & \text{if $\chi$ is odd}. \end{cases}
    \]
    Lastly, we would like to discuss quadratic Dirichlet characters. We can construct quadratic Dirichlet characters using Jacobi symbols. If $m \ge 1$ is odd, consider
    \[
      \chi_{m}(n) = \tlegendre{n}{m}.
    \]
    Clearly $\chi_{m}$ a quadratic Dirichlet character modulo $m$ because the Jacobi symbol is multiplicative, nonzero if and only if $(n,m) = 1$, and determined modulo $m$. However, quadratic Dirichlet characters given by Jacobi symbols do not exhaust all possible quadratic Dirichlet characters. For this, we need to use Kronecker symbols. We say that $D \in \Z$ is a \textbf{fundamental discriminant}\index{fundamental discriminant} if $D$ is of the form
    \[
      D = \begin{cases} d & \text{if $D \equiv 1 \tmod{4}$}, \\ 4d & \text{if $\frac{D}{4} \equiv 2,3 \tmod{4}$}, \end{cases}
    \]
    for some square-free $d \in \Z$. Necessarily $d \equiv 1 \tmod{4}$ or $d \equiv 2,3 \tmod{4}$ respectively and thus nonzero. We define the \textbf{quadratic Dirichlet character} $\chi_{D}$ associated to the fundamental discriminant $D$ by
    \[
      \chi_{D}(m) = \legendre{D}{m}.
    \]
    It turns out that $\chi_{D}$ defines a primitive quadratic Dirichlet character, and exhausts all primitive quadratic Dirichlet characters, as the following theorem shows:

    \begin{theorem}\label{thm:fundamental_discriminant_character_primitive}
      If $D$ is a fundamental discriminant and $D \neq 1$ then $\chi_{D}$ is a primitive quadratic Dirichlet character of conductor $|D|$. Moreover, all primitive quadratic Dirichlet characters are of this form.
    \end{theorem}
    \begin{proof}
      We first show that $\chi_{D}$ is a quadratic Dirichlet character of conductor $|D|$. If $D \equiv 1 \tmod{4}$, the sign in quadratic reciprocity is always $1$ so that
      \[
        \chi_{D}(m) = \legendre{m}{|D|},
      \]
      and hence is a quadratic Dirichlet character modulo $|D|$ because it is given by the Jacobi symbol. If $\frac{D}{4} \equiv 3 \tmod{4}$, the sign in quadratic reciprocity is $\tlegendre{-1}{m}$ which is the primitive quadratic Dirichlet character modulo $4$ (there are only two Dirichlet characters modulo $4$ since $\vphi(4) = 2$ and clearly $\legendre{-1}{m}$ is not principal) so that
      \[
        \chi_{D}(m) = \legendre{-1}{m}\legendre{m}{\left|\frac{D}{4}\right|},
      \]
      and hence is a Dirichlet character modulo $|D|$. If $\frac{D}{4} \equiv 2 \tmod{16}$, first observe that $\tlegendre{D}{m} = \tlegendre{8}{m}\tlegendre{\frac{D}{8}}{m}$ where $\tlegendre{8}{m}$ is one of the two primitive quadratic Dirichlet character modulo $8$ (the other is $\tlegendre{-8}{m}$ as there are four Dirichlet character modulo $8$ because $\vphi(8) = 4$ and the other two are the principal Dirichlet character and the Dirichlet character induced from $\legendre{-1}{m}$ as mentioned previously). As $\frac{D}{8} \equiv 1,3 \tmod{4}$, the sign in quadratic reciprocity is either $1$ or $\tlegendre{-1}{m}$ according to these two cases. Thus
      \[
        \chi_{D}(m) = \legendre{8}{m}\legendre{m}{\left|\frac{D}{8}\right|} \quad \text{or} \quad \chi_{D}(m) = \legendre{-8}{m}\legendre{m}{\left|\frac{D}{8}\right|},
      \]
      according to if $\frac{D}{8} \equiv 1,3 \tmod{4}$ respectively, and hence is a quadratic Dirichlet character modulo $|D|$. We can compactly express all of these cases as follows:
      \[
        \chi_{D}(m) = \begin{cases} \legendre{m}{|D|} & \text{if $D \equiv 1 \tmod{4}$}, \\ \legendre{-1}{m}\legendre{m}{\left|\frac{D}{4}\right|} & \text{if $\frac{D}{4} \equiv 3 \tmod{4}$}, \\ \legendre{8}{m}\legendre{m}{\left|\frac{D}{8}\right|} & \text{if $\frac{D}{8} \equiv 1 \tmod{4}$}, \\ \legendre{-8}{m}\legendre{m}{\left|\frac{D}{8}\right|} & \text{if $\frac{D}{8} \equiv 3 \tmod{4}$}. \end{cases}
      \]
      This shows that $\chi_{D}$ is a quadratic Dirichlet characters modulo $|D|$. It easily follows from the above that $\chi_{D}$ is primitive. Indeed, we have already mentioned that the characters $\tlegendre{-1}{m}$, $\tlegendre{8}{m}$, and $\tlegendre{-8}{m}$ are all primitive. Therefore, since $D$, $\frac{D}{4}$, and $\frac{D}{8}$ are square-free according to their equivalences modulo $4$ as given above, and $D \neq 1$, it suffices to show by \cref{prop:primitive_characters_multiplicative_relatively_prime} that $\chi_{p}$ is primitive for all primes $p$ with $p \neq 2$. This is immediate since $p$ is prime and clearly $\chi_{p}$ is not principal. We now show that every primitive quadratic Dirichlet character is of the form $\chi_{D}$ for some fundamental discriminant $D$. By \cref{prop:primitive_characters_multiplicative_relatively_prime}, it suffices to consider primitive quadratic Dirichlet character modulo $q = p^{m}$ for some prime $p$ and $m \ge 1$. First suppose that $p \neq 2$. Then $(\Z/q\Z)^{\ast}$ is cyclic and so every $n \in (\Z/p^{m}\Z)^{\ast}$ is of the form $n = v^{\nu}$ for some $\nu \in (\Z/\vphi(p^{m})\Z)$ and where $v$ is a generator of $(\Z/p^{m}\Z)^{\ast}$. It follows that every Dirichlet character $\chi$ modulo $p^{m}$ is of the form
      \[
        \chi(n) = e^{\frac{2\pi ik\nu}{\vphi(p^{m})}},
      \]
      where $0 \le k \le \vphi(q)-1$. Indeed, this is a unique Dirichlet character for every such $k$ and there are $\vphi(p^{m})$ Dirichlet characters modulo $p^{m}$ which is the same number of choices for $k$. Moreover, $\chi$ is primitive if and only if $p \nmid k$ for otherwise $\chi$ is a Dirichlet character modulo $p^{m-1}$. Similarly, $\chi$ is quadratic if and only if $\frac{k}{\vphi(p^{m})}$ has at most $2$ in its denominator which is equivalent to $k \equiv \frac{\vphi(p^{m})}{2} \tmod{\vphi(p^{m})}$ and hence such a $k$ exists and is unique because $p \neq 2$. We also see that if $\chi$ is quadratic, it is imprimitive unless $m = 1$ for then $\vphi(p) = p-1$ is not a multiple of $p$. All of this is to say that there is a unique quadratic Dirichlet character modulo $q$ and it is primitive if and only if $q = p$. Necessarily, this unique primitive quadratic Dirichlet character modulo $p$ is given by $\chi_{D}$ for the fundamental discriminant $D = p$ if $p \equiv 1 \tmod{4}$ and $D = -p$ if $p \equiv 3 \tmod{4}$. Now suppose $p = 2$ so that $q = 2^{m}$ for some $m \ge 1$. If $m = 1$, $\vphi(2) = 1$ and there are no primitive quadratic Dirichlet characters as the only Dirichlet character is principal. If $m = 2$, $\vphi(4) = 2$ so that there are two Dirichlet characters. They are both quadratic but only one is primitive, namely the principal Dirichlet character as well as the aforementioned primitive quadratic Dirichlet character $\tlegendre{-1}{m}$. For $m \ge 3$, $(\Z/2^{m}\Z)^{\ast} \cong C_{2} \x C_{2^{m-2}}$ where $C_{2}$ and $C_{2^{m-2}}$ are the cyclic groups of order $2$ and $2^{m-2}$ respectively. Therefore every $n \in (\Z/2^{m}\Z)^{\ast}$ is of the form $n = (-1)^{\mu}5^{\nu}$ for $\mu \in \Z/2\Z$ and $\nu \in \Z/2^{m-2}\Z$ (because the orders of $-1$ and $5$ modulo $2^{m}$ are $2$ and $2^{m-2}$ respectively, with the latter case following by induction for $m \ge 3$, and that $\<-1\> \cap \<5\> = \{1\}$). Then every Dirichlet character $\chi$ modulo $2^{m}$, for $m \ge 3$, is of the form
      \[
        \chi(n) = e^{\frac{2\pi ij\mu}{2}}e^{\frac{2\pi ik\nu}{2^{m-2}}},
      \]
      where $0 \le j \le 1$ and $0 \le k \le 2^{m-2}-1$. Indeed, this is a unique Dirichlet character for every such choice of $j$ and $k$ and there are $2^{m-1}$ Dirichlet characters modulo $2^{m}$ which is the same number of choices for $j$ and $k$. Similarly to the case for $p \neq 2$, $\chi$ is primitive if and only if $2^{m-2} \nmid k$, or equivalently, $k$ is odd. Moreover, $\chi$ is quadratic if and only if $\frac{k}{2^{m-2}}$ has at most $2$ in its denominator which is to say that $2^{m-3} \mid k$. Therefore for a primitive quadratic Dirichlet to exist we must have $k$ odd and $2^{m-3} \mid k$ which can happen if and only if $m = 3$. Then $\phi(8) = 4$, so that there are four Dirichlet characters. They are all quadratic but only two are primitive, namely the principal Dirichlet character, the Dirichlet character induced from $\tlegendre{-1}{m}$, and the two aforementioned primitive quadratic Dirichlet characters given by $\tlegendre{8}{m}$ and $\tlegendre{-8}{m}$. These three primitive quadratic Dirichlet characters are given by $\chi_{D}$ for the fundamental discriminants $D = -4$, $D = 8$, and $D = -8$ respectively. We have now shown that all primitive quadratic Dirichlet characters of prime power modulus are given by $\chi_{D}$ for some fundamental discriminant $D$ and thus the same follows for all primitive quadratic Dirichlet characters by \cref{prop:primitive_characters_multiplicative_relatively_prime}. This completes the proof.
    \end{proof}

    It follows from \cref{thm:fundamental_discriminant_character_primitive} that all quadratic Dirichlet characters are induced from some $\chi_{D}$ (including $D = 1$ since this corresponds to the trivial Dirichlet character). In particular, so too are the quadratic Dirichlet characters given by Jacobi symbols.
  \section{Exponential Sums}
    Number theory comes with its class of exponential sums that appear naturally. They play the role of discrete counterparts to continuous objects (there is a rich underpinning here). Without a sufficient understanding of these sums, they would cause a discrete obstruction to an analytic problem that we wish to solve.
    \subsection*{Ramanujan and Gauss Sums}
      Let's begin with the Ramanujan sum. For $m \ge 1$ and $n \in \Z$, the \textbf{Ramanujan sum}\index{Ramanujan sum} $r(n,m)$ is defined by
      \[
        r(n,m) = \psum_{a \tmod{m}}e^{\frac{2\pi ian}{m}}.
      \]
      Note that the Ramanujan sum is a finite sum of $m$-th roots of unity on the unit circle. Clearly we have $r(0,m) = \vphi(m)$. Ramanujan sums can be computed explicitly by means of the M\"obius function:

      \begin{proposition}\label{prop:Ramanujan_sum_evaluation}
        For any $m \ge 1$ and any nonzero $n \in \Z$,
        \[
          r(n,m) = \sum_{d \mid (n,m)}d\mu\left(\frac{m}{d}\right).
        \]
      \end{proposition}
      \begin{proof}
        Summing $r(n,d)$ over the divisors $d$ of $m$ yields
        \[
          \sum_{d \mid m}r(n,d) = \sum_{d \mid m}\psum_{a \tmod{d}}e^{\frac{2\pi ian}{d}} = \sum_{d \mid m}\psum_{a \tmod{\frac{m}{d}}}e^{\frac{2\pi iadn}{m}} = \sum_{b \tmod{m}}e^{\frac{2\pi ibn}{m}} ,
        \]
        where the second equality follows by making the change of variables $d \to \frac{m}{d}$ and the third equality holds by observing that every integer $b$ modulo $m$ is of the form $b = ad$ for some $d \mid m$ and $a$ taken modulo $\frac{m}{d}$ with $\left(a,\frac{m}{d}\right) = 1$. Indeed, this is seen upon taking $d = (b,m)$. If $m \mid n$ the inner sum is $m$ and otherwise it is zero because it is the sum of all the $m$-th roots of unity. Thus
        \[
          \sum_{d \mid m}r(n,d) = \begin{cases} m & \text{if $m \mid n$}, \\ 0 & \text{if $m \nmid n$}. \end{cases}
        \]
        By the M\"obius inversion formula, we have
        \[
          r(n,m) = \sum_{d \mid (n,m)}d\mu\left(\frac{m}{d}\right),
        \]
        as desired.
      \end{proof}
      We can also define a Ramanujan sum associated to Dirichlet characters. Let $\chi$ be a Dirichlet character modulo $m$. For any $b \in \Z$, the \textbf{Ramanujan sum}\index{Ramanujan sum} $\tau(b,\chi)$ associated to $\chi$ is given by
      \[
        \tau(b,\chi) = \sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi iab}{m}}.
      \]
      If $b = 1$ we will write $\tau(\chi)$ instead. That is, $\tau(\chi) = \tau(1,\chi)$. We call $\tau(\chi)$ the \textbf{Gauss sum}\index{Gauss sum} associated to $\chi$. Observe that if $m = 1$ then $\chi$ is the trivial character and $\tau(b,\chi) = 1$. So the interesting cases are when $m \ge 2$. There are some basic properties of these sums which are very useful:

      \begin{proposition}\label{prop:Gauss_sum_reduction}
        Let $\chi$ and $\psi$ be nontrivial Dirichlet characters modulo $m$ and $n$ respectively and let $b \in \Z$. Then the following hold:
        \begin{enumerate}[label*=(\roman*)]
          \item $\conj{\tau(b,\cchi)} = \chi(-1)\tau(b,\chi)$.
          \item If $(b,m) = 1$ then $\tau(b,\chi) = \cchi(b)\tau(\chi)$.
          \item If $(b,m) > 1$ and $\chi$ is primitive then $\tau(b,\chi) = 0$.
          \item If $(m,n) = 1$ then $\tau(b,\chi\psi) = \chi(n)\psi(m)\tau(b,\chi)\tau(b,\psi)$.
          \item Let $q$ be the conductor of $\chi$ and let $\wtilde{\chi}$ be the primitive Dirichlet character that lifts to $\chi$. Then
          \[
            \tau(\chi) = \mu\left(\frac{m}{q}\right)\wtilde{\chi}\left(\frac{m}{q}\right)\tau(\wtilde{\chi}).
          \]
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        We will prove the statements separately.
        \begin{enumerate}[label*=(\roman*)]
          \item We compute
          \begin{align*}
            \conj{\tau(b,\cchi)} &= \conj{\sum_{a \tmod{m}}\cchi(a)e^{\frac{2\pi iab}{m}}} \\
            &= \sum_{a \tmod{m}}\chi(a)e^{-\frac{2\pi iab}{m}} \\
            &= \sum_{a \tmod{m}}\chi(-a)e^{\frac{2\pi iab}{m}} && \text{$a \mapsto -a$} \\
            &= \chi(-1)\sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi iab}{m}} \\
            &= \chi(-1)\tau(b,\chi).
          \end{align*}
          This proves (i).
          \item We compute
          \begin{align*}
            \tau(b,\chi) &= \sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi iab}{m}} \\
            &= \sum_{a \tmod{m}}\chi(a\conj{b})e^{\frac{2\pi ia}{m}} && \text{$a \mapsto a\conj{b}$} \\
            &= \cchi(b)\sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi ia}{m}} \\
            &= \cchi(b)\tau(\chi).
          \end{align*}
          This proves (ii).
          \item Suppose $d$ is a proper divisor of $m$ and $c$ is an integer $c$ such that $c \equiv 1 \tmod{m}$. Then necessarily $(c,m) = 1$. Also note that as $d \mid m$, $c \equiv 1 \tmod{d}$ and $(c,d) = 1$. Moreover, there is such a $c$ with the additional property that $\chi(c) \neq 1$. For if not, $\chi$ is induced from $\chi_{d,0}$ which contradicts $\chi$ being primitive. Now take $d = \frac{m}{(b,m)}$ and choose $c$ as above. Then
          \[
            \chi(c)\tau(b,\chi) = \sum_{a \tmod{m}}\chi(ac)e^{\frac{2\pi iab}{m}} = \sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi iab\conj{c}}{m}} = \tau(b,\chi)
          \]
          upon making the change of variables $a \mapsto a\conj{c}$ and where the last equality holds because $\conj{c} \equiv 1 \tmod{d}$ and $e^{\frac{2\pi ib}{m}}$ is a $d$-th root of unity. So altogether $\chi(c)\tau(b,\chi) = \tau(b,\chi)$. Since $\chi(c) \neq 1$, we conclude $\tau(b,\chi) = 0$ and (iii) follows.
          \item Since $(m,n) = 1$, the Chinese remainder theorem implies that we have an isomorphism
          \[
            (\Z/m\Z) \op (\Z/n\Z) \to (\Z/mn\Z) \qquad a \oplus a' \mapsto an+a'm.
          \]
          Under this isomorphism, we make the following computation:
          \begin{align*}
            \tau(b,\chi\psi) &= \sum_{an+a'm \tmod{mn}}\chi\psi(an+a'm)e^{\frac{2\pi i(an+a'm)b}{mn}} \\
            &= \sum_{a\tmod{m}}\sum_{a'\tmod{n}}\chi\psi(an+a'm)e^{\frac{2\pi i(an+a'm)b}{mn}} \\
            &= \sum_{a\tmod{m}}\sum_{a'\tmod{n}}\chi(an)\psi(a'm)e^{\frac{2\pi iab}{m}}e^{\frac{2\pi ia'b}{n}} \\
            &= \chi(n)\psi(m)\sum_{a\tmod{m}}\chi(a)e^{\frac{2\pi iab}{m}}\sum_{a'\tmod{n}}\psi(a')e^{\frac{2\pi ia'b}{n}} \\
            &= \chi(n)\psi(m)\tau(b,\chi)\tau(b,\psi).
          \end{align*}
          This proves (iv).
          \item If $\left(\frac{m}{q},q\right) > 1$ then $\wtilde{\chi}\left(\frac{m}{q}\right) = 0$ so we need to show $\tau(\chi) = 0$. As $\left(\frac{m}{q},q\right) > 1$, there exists a prime $p$ such that $p \mid \frac{m}{q}$ and $p \mid q$. By Euclidean division we may write any $a$ modulo $m$ in the form $a = a'\frac{m}{p}+a''$ with $a'$ taken modulo $p$ and $a''$ taken modulo $\frac{m}{p}$. Then
          \begin{equation}\label{equ:Gauss_sum_reduction_1}
            \tau(\chi) = \sum_{a \tmod{m}}\chi(a)e^{\frac{2\pi ia}{m}} = \sum_{\substack{a' \tmod{p} \\ a'' \tmod{\frac{m}{p}}}}\chi\left(a'\frac{m}{p}+a''\right)e^{\frac{2\pi i\left(a'\frac{m}{p}+a''\right)}{m}}.
          \end{equation}
          Since $p \mid \left(\frac{m}{q},q\right)$, we have $p^{2} \mid m$. Therefore $\left(a'\frac{m}{p}+a'',m\right) = 1$ if and only if $\left(a'\frac{m}{p}+a'',\frac{m}{p}\right) = 1$ and this latter condition is equivalent to $\left(a'',\frac{m}{p}\right) = 1$. Thus the last sum in \cref{equ:Gauss_sum_reduction_1} is
          \[
            \sum_{\substack{a' \tmod{p} \\ a'' \tmod{\frac{m}{p}} \\ \left(a'',\frac{m}{p}\right) = 1}}\chi\left(a'\frac{m}{p}+a''\right)e^{\frac{2\pi i\left(a'\frac{m}{p}+a''\right)}{m}}.
          \]
          As $p \mid \frac{m}{q}$, we know $q \mid \frac{m}{p}$ so that $a'\frac{m}{p}+a'' \equiv a'' \tmod{q}$. Then \cref{prop:Dirichlet_character_induction_classification} implies $\chi\left(a'\frac{m}{p}+a''\right) = \wtilde{\chi}(a'')$ and this sum is further reduced to
          \begin{equation}\label{equ:Gauss_sum_reduction_2}
            \psum_{a'' \tmod{\frac{m}{p}}}\wtilde{\chi}(a'')e^{\frac{2\pi ia''}{m}}\sum_{a' \tmod{p}}e^{\frac{2\pi ia'}{p}}.
          \end{equation}
          The inner sum in \cref{equ:Gauss_sum_reduction_2} vanishes since it is the sum over all $p$-th roots of unity and thus $\tau(\chi) = 0$. Now suppose $\left(\frac{m}{q},q\right) = 1$. Then (iv) implies
          \[
            \tau(\chi) = \tau(\wtilde{\chi}\chi_{\frac{m}{q},0}) = \wtilde{\chi}\left(\frac{m}{q}\right)\chi_{\frac{m}{q},0}(q)\tau(\wtilde{\chi})\tau(\chi_{\frac{m}{q},0}) = \tau(\chi_{\frac{m}{q},0})\wtilde{\chi}\left(\frac{m}{q}\right)\tau(\wtilde{\chi}).
          \]
          Now observe that $\tau(\chi_{\frac{m}{q},0}) = r\left(1,\frac{m}{q}\right)$. By \cref{prop:Ramanujan_sum_evaluation} we see that $r\left(1,\frac{m}{q}\right) = \mu\left(\frac{m}{q}\right)$ and
          \[
            \tau(\chi) = \mu\left(\frac{m}{q}\right)\wtilde{\chi}\left(\frac{m}{q}\right)\tau(\wtilde{\chi}),
          \]
          as claimed. This proves (v).
        \end{enumerate}
      \end{proof}

      Notice that \cref{prop:Gauss_sum_reduction} reduces the evaluation of the Ramanujan sum $\tau(b,\chi)$ to that of the Gauss sum $\tau(\chi)$ at least when $\chi$ is primitive. When $\chi$ is imprimitive and $(b,m) > 1$ we need to appeal to evaluating $\tau(b,\chi)$ by more direct means. Evaluating $\tau(\chi)$ for general characters $\chi$ turns out to be a very difficult problem and is still open. However, it is not difficult to determine the modulus of $\tau(\chi)$ when $\chi$ is primitive:

      \begin{theorem}\label{thm:Gauss_sum_modulus}
        Let $\chi$ be a primitive Dirichlet character of conductor $q$. Then
        \[
          |\tau(\chi)| = \sqrt{q}.
        \]
      \end{theorem}
      \begin{proof}
        If $\chi$ is the trivial character this is obvious since $\tau(\chi) = 1$. So we may assume $\chi$ is nontrivial. Now this is just a computation:
        \begin{align*}
          |\tau(\chi)|^{2} &= \tau(\chi)\conj{\tau(\chi)} \\
          &= \sum_{a \tmod{q}}\tau(\chi)\cchi(a)e^{-\frac{2\pi ia}{q}} \\
          &=  \sum_{a \tmod{q}}\tau(a,\chi)e^{-\frac{2\pi ia}{q}} & \text{\cref{prop:Gauss_sum_reduction} (ii)} \\
          &= \sum_{a \tmod{q}}\left(\sum_{a' \tmod{q}}\chi(a')e^{\frac{2\pi iaa'}{q}}\right)e^{-\frac{2\pi ia}{q}} \\
          &= \sum_{a,a' \tmod{q}}\chi(a')e^{\frac{2\pi ia(a'-1)}{q}} \\
          &= \sum_{a' \tmod{q}}\chi(a')\left(\sum_{a \tmod{q}}e^{\frac{2\pi ia(a'-1)}{q}}\right).
        \end{align*}
        Let $S(a')$ denote the inner sum. For the $a'$ such that $a'-1 \equiv 0 \tmod{q}$, we have $S(a') = q$. Otherwise, the change of variables $a \mapsto a\conj{(a'-1)}$ shows that $S(a') = 0$ because it is the sum of all $q$-th roots of unity. It follows that the double sum is $\chi(1)q = q$. So altogether $|\tau(\chi)|^{2} = q$ and hence $|\tau(\chi)| = \sqrt{q}$.
      \end{proof}

      As an almost immediate corollary to \cref{thm:Gauss_sum_modulus}, we deduce a useful expression for primitive Dirichlet characters of conductor $q$:

      \begin{corollary}\label{cor:gauss_sum_primitive_formula}
        Let $\chi$ be a primitive Dirichlet character of conductor $q$. Then
        \[
          \tau(n,\chi) = \cchi(n)\tau(\chi),
        \]
        for all $n \in \Z$. In particular,
        \[
          \chi(n) = \frac{1}{\tau(\cchi)}\sum_{a \tmod{q}}\cchi(a)e^{\frac{2\pi ian}{q}},
        \]
        for all $n \in \Z$.
      \end{corollary}
      \begin{proof}
        If $\chi$ is the trivial character this is obvious since $\tau(n,\chi) = 1$. So assume $\chi$ is nontrivial. If $(n,q) = 1$ then the first identity is \cref{prop:Gauss_sum_reduction} (ii). If $(n,q) > 1$ then the first identity follows from \cref{prop:Gauss_sum_reduction} (iii) and that $\cchi(n) = 0$. This proves the first identity in full. For the second identity, first note that $\tau(\chi) \neq 0$ by \cref{thm:Gauss_sum_modulus}. Replacing $\chi$ with $\cchi$, dividing the first identity by $\tau(\chi)$, and expanding the Ramanujan sum, gives the second identity.
      \end{proof}

      In light of \cref{thm:Gauss_sum_modulus} we define the \textbf{epsilon factor}\index{epsilon factor} $\e_{\chi}$ for a Dirichlet character $\chi$ modulo $m$ by
      \[
        \e_{\chi} = \frac{\tau(\chi)}{\sqrt{m}}.
      \]
      \cref{thm:Gauss_sum_modulus} says that this value lies on the unit circle when $\chi$ is primitive and not the trivial character. The question of the evaluation of Gauss sums boils down to determining what value the epsilon factor is. This is the real difficultly as the epsilon factor is quite difficult to calculate and its value is not known for general Dirichlet characters. However, when $\chi$ is primitive there is a simple relationship between $\e_{\chi}$ and $\e_{\cchi}$:

      \begin{proposition}\label{prop:epsilon_factor_relationship}
        Let $\chi$ be a primitive Dirichlet character of conductor $q$. Then
        \[
          \e_{\chi}\e_{\cchi} = \chi(-1).
        \]
      \end{proposition}
      \begin{proof}
        If $\chi$ is trivial this is obvious since $\e_{\chi} = \e_{\cchi} = 1$. So assume $\chi$ is nontrivial. By \cref{prop:Gauss_sum_reduction} (i) and that $\e_{\chi}$ lies on the unit circle, we have
        \[
          \e_{\chi} = \frac{\tau(\chi)}{\sqrt{q}} = \chi(-1)\conj{\frac{\tau(\chi)}{\sqrt{q}}} = \chi(-1)\e_{\cchi}^{-1},
        \]
        from whence the statement follows.
      \end{proof}
    \subsection*{Quadratic Gauss Sums}
      Another important sum is the quadratic Gauss sum. For any $m \ge 1$ and any $b \in \Z$, the \textbf{quadratic Gauss sum}\index{quadratic Gauss sum} $g(b,m)$ is defined by
      \[
        g(b,m) = \sum_{a \tmod{m}}e^{\frac{2\pi ia^{2}b}{m}}.
      \]
      If $b = 1$ we write $g(m)$ instead. That is, $g(m) = g(1,m)$. It turns out that if $\chi_{m}$ is the quadratic Dirichlet character given by the Jacobi symbol then $\tau(b,\chi_{m}) = g(b,m)$ provided $m$ is square-free. This will take a little work to prove. We first reduce to the case when $(b,m) = 1$:

      \begin{proposition}\label{prop:quadratic_Gauss_sum_relatively_prime_reduction}
        Let $m \ge 1$ be odd and let $b \in \Z$. Then
        \[
          g(b,m) = (b,m)g\left(\frac{b}{(b,m)},\frac{m}{(b,m)}\right).
        \]
      \end{proposition}
      \begin{proof}
        By Euclidean division write any $a$ modulo $m$ in the form $a = a'\frac{m}{(b,m)}+a''$ with $a'$ take modulo $(b,m)$ and $a''$ take modulo $\frac{m}{(b,m)}$. Then
        \begin{align*}
          g(b,m) &= \sum_{a \tmod{m}}e^{\frac{2\pi ia^{2}b}{m}} \\
          &= \sum_{\substack{a' \tmod{(b,m)} \\ a'' \tmod{\frac{m}{(b,m)}}}}e^{\frac{2\pi i\left(a'\frac{m}{(b,m)}+a''\right)^{2}b}{m}} \\
          &= \sum_{a'' \tmod{\frac{m}{(b,m)}}}e^{\frac{2\pi i(a'')^{2}b}{m}}\sum_{a' \tmod{(b,m)}}e^{\frac{2\pi i\left(2a''a'\frac{m}{(b,m)}+\left(a'\frac{m}{(b,m)}\right)^{2}\right)b}{m}} \\
          &= \sum_{a'' \tmod{\frac{m}{(b,m)}}}e^{\frac{2\pi i(a'')^{2}\frac{b}{(b,m)}}{\frac{m}{(b,m)}}}\sum_{a' \tmod{(b,m)}}e^{\frac{2\pi i\left(2a''a'\frac{m}{(b,m)}+\left(a'\frac{m}{(b,m)}\right)^{2}\right)\frac{b}{(b,m)}}{\frac{m}{(b,m)}}} \\
          &= (b,m)\sum_{a'' \tmod{\frac{m}{(b,m)}}}e^{\frac{2\pi i(a'')^{2}\frac{b}{(b,m)}}{\frac{m}{(b,m)}}},
        \end{align*}
        where the last line follows because $\left(2a''a'\frac{m}{(b,m)}+\left(a'\frac{m}{(b,m)}\right)^{2}\right) \equiv 0 \tmod{\frac{m}{(b,m)}}$ and thus the inner sum is $(b,m)$. The remaining sum is $g\left(\frac{b}{(b,m)},\frac{m}{(b,m)}\right)$ which finishes the proof.
      \end{proof}

      As a consequence of \cref{prop:quadratic_Gauss_sum_relatively_prime_reduction}, we may always assume $(b,m) = 1$. Now we give an equivalent formulation of the Ramanujan sum associated to quadratic Dirichlet characters given by Jacobi symbols and show that in the case $m = p$ an odd prime, the Ramanujan and quadratic Gauss sums agree:

      \begin{proposition}\label{prop:Gauss_sum_equivalence_for_primes}
        Let $m \ge 1$ and $b \in \Z$ be such that $(b,m) = 1$. Also let $\chi_{m}$ be the quadratic Dirichlet character given by the Jacobi symbol. Then
        \[
          \tau(b,\chi_{m}) = \sum_{a \tmod{m}}\left(1+\legendre{a}{m}\right)e^{\frac{2\pi iab}{m}}.
        \]
        Moreover, when $m = p$ is prime,
        \[
          \tau(b,\chi_{p}) = g(b,p).
        \]
      \end{proposition}
      \begin{proof}
        If $m = 1$ the claim is obvious since $\tau(b,\chi_{1}) = 1$ so assume $m > 1$. To prove the first statement, observe that
        \[
          \sum_{a \tmod{m}}\left(1+\legendre{a}{m}\right)e^{\frac{2\pi iab}{m}} = \sum_{a \tmod{m}}e^{\frac{2\pi iab}{m}}+\sum_{a \tmod{m}}\legendre{a}{m}e^{\frac{2\pi iab}{m}}.
        \]
        The first sum on the right-hand side is zero as it is the sum over all $m$-th roots of unity since $(b,m) = 1$. This proves the first claim. Now let $m = p$ be an odd prime. From the definition of the Jacobi symbol we see that $1+\tlegendre{a}{p} = 2,0$ depending on if $a$ is a quadratic residue modulo $p$ or not provided $a \not\equiv 0 \tmod{p}$. If $a \equiv 0 \tmod{p}$ then $1+\tlegendre{a}{p} = 1$. Moreover, if $a$ is a quadratic residue modulo $p$ then $a \equiv (a')^{2} \tmod{p}$ for some $a'$. So one the one hand,
        \[
          \tau(b,\chi_{p}) = \sum_{a \tmod{p}}\left(1+\legendre{a}{p}\right)e^{\frac{2\pi iab}{p}} = 1+2\sum_{\substack{a \tmod{p} \\ a \equiv (a')^{2} \tmod{p} \\ a \not\equiv 0 \tmod{p}}}e^{\frac{2\pi i(a')^{2}b}{p}}.
        \]
        On the other hand,
        \[
          g(b,p) = 1+\sum_{\substack{a \tmod{p} \\ a \not\equiv 0 \tmod{p}}}e^{\frac{2\pi ia^{2}b}{p}},
        \]
        but this last sum counts every quadratic residue twice because $(-a)^{2} = a^{2}$. Hence the previous two sums are equal completing the proof.
      \end{proof}

      We would like to generalize the second statement in \cref{prop:Gauss_sum_equivalence_for_primes} to when $m$ is square-free. In this direction, a series of reduction properties will be helpful:

      \begin{proposition}\label{prop:quadratic_Gauss_sum_reduction}
        Let $m,n \ge 1$, $p$ be an odd prime, and $b \in \Z$. Then the following hold:
        \begin{enumerate}[label*=(\roman*)]
          \item If $(b,p) = 1$ then $g(b,p^{r}) = pg(b,p^{r-2})$ for all $r \in \Z$ with $r \ge 2$.
          \item If $(m,n) = 1$ and $(b,mn) = 1$ then $g(b,mn) = g(bn,m)g(bm,n)$.
          \item If $m$ is odd and $(b,m) = 1$ then $g(b,m) = \tlegendre{b}{m}g(m)$ where $\tlegendre{b}{m}$ is the Jacobi symbol.
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        We will prove the statements separately.
        \begin{enumerate}[label*=(\roman*)]
          \item First notice that
          \[
            g(b,p^{r}) = \sum_{a \tmod{p^{r}}}e^{\frac{2\pi ia^{2}b}{p^{r}}} = \psum_{a \tmod{p^{r}}}e^{\frac{2\pi ia^{2}b}{p^{r}}}+\sum_{a \tmod{p^{r-1}}}e^{\frac{2\pi ia^{2}b}{p^{r-2}}},
          \]
          since every $a$ modulo $p$ satisfies $(a,p) = 1$ or not. By Euclidean division every element $a$ modulo $p^{r-1}$ is of the form $a = a'p^{r-2}+a''$ with $a'$ taken modulo $p$ and $a''$ taken modulo $p^{r-2}$. Since $(a'p^{r-2}+a'') \equiv a'' \tmod{p^{r-2}}$, every $a''$ is counted $p$ times modulo $p^{r-2}$. Along with the fact that $(a'p^{r-2}+a'')^{2} \equiv (a'')^{2} \tmod{p^{r-2}}$, we have
          \[
            \sum_{a \tmod{p^{r-1}}}e^{\frac{2\pi ia^{2}b}{p^{r-2}}} = \sum_{\substack{a' \tmod{p} \\ a'' \tmod{p^{r-2}}}}e^{\frac{2\pi i\left(a'p^{r-2}+a''\right)^{2}b}{p^{r-2}}} = p\sum_{a'' \tmod{p}}e^{\frac{2\pi i(a'')^{2}b}{p^{r-2}}} = pg(b,p^{r-2}).
          \]
          It remains to show that the sum
          \[
            \psum_{a \tmod{p^{r}}}e^{\frac{2\pi ia^{2}b}{p^{r}}},
          \]
          is zero. As this sum is exactly $r(b,p^{r})$, \cref{prop:Ramanujan_sum_evaluation} implies
          \[
            \psum_{a \tmod{p^{r}}}e^{\frac{2\pi ia^{2}b}{p^{r}}} = \mu(p^{r}) = 0,
          \]
          because $(b,p) = 1$ and $r \ge 2$. This proves (i).
          \item Observe that
            \[
              g(bn,m)g(bm,n) = \left(\sum_{a \tmod{m}}e^{\frac{2\pi ia^{2}bn}{m}}\right)\left(\sum_{a' \tmod{n}}e^{\frac{2\pi i(a')^{2}bm}{n}}\right) = \sum_{\substack{a \tmod{m} \\ a' \tmod{n}}}e^{\frac{2\pi i\left((an)^{2}+(a'm)^{2}\right)b}{mn}}.
            \]
            Since $(m,n) = 1$, the Chinese remainder theorem gives an isomorphism
            \[
              (\Z/m\Z) \op (\Z/n\Z) \to (\Z/mn\Z) \qquad a \oplus a' \mapsto an+a'm.
            \]
            Set $a'' = an+a'm$ so that $(a'')^{2} \equiv (an)^{2}+(a'm)^{2} \tmod{mn}$. Under this isomorphism, the last sum above is then equal to
            \[
              \sum_{a'' \tmod{mn}}e^{\frac{2\pi i(a'')^{2}b}{mn}},
            \]
            which is precisely $g(b,mn)$. This proves (ii).
          \item The claim is obvious if $m = 1$ because $g(b,1) = 1$ so assume $m > 1$. If $m = p$ then \cref{prop:Gauss_sum_equivalence_for_primes}, \cref{prop:Gauss_sum_reduction} (ii), and that quadratic Dirichlet characters are their own conjugate altogether imply the claim. Now let $r \ge 1$ and assume by induction that the claim holds when $m = p^{r'}$ for all positive integers $r'$ such that $r' < r$. Then by (i), we have
          \begin{equation}\label{equ:quadratic_Gauss_sum_reduction_1}
            g(b,p^{r}) = pg(b,p^{r-2}) = \legendre{b}{p^{r-2}}pg(p^{r-2}) = \legendre{b}{p^{r-2}}g(p^{r}) = \legendre{b}{p^{r}}g(p^{r}).
          \end{equation}
          It now suffices to prove the claim when $m = p^{r}q^{s}$ where $q$ is another odd prime and $s \ge 1$. Then by (ii) and \cref{equ:quadratic_Gauss_sum_reduction_1}, we compute
          \begin{align*}
            g(b,p^{r}q^{s}) &= g(bq^{s},p^{r})g(bp^{r},q^{s}) \\
            &= \legendre{bq^{s}}{p^{r}}\legendre{bp^{r}}{q^{s}}g(p^{r})g(q^{s}) \\
            &= \legendre{b}{p^{r}q^{s}}\legendre{q^{s}}{p^{r}}\legendre{p^{r}}{q^{s}}g(p^{r})g(q^{s}) \\
            &= \legendre{b}{p^{r}q^{s}}g(q^{s},p^{r})g(p^{r},q^{s}) \\
            &= \legendre{b}{p^{r}q^{s}}g(p^{r}q^{s}).
          \end{align*}
          This proves (iii).
        \end{enumerate}
      \end{proof}

      At last we can prove that our Ramanujan and quadratic Gauss sums agree for square-free $m$:

      \begin{theorem}
        Suppose $m \ge 1$ be square-free and odd and let $\chi_{m}$ be the quadratic Dirichlet character given by the Jacobi symbol. Let $b \in \mathbb{Z}$ such that $(b,m) = 1$. Then
        \[
          \tau(b,\chi_{m}) = g(b,m).
        \]
      \end{theorem}
      \begin{proof}
        The claim is obvious if $m = 1$ because $\tau(b,\chi_{1}) = 1$ and $g(b,1) = 1$ so assume $m > 1$. Since $\chi_{m}$ is quadratic, it suffices to assume $b = 1$ by \cref{prop:Gauss_sum_reduction} (ii) and \cref{prop:quadratic_Gauss_sum_reduction} (iii). Now let $m = p_{1}p_{2} \cdots p_{k}$ be the prime decomposition of $m$. Repeated application of \cref{prop:Gauss_sum_reduction} (iv) gives the first equality in the following chain:
        \begin{align*}
          \tau(\chi) &= \prod_{1 \le i < j \le k}\chi_{p_{i}}(p_{j})\chi_{p_{j}}(p_{i})\tau(\chi_{p_{i}})\tau(\chi_{p_{j}}) \\
          &= \prod_{1 \le i < j \le k}\chi_{p_{i}}(p_{j})\chi_{p_{j}}(p_{i})g(p_{i})g(p_{j}) \\
          &= \prod_{1 \le i < j \le k}g(p_{j},p_{i})g(p_{i},p_{j}) \\
          &= g(q).
        \end{align*}
        This completes the proof.
      \end{proof}

      Now let's turn to \cref{prop:quadratic_Gauss_sum_reduction} and the evaluation of the quadratic Gauss sum. \cref{prop:quadratic_Gauss_sum_reduction} (ii) and (iii) reduce the evaluation of $g(b,m)$ for odd $m$ and $(b,m) = 1$ to computing $g(p)$ for $p$ an odd prime. As with the Gauss sum, it is not difficult to compute the modulus of the quadratic Gauss sum:

      \begin{theorem}\label{thm:quadratic_Gauss_sum_modulus}
        Let $m \ge 1$ be odd. Then
        \[
          |g(m)| = \sqrt{m}.
        \]
      \end{theorem}
      \begin{proof}
        By \cref{prop:quadratic_Gauss_sum_reduction} (ii), it suffices to assume $m = p^{r}$ is a power of an odd prime. By Euclidean division write $r = 2n+r'$ for some positive integer $n$ and with $r' = 0,1$ depending on if $r$ is even or odd respectively. Then \cref{prop:quadratic_Gauss_sum_reduction} (i) implies
        \[
          |g(p^{r})|^{2} = p^{2n}|g(p^{r'})|^{2}.
        \]
        If $r' = 0$ then $2n = r$ so that $p^{2n} = p^{r}$. Thus $|g(p^{r})| = \sqrt{p^{r}}$. If $r' = 1$ then \cref{thm:Gauss_sum_modulus,prop:Gauss_sum_equivalence_for_primes} together imply $|g(p^{r'})|^{2} = p$ so that the right-hand side above is $p^{2n+1} = p^{r}$ and again we have $|g(p^{r})| = \sqrt{p^{r}}$.
      \end{proof}

      Accordingly, we define the \textbf{epsilon factor}\index{epsilon factor} $\e_{m}$ for any $m \ge 1$ by
      \[
        \e_{m} = \frac{g(m)}{\sqrt{m}}.
      \]
      \cref{thm:quadratic_Gauss_sum_modulus} says that this value lies on the unit circle when $m$ is odd. Thus the question of the evaluation of quadratic Gauss sums reduces to determining what the epsilon factor is. This was completely resolved and the original proof is due to Gauss in 1808 (see \cite{Gauss1808summatio}) while more modern proofs uses analytic techniques (see \cite{lang1994algebraic}). The precise statement is the following:

      \begin{theorem}\label{thm:Gauss's_evaluation}
        Let $m \ge 1$. Then
        \[
          \e_{m} = \begin{cases} (1+i) & \text{if $m \equiv 0 \tmod{4}$}, \\ 1 & \text{if $m \equiv 1 \tmod{4}$}, \\ 0 & \text{if $m \equiv 2 \tmod{4}$}, \\ i & \text{if $m \equiv 3 \tmod{4}$}. \end{cases}
        \]
      \end{theorem}

      As an immediate corollary, this implies the evaluation of the epsilon factor $\e_{\chi_{p}}$ where $\chi_{p}$ is the quadratic Dirichlet character given by the Jacobi symbol for an odd prime $p$:

      \begin{corollary}
        Let $p$ be an odd prime and $\chi_{p}$ be the quadratic Dirichlet character given by the Jacobi symbol. Then
        \[
          \e_{\chi_{p}} = \begin{cases} 1 & \text{if $p \equiv 1 \tmod{4}$}, \\ i & \text{if $p \equiv 3 \tmod{4}$}. \end{cases}
        \]
      \end{corollary}
      \begin{proof}
        The statement follows immediately from \cref{thm:Gauss's_evaluation,prop:Gauss_sum_equivalence_for_primes}.
      \end{proof}
    \subsection*{Kloosterman and Sali\'e Sums}
      Our last class of sums generalize both types of Ramanujan sums. For any $c \ge 1$ and $n,m \in \Z$, the \textbf{Kloosterman sum}\index{Kloosterman sum} $K(n,m,c)$ is defined by
      \[
        K(n,m,c) = \sum_{\substack{a \tmod{c} \\ (a,c) = 1}}e^{\frac{2\pi i(an+\conj{a}m)}{c}} = \psum_{a \tmod{c}}e^{\frac{2\pi i(an+\conj{a}m)}{c}}.
      \]
      Notice that if either $n = 0$ or $m = 0$ then the Kloosterman sum reduces to a Ramanujan sum. Kloosterman sums have similar properties to those of Ramanujan sums, but we will not need them. The only result we will need is a famous bound, often called the \textbf{Weil bound}\index{Weil bound} for Kloosterman sums, proved by Weil (see \cite{weil1948some} for a proof):

      \begin{theorem*}[Weil bound]
        Let $c \ge 1$ and $n,m \in \Z$. Then
        \[
          |K(n,m,c)| \le \s_{0}(c)\sqrt{(n,m,c)c}.
        \]
      \end{theorem*}

      Lastly, Sali\'e sums are Kloosterman sums with Dirichlet characters. To be precise, for any $c \ge 1$, $n,m \in \Z$, and a Dirichlet character $\chi$ with conductor $q \mid c$, the \textbf{Sali\'e sum}\index{Sali\'e sum} $S_{\chi}(n,m,c)$ is defined by
      \[
        S_{\chi}(n,m,c) = \sum_{\substack{a \tmod{c} \\ (a,c) = 1}}\chi(a)e^{\frac{2\pi i(an+\conj{a}m)}{c}} = \psum_{a \tmod{c}}\chi(a)e^{\frac{2\pi i(an+\conj{a}m)}{c}}.
      \]
      If either $n = 0$ or $m = 0$ then the Sali\'e sum reduces to the Ramanujan sum associated to $\chi$.
  \section{Integral Lattices}
    Integral Lattices are ubiquitous in number theory because they provide a way to study discrete points by geometric methods. Let $F$ be a characteristic zero field and $V$ be an $n$-dimensional $F$-vector space with nondegenerate symmetric inner product $\<\cdot,\cdot\>$. We say that a subset $\L$ of $V$ is a \textbf{integral lattice}\index{integral lattice} if $\L$ is a free abelian group. In particular, any integral lattice $\L$ is of the form
    \[
      \L = \Z v_{1}+\cdots+\Z v_{m},
    \]
    for some linearly independent vectors $v_{1},\ldots,v_{m}$ of $V$ with $m \le n$. We say $\L$ is \textbf{complete}\index{complete} if $n = m$. Necessarily, $v_{1},\ldots,v_{n}$ is a basis of $V$. If $e_{1},\ldots,e_{n}$ is an orthonormal basis for $V$, write
    \[
      v_{i} = \sum_{1 \le i \le n}v_{i,j}e_{j},
    \]
    with $v_{i,j} \in F$ for $1 \le i,j \le n$, and define the associated \textbf{generator matrix}\index{generator matrix} $P$ by
    \[
      P = \begin{pmatrix} v_{1,1} & \cdots & v_{n,1} \\ \vdots & & \vdots \\ v_{1,n} & \cdots & v_{n,n} \end{pmatrix}.
    \]
    Then $P$ is the base change matrix from $e_{1},\ldots,e_{n}$ to $v_{1},\ldots,v_{n}$. The \textbf{covolume}\index{covolume} $V_{\L}$ of a complete integral lattice $\L$ is defined to be
    \[
      V_{\L} = |\det(P)|.
    \]
    By \cref{prop:base_change_quotient_determinant} and that the base change matrix between any two orthonormal basses is an orthogonal matrix and hence has determinant $\pm 1$, the covolume is independent of the choice of bases. In particular, if $V$ is a real or complex vector space we will take $e_{1},\ldots,e_{n}$ to be the standard basis. We now introduce the notion of dual integral lattices. If $\L$ is a complete integral lattice in $V$ then the \textbf{dual}\index{dual} $\L^{\vee}$ of $\L$ is defined by
    \[
      \L^{\vee} = \{v \in V:\text{$\<\l,v\> \in \Z$ for all $\l \in \L$}\}.
    \]
    In other words, $\L^{\vee}$ consists of all of the vectors in $V$ whose inner product with elements of $\L$ are integers. The dual integral lattice is indeed a complete integral lattice as the following proposition shows:

    \begin{proposition}\label{prop:dual_integral lattice_exists}
      If $v_{1},\ldots,v_{n}$ is a basis for the complete integral lattice $\L$ then the dual basis $v_{1}^{\vee},\ldots,v_{n}^{\vee}$ is a basis for $\L^{\vee}$. In particular, $\L^{\vee}$ is a complete integral lattice.
    \end{proposition}
    \begin{proof}
      Let $v \in V$ and write
      \[
        v = \sum_{1 \le j \le n}a_{i}v_{j}^{\vee},
      \]
      with $v_{j}^{\vee} \in \R$ for all $j$. Since $\<v_{i},v_{j}^{\vee}\> = \d_{i,j}$ for $1 \le i,j \le n$, it follows that $v \in \L^{\vee}$ if and only if $v_{j}^{\vee} \in \Z$ for all $j$. This means that $v_{1}^{\vee},\ldots,v_{n}^{\vee}$ is a free abelian group of rank $n$. Hence $\L^{\vee}$ is a complete integral lattice.
    \end{proof}

    Since the dual of the dual basis is the original basis, $(\L^{\vee})^{\vee} = \L$. We say that $\L$ is \textbf{self-dual}\index{self-dual} if $\L^{\vee} = \L$. For example, the integral lattice $\Z^{n}$ is self-dual because the standard basis is self-dual. It also turns out that the covolume of the dual integral lattice is the inverse of the volume of the original integral lattice:

    \begin{proposition}\label{prop:covolume_of_dual_is_inverse}
      Let $\L$ be a complete integral lattice in $V$ and let $\L^{\vee}$ be its dual. Then
      \[
        V_{\L^{\vee}} = \frac{1}{V_{\L}}.
      \]
    \end{proposition}
    \begin{proof}
      Let $e_{1},\ldots,e_{n}$ be the standard basis for $V$ and let $v_{1},\ldots,v_{n}$ be a basis for $\L$. By \cref{prop:dual_integral lattice_exists}, $v_{1}^{\vee},\ldots,v_{n}^{\vee}$ is a basis for $\L^{\vee}$. If $P$ is the associated generator matrix for $\L$ then $P$ is the base change matrix from $e_{1},\ldots,e_{n}$ to $v_{1},\ldots,v_{n}$. Hence $(P^{-1})^{t}$ is the base change matrix from $e_{1},\ldots,e_{n}$ to $v_{1}^{\vee},\ldots,v_{n}^{\vee}$. The claim follows by \cref{prop:base_change_quotient_determinant}.
    \end{proof}
    
    We now turn to the case when $V$ is an $n$-dimensional real inner product space with inner product $\<\cdot,\cdot\>$. We let $d\l$ be the associated Lebesgue measure on $V$. If $v_{1},\ldots,v_{n}$ is a basis of $V$ so that any $v \in V$ has the form
    \[
      v = t_{1}v_{1}+\cdots+t_{n}v_{n},
    \]
    for some $t_{i} \in \R$ for $1 \le i \le n$, the Lebesgue measure $d\l$ is $dt_{1} \cdots dt_{n}$. Then
    \[
      \Vol(X) = \int_{X}\,d\l = \int_{X}\,dt_{1} \cdots dt_{n},
    \]
    is the volume of any measurable subset $X \subseteq V$. Note that $\L$ acts on $V$ by automorphisms given by translation. That is, we have a group action
    \[
      \L \x V \to V \qquad (\l,v) \mapsto \l+v.
    \]
    Moreover, $\L$ acts properly discontinuously on $V$ (see \cref{append:Fundamental_Domains}). To see this, let $v \in V$ and let $\d_{v}$ be such that $0 < \d_{v} < \min_{1 \le i \le n}(v-v_{i})$. Then taking $U_{v}$ to be the ball of radius $\d_{v}$ about $v$, the intersection $\l+U_{v} \cap U_{v}$ is empty unless $\l = 0$. As $\L$ is also discrete, it follows by \cref{prop:quotient_by_discrete_property_discontinuously} that $V/\L$ is also connected Hausdorff (recall that $V$ is connected Hausdorff). In particular, $V/\L$ admits a fundamental domain
    \[
      \mc{M} = \{t_{1}v_{1}+\cdots+t_{n}v_{n} \in V:\text{$0 \le t_{i} \le 1$ for $1 \le i \le n$}\}.
    \]
    Indeed, since $\L$ acts by translations, it is obvious that $\mc{M}$ is a fundamental domain for $V/\L$. Moreover, any translation of $\mc{M}$ by an element of $\L$ is also a fundamental domain. As we might expect, the covolume of $\L$ is equal to the volume of $\mc{M}$:

    \begin{proposition}\label{prop:covolume_equals_volume_of_fundamental_domain}
      Let $\L$ be a complete integral lattice in a finite dimensional real inner product space $V$ and let $\mc{M}$ be a fundamental domain for $\L$. Then
      \[
        V_{\L} = \Vol(\mc{M}).
      \]
    \end{proposition}
    \begin{proof}
      Let $n$ be the dimension of $V$, $e_{1},\ldots,e_{n}$ be an orthonormal basis of $V$, and $v_{1},\ldots,v_{n}$ be a basis for $\L$. The change of variables
      \[
        x_{1}e_{1}+\cdots+x_{n}e_{n} \mapsto x_{1}v_{1}+\cdots+x_{n}v_{n},
      \]
      has the generator matrix $P$ for $v_{1},\ldots,v_{n}$ as its Jacobian matrix since $P$ is the base change matrix from $e_{1},\ldots,e_{n}$ to $v_{1},\ldots,v_{n}$. Then
      \[
        \Vol(\mc{M}) = \int_{\mc{M}}\,d\mathbf{x} = |\det(P)|\int_{[0,1]^{n}}\,d\mathbf{x} = |\det(P)| = V_{\L},
      \]
      where the last equality follows by \cref{prop:base_change_quotient_determinant}.
    \end{proof}

    From \cref{prop:covolume_equals_volume_of_fundamental_domain} we see that the covolume of a complete integral lattice in a real inner product space is a measure of the density of the integral lattice. The smaller the covolume the smaller the fundamental domain and the more dense the integral lattice is. It turns out that for $V$, being a integral lattice is equivalent to being a discrete subgroup:

    \begin{proposition}\label{prop:integral lattice_if_and_only_if_discrete_subgroup}
      Let $\L$ be a subset of a finite dimensional real inner product space $V$. Then $\L$ is a integral lattice if and only if it is a discrete subgroup.
    \end{proposition}
    \begin{proof}
      It is clear that if $\L$ is a integral lattice then it is a discrete subgroup. So suppose $\L$ is a discrete subgroup. Then $\L$ is closed (since $V$ is a metric space). Let $V'$ be the subspace spanned by $\L$ and let its dimension be $m$. Choosing a basis $v'_{1},\ldots,v'_{m}$ of $V'$, set
      \[
        \L' = \Z v'_{1}+\cdots+\Z v'_{m}.
      \]
      Then $\L'$ is a complete integral lattice in $V'$ and $\L' \subseteq \L \subset V$. We claim that $\L/\L'$ is finite. To see this, let $\l$ run over a complete set of representatives for $\L/\L'$ and let $\mc{M}'$ be a fundamental domain for $\L'$ in $V'$. As $\mc{M}'$ is a fundamental domain, there exists unique $w' \in \mc{M}'$ and $\l' \in L$ such that $\l = w'+\l'$ for every $\l$. But then $w' = \l-\l' \in \mc{M}' \cap \L$ and since $\mc{M}' \cap \L$ is closed, discrete, and compact, it must be finite ($\mc{M}'$ is compact and $\L$ is closed and discrete). Hence there are finitely many $w'$ and thus finitely many $\l$. Letting $|\L/\L'| = q$, we have $q\L \subseteq \L'$ and therefore
      \[
        \L \subseteq \L' = \Z\frac{1}{q}v'_{1}+\cdots+\Z\frac{1}{q}v'_{m}.
      \]
      In particular, $\L$ is a subgroup of a free abelian group and therefore is free abelian. This means $\L$ is a integral lattice.
    \end{proof}

    As for complete integral lattices in $V$, they are equivalent to the existence of a bounded set whose translates cover $V$:

    \begin{proposition}\label{prop:complete_integral lattice_if_and_only_if_bounded_translates_cover}
      Let $\L$ be a integral lattice in a finite dimensional real inner product space $V$. Then $\L$ complete if and only if there exists a bounded subset $M$ of $V$ whose translates by $\L$ cover $V$.
    \end{proposition}
    \begin{proof}
      First suppose $\L$ is complete. Then we may take $M = \mc{M}$ to be the fundamental domain of $\L$ which is bounded and whose translates by $\L$ cover $V$. Now suppose $\L$ is a integral lattice and there exists a bounded subset $M$ of $V$ whose translates by $\L$ cover $V$. Let $W$ be the subspace of $V$ spanned by $\L$. Then $W$ is closed. Moreover, $\L$ is complete if and only if $V = W$ and this is what we will show. So let $v \in V$. Since the translates of $M$ by $\L$ cover $V$, for every $n \ge 1$ we may write
      \[
        nv = w_{n}+\l_{n},
      \]
      with $v_{n} \in M$ and $\l_{n} \in L \subset W$. As $M$ is bounded, $\lim_{n \to \infty}\frac{1}{n}v_{n} = 0$. Moreover, $\frac{1}{n}\l_{n} \in W$ for every $n$ and since $W$ is closed we must have that $\lim_{n \to \infty}\frac{1}{n}\l_{n}$ exists. These facts, and that $v$ is independent of $n$, together imply
      \[
        v = \lim_{n \to \infty}\frac{1}{n}w_{n}+\lim_{n \to \infty}\frac{1}{n}\l_{n} = \lim_{n \to \infty}\frac{1}{n}\l_{n},
      \]
      is an element of $W$. Since $v$ was arbitrary, $V = W$ and hence $\L$ is complete.
    \end{proof}

    The most important result we will require about integral lattices is \textbf{Minkowski's lattice point theorem}\index{Minkowski's lattice point theorem} which states that, under some mild conditions, a set of sufficiently large volume in $V$ contains a nonzero point of a complete integral lattice:

    \begin{theorem*}[Minkowski's lattice point theorem]
      Let $\L$ be a integral lattice in an $n$-dimensional real inner product space $V$. Suppose $X \subset V$ is a compact convex symmetric set. If
      \[
        \Vol(X) \ge 2^{n}V_{\L},
      \]
      then there exists a nonzero $\l \in L$ with $\l \in X$ 
    \end{theorem*}
    \begin{proof}
      We will prove the claim depending on if the inequality is strict or not. First suppose $\Vol(X) > 2^{n}V_{\L}$. Consider the linear map
      \[
        \phi:\frac{1}{2}X \to V/\L \qquad \frac{1}{2}x \mapsto \frac{1}{2}x \pmod{\L}.
      \]
      If $\phi$ were injective then
      \[
        \Vol\left(\frac{1}{2}X\right) = \frac{1}{2^{n}}\Vol(X) \le V_{\L}, 
      \]
      so that $\Vol(X) \le  2^{n}V_{\L}$. This is a contradiction, so $\phi$ cannot be injective. Hence there exists distinct $x_{1},x_{2} \in \frac{1}{2}X$ such that $\phi(x_{1}) = \phi(x_{2})$. Thus $2x_{1},2x_{2} \in X$. In particular, since $X$ is symmetric we must have $-2x_{2} \in X$. But then the fact $X$ is convex implies that
      \[
        \left(1-\frac{1}{2}\right)2x_{1}+\frac{1}{2}(-2x_{2}) = x_{1}-x_{2},
      \]
      is an element of $X$. Note that $x_{1}-x_{2} \in \L$ because $\phi(x_{1}) = \phi(x_{2})$ and $\phi$ is linear. Then $\l = x_{1}-x_{2}$ is a nonzero element of $\L$ with $\l \in X$. Now suppose $\Vol(X) = 2^{n}V_{\L}$. Then
      \[
        \Vol((1+\e)X) = (1+\e)^{n}\Vol(X) = (1+\e)^{n}2^{n}V_{\L} > 2^{n}V_{\L}.
      \]
      What we have just proved shows that there exists a nonzero $\l_{\e} \in \L$ with $\l_{\e} \in (1+\e)X$. In particular, if $\e \le 1$ then $\l_{\e} \in 2X \cap \L$. The set $2X \cap \L$ is compact and discrete, because $X$ is compact and $\L$ is discrete, and therefore is finite. But as this holds for all $\e \le 1$, the sequence $(\l_{\frac{1}{n}})_{n \ge 1}$ belongs to the finite set $2X \cap \L$ and so must converge to a point $\l$. Since $\L$ is discrete and the $\l_{\frac{1}{n}}$ are nonzero so too is $\l$. As $\l$ is an element of
      \[
        \bigcap_{n \ge 1}\left(1+\frac{1}{n}\right)X,
      \]
      and $X$ is closed, $\l \in X$ as well. Thus we have found a nonzero $\l \in L$ with $\l \in X$ and we are done.
    \end{proof}
  \section{Summation Formulas}\label{append:Summation_Formulas}
    Summation formulas are important because they often allow for a difficult to estimate sums to be replaced with one that is much easier to estimate. In fact, the most useful summation formulas replace a discrete sum with an integral of a continuous function. This allows for much more precise estimation of the original sum. We first setup some notation. If $(a_{n})_{n \ge 1}$ is a sequence of complex numbers for every $X \ge 0$ let
    \[
        A(X) = \sum_{n \le X}a_{n}.
    \]
    Note that $A(X) = 0$ unless $X \ge 1$. The most well-known summation formula is (classical) \textbf{summation by parts}\index{summation by parts}:

    \begin{theorem*}[Summation by parts, classical]
      Let $(a_{n})_{n \ge 1}$ and $(b_{n})_{n \ge 1}$ be two sequences of complex numbers. Then for any positive integers $N$ and $M$ with $1 \le M \le N$, we have
      \[
        \sum_{M \le n \le N}a_{n}(b_{n+1}-b_{n}) = (a_{N+1}b_{N+1}-a_{M}b_{M})-\sum_{M \le n \le N}b_{n+1}(a_{n+1}-a_{n}).
      \]
    \end{theorem*}
    \begin{proof}
      It is equivalent to prove that
      \[
        \sum_{M \le n \le N}a_{n}(b_{n+1}-b_{n})+\sum_{M \le n \le N}b_{n+1}(a_{n+1}-a_{n}) = (a_{N+1}b_{N+1}-a_{M}b_{M}).
      \]
      This is just a computation:
      \begin{align*}
        \sum_{M \le n \le N}a_{n}(b_{n+1}-b_{n})+\sum_{M \le n \le N}b_{n+1}(a_{n+1}-a_{n}) &= \sum_{M \le n \le N}a_{n}b_{n+1}-a_{n}b_{n}+a_{n+1}b_{n+1}-a_{n}b_{n+1} \\
        &= \sum_{M \le n \le N}a_{n+1}b_{n+1}-a_{n}b_{n} \\
        &= a_{N+1}b_{N+1}-a_{M}b_{M}.
      \end{align*}
    \end{proof}

    Replacing the sequences $(a_{n})_{n \ge 1}$ and $(b_{n})_{n \ge 1}$ with $(b_{n})_{n \ge 0}$ and $(A(n-1))_{n \ge 1}$ respectively gives (partial sum) \textbf{summation by parts}\index{summation by parts}:

    \begin{theorem*}[Summation by parts, partial sum]
      Let $(a_{n})_{n \ge 1}$ and $(b_{n})_{n \ge 0}$ be two sequences of complex numbers. Then for any integers $M$ and $N$ with $0 \le M \le N$, we have
      \[
        \sum_{M+1 \le n \le N}a_{n}b_{n} = A(N)b_{N}-A(M)b_{M}-\sum_{M \le n \le N-1}A(n)(b_{n+1}-b_{n}).
      \]
    \end{theorem*}
    \begin{proof}
      Classical summation by parts gives
      \[
        \sum_{M+1 \le n \le N}b_{n}(A(n)-A(n-1)) = A(N)b_{N+1}-A(M)b_{M+1}-\sum_{M+1 \le n \le N}A(n)(b_{n+1}-b_{n}).
      \]
      As $A(n)-A(n-1) = a_{n}$, this formula becomes
      \[
        \sum_{M+1 \le n \le N}a_{n}b_{n} = A(N)b_{N+1}-A(M)b_{M+1}-\sum_{M+1 \le n \le N}A(n)(b_{n+1}-b_{n}),
      \]
      which we can rewrite as
      \[
        \sum_{M+1 \le n \le N}a_{n}b_{n} = A(N)b_{N}-A(M)b_{M}-\sum_{M \le n \le N-1}A(n)(b_{n+1}-b_{n}).
      \]
      This is the desired formula so the proof is complete.
    \end{proof}
    
    Taking $M = 0$ gives a useful corollary:

    \begin{corollary}\label{cor:partial_sum_summation_by_parts_corollary}
      Let $(a_{n})_{n \ge 1}$ and $(b_{n})_{n \ge 0}$ be two sequences of complex numbers. Then for any positive integer $N \ge 1$, we have
      \[
        \sum_{n \le N}a_{n}b_{n} = A(N)b_{N}-\sum_{n \le N-1}A(n)(b_{n+1}-b_{n}).
      \]
    \end{corollary}
    \begin{proof}
      Taking $M = 0$ in partial sum summation by parts and observing that $A(M) = 0$ gives the result.
    \end{proof}
    
    There is another useful summation formula known as \textbf{Abel's summation formula}\index{Abel's summation formula} that lets one estimate discrete sums by integrals:

    \begin{theorem*}[Abel's summation formula]
      Let $(a_{n})_{n \ge 1}$ be a sequence of complex numbers. For every $X$ and $Y$ with $0 \le X < Y$ and continuously differentiable function $\phi:[X,Y] \to \C$, we have
      \[
        \sum_{X < n \le Y}a_{n}\phi(n) = A(Y)\phi(Y)-A(X)\phi(X)-\int_{X}^{Y}A(u)\phi'(u)\,du.
      \]
    \end{theorem*}
    \begin{proof}
      First suppose $X = M$ and $Y = N$ with $0 \le M \le N$. From partial sum summation by parts, we have
      \[
        \sum_{M+1 \le n \le N}a_{n}\phi(n) = A(N)\phi(N)-A(M)\phi(M)-\sum_{M \le n \le N-1}A(n)(\phi(n+1)-\phi(n)).
      \]
      As $\phi$ is continuously differentiable, we can express the sum on the right-hand side as
      \begin{align*}
        \sum_{M \le n \le N-1}A(n)(\phi(n+1)-\phi(n)) &= \sum_{M \le n \le N-1}A(n)\int_{n}^{n+1}\phi'(u)\,du \\
        &= \sum_{M \le n \le N-1}\int_{n}^{n+1}A(u)\phi'(u)\,du \\
        &= \int_{M}^{N}A(u)\phi'(u)\,du,
      \end{align*}
      where in the second equality we have used that $A(n)$ is constant on the interval $[n,n+1)$. Thus 
      \[
        \sum_{M+1 < n \le N}a_{n}\phi(n) = A(N)\phi(N)-A(M)\phi(M)-\int_{M}^{N}A(u)\phi'(u)\,du.
      \]
      In fact, the formula holds upon replacing $M$ with $X$ and $M$ with $Y$ since $M+1 \le n$ if and only if $X < n$ and that $A(u)$ is constant on the intervals $[n,n+1)$ for all $n \ge 0$.
    \end{proof}

    There are also some useful corollaries. On the one hand, if we let $X = 0$, so that $A(X) = 0$, we get the following:

    \begin{corollary}\label{cor:Abels_summation_formula_zero_version}
      Let $(a_{n})_{n \ge 1}$ be a sequence of complex numbers. For every $Y > 0$ and continuously differentiable function $\phi:[0,Y] \to \C$, we have
      \[
        \sum_{n \le Y}a_{n}\phi(n) = A(Y)\phi(Y)-\int_{0}^{Y}A(u)\phi'(u)\,du.
      \]
    \end{corollary}
    
    On the other hand, if we take the limit as $Y \to \infty$ we obtain:

    \begin{corollary}\label{cor:Abels_summation_formula_limit_version}
      Let $(a_{n})_{n \ge 1}$ be a sequence of complex numbers. For every $X \ge 0$ and continuously differentiable function $\phi:[X,\infty) \to \C$, we have
      \[
        \sum_{n > X}a_{n}\phi(n) = \lim_{Y \to \infty}A(Y)\phi(Y)-A(X)\phi(X)-\int_{X}^{\infty}A(u)\phi'(u)\,du.
      \]
    \end{corollary}

    Lastly, combining these cases by taking $X = 0$ and the limit as $Y \to \infty$, we arrive at the following result:

    \begin{corollary}\label{cor:Abels_summation_formula_limit_version_specialization}
      Let $(a_{n})_{n \ge 1}$ be a sequence of complex numbers. For every continuously differentiable function $\phi:[0,\infty) \to \C$, we have
      \[
        \sum_{n \ge 1}a_{n}\phi(n) = \lim_{Y \to \infty}A(Y)\phi(Y)-\int_{1}^{\infty}A(u)\phi'(u)\,du.
      \]
    \end{corollary}
  \section{Integration Techniques and Transforms}
    \subsection*{Integration Techniques}
      Complex integrals are a core backbone of many number theory techniques. An extremely useful one is called \textbf{shifting the line of integration}\index{shifting the line of integration}:

      \begin{theorem*}[Shifting the line of integration]
        Suppose we are given an integral
        \[
          \int_{\Re(z) = a}f(z)\,dz \quad \text{or} \quad \int_{\Im(z) = a}f(z)\,dz,
        \]
        and some real $b$ with $b < a$ in the first case and $b > a$ in the second case. Suppose $f(z)$ is meromorphic on a strip bounded by the lines $\Re(z) = a,b$ or $\Im(z) = a,b$ and is holomorphic about the lines $\Re(z) = a,b$ or $\Im(z) = a,b$ respectively. Moreover, suppose $f(z) \to 0$ as $y \to \infty$ or $x \to \infty$ respectively. Then
        \[
          \int_{(a)}f(z)\,dz = \int_{(b)}f(z)\,dz+2\pi i\sum_{\rho \in P}\Res_{z = \rho}f(z),
        \]
        where $P$ is the set of poles inside of the strip bounded by the lines $\Re(z) = a,b$ or $\Im(z) = a,b$ respectively.
      \end{theorem*}
      \begin{proof}
        To collect these cases, let $(a)$ stand for the line $\Re(z) = a$ or $\Im(z) = a$ respectively with positive orientation. Let $R_{T}$ be a positively oriented rectangle of height or width $T$ and with its edges on $(a)$ or $(b)$ respectively. Consider the limit
        \[
          \lim_{T \to \infty}\int_{R_{T}}f(z)\,dz.
        \]
        On the one hand, the residue theorem implies the integral is a sum of a $2\pi i$ multiple of the residues $r_{i}$ in the rectangle $R_{T}$ and hence the limit is a $2\pi i$ multiple of the sum of the residues in the strip bounded by $(a)$ and $(b)$. On the other hand, the integral can be decomposed into a sum of four integrals along the edges of $R_{T}$ and by taking the limit, the edges other than $(a)$ and $(b)$ will tend to zero because $f(z) \to 0$ as $y \to \infty$ or $x \to \infty$ respectively. What remains in the limit is the difference between the integral along $(a)$ and $(b)$. So in total,
        \[
          \int_{(a)}f(z)\,dz = \int_{(b)}f(z)\,dz+2\pi i\sum_{\rho \in P}\Res_{z = \rho}f(z).
        \]
      \end{proof}

      A particular application of interest is when the integral in question is real and over the entire real line, the integrand is entire as a complex function, and one is trying to shift the line of integration of the complexified integral to $\Im(z) = a$. In this case, shifting the line of integration amounts to making the change of variables $x \mapsto x-ia$ without affecting the initial line of integration. The second integral technique we will use is when we are summing integrals over a group and is called the \textbf{unfolding/folding method}\index{unfolding/folding method}:

      \begin{theorem*}[Unfolding/folding method]
        Suppose $f(z)$ is holomorphic on some region $\W$. Moreover, suppose $G$ is a countable group acting by automorphisms on $\W$ and let $D$ and $F$ be regions such that
        \[
          D = \bigcup_{g \in G}gF,
        \]
        where the intersections $gF \cap hF$ are measure zero for all $g,h \in G$ with respect to some $G$-invariant measure $d\mu$. Then 
        \[
          \int_{F}\sum_{g \in G}f(g z)\,d\mu = \int_{D}f(z)\,d\mu,
        \]
        provided either side is absolutely convergent.
      \end{theorem*}
      \begin{proof}
        First suppose $\int_{F}\sum_{g \in G}f(g z)\,d\mu$ converges absolutely. By the FubiniTonelli theorem, we may interchange the sum and integral. Upon making the change of variables $z \mapsto g^{-1}z$, the invariance of $d\mu$ implies that the integral takes the form
        \[
          \sum_{g \in G}\int_{gF}f(z)\,d\mu.
        \]
        As $G$ is countable and the intersections $gF \cap hF$ are measure zero, the overlap in $\bigcup_{g \in G}gF$ is also measure zero. As $D = \bigcup_{g \in G}gF$, the result follows. Of course, there is equality everywhere so we can also run the procedure in reverse provided $\int_{D}f(z)\,d\mu$ converges absolutely.
      \end{proof}

      In the unfolding/folding method, we refer to the going from the left-hand side to right-hand as \textbf{unfolding}\index{unfolding} and going from the right-hand to left-hand side as \textbf{folding}\index{folding}.
    \subsection*{The Fourier Transform}
      The first type of integral transform we will need is the Fourier transform. Suppose $f(\mathbf{x})$ is absolutely integrable on $\R^{n}$. The \textbf{Fourier transform}\index{Fourier transform} $(\mc{F}f)(\boldsymbol{\xi})$ of $f(\mathbf{x})$ is defined by
      \[
        (\mc{F}f)(\boldsymbol{\xi}) = \int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}\>}\,d\mathbf{x},
      \]
      for $\boldsymbol{\xi} \in \R^{n}$. This integral is absolutely convergent precisely because $f(\mathbf{x})$ is absolutely integrable on $\R^{n}$. The Fourier transform is intimately related to periodic functions. If $f(\mathbf{x})$ is $1$-periodic in each component and integrable on $[0,1]^{n}$ then we define the $\mathbf{n}$-th \textbf{Fourier coefficient}\index{Fourier coefficient} $\hat{f}(\mathbf{n})$ of $f(\mathbf{x})$ by
      \[
        \hat{f}(\mathbf{n}) = \int_{[0,1]^{n}}f(\mathbf{x})e^{-2\pi i\<\mathbf{n},\mathbf{x}\>}\,d\mathbf{x}.
      \]
      The \textbf{Fourier series}\index{Fourier series} of $f(\mathbf{x})$ is defined by the series
      \[
        \sum_{\mathbf{n} \in \Z^{n}}\hat{f}(\mathbf{n})e^{2\pi i\<\mathbf{n},\mathbf{x}\>}.
      \]
      There is the question of whether the Fourier series of $f(\mathbf{x})$ converges at all and if so does it even converge to $f(\mathbf{x})$ itself. Under reasonable conditions this is possible as the following proposition shows (see \cite{grafakos2008classical} for a proof):

      \begin{proposition}
        If $f(\mathbf{x})$ is smooth and $1$-periodic in each component then it converges uniformly to its Fourier series.
      \end{proposition}

      The link between the Fourier transform and Fourier series is given by the \textbf{Poisson summation formula}\index{Poisson summation formula}:

      \begin{theorem*}[Poisson summation formula]
        Suppose $\L$ is a complete integral lattice in $\R^{n}$, $f(\mathbf{x})$ is absolutely integrable on $\R^{n}$, and the function
        \[
          F(\mathbf{x}) = \sum_{\l \in \L}f(\mathbf{x}+\l),
        \]
        is locally absolutely uniformly convergent and smooth. Then
        \[
          \sum_{\l \in \L}f(\mathbf{x}+\l) = \frac{1}{V_{\L}}\sum_{\l^{\vee} \in \L^{\vee}}(\mc{F}f)(\l^{\vee})e^{2\pi i\<\l^{\vee},\mathbf{x}\>},
        \]
        and
        \[
          \sum_{\l \in \L}f(\l) = \frac{1}{V_{\L}}\sum_{\l^{\vee} \in \L^{\vee}}(\mc{F}f)(\l^{\vee}).
        \]
      \end{theorem*}
      \begin{proof}
        Fix a basis $\l_{1},\ldots,\l_{n}$ for $\L$ and let $P$ be the associated generator matrix. Then $P$ is the base change matrix from the standard basis to $\l_{1},\ldots,\l_{n}$. In particular, $P$ is an invertible linear transformation on $\R^{n}$ satisfying $\L = P\Z^{n}$ and $\L^{\vee} = (P^{-1})^{t}\Z^{n}$. Letting $ F_{P}(\mathbf{x}) = F(P\mathbf{x})$, we may write
        \[
          F_{P}(\mathbf{x}) = \sum_{\mathbf{n} \in \Z^{n}}f(P\mathbf{x}+P\mathbf{n}).
        \]
        Now $F_{P}(\mathbf{x})$ is smooth and $1$-periodic in each component because $F(\mathbf{x})$ is smooth and invariant under translation by $\L$. Therefore $F_{P}(\mathbf{x})$ admits a Fourier series. We compute the $\mathbf{t}$-th Fourier coefficient of $F_{P}(\mathbf{x})$ as follows:
        \begin{align*}
          \hat{F}_{P}(\mathbf{t}) &= \int_{[0,1]^{n}}F_{P}(\mathbf{x})e^{-2\pi i\<\mathbf{t},\mathbf{x}\>}\,d\mathbf{x} \\
          &= \int_{[0,1]^{n}}\sum_{\mathbf{n} \in \Z^{n}}f(P\mathbf{x}+P\mathbf{n})e^{-2\pi i\<\mathbf{t},\mathbf{x}\>}\,d\mathbf{x} \\
          &= \sum_{\mathbf{n} \in \Z^{n}}\int_{[0,1]^{n}}f(P\mathbf{x}+P\mathbf{n})e^{-2\pi i\<\mathbf{t},\mathbf{x}\>}\,d\mathbf{x} && \text{FTT} \\
          &= \frac{1}{V_{\L}}\sum_{\mathbf{n} \in \Z^{n}}\int_{P[0,1]^{n}}f(\mathbf{x}+P\mathbf{n})e^{-2\pi i\<\mathbf{t},P^{-1}\mathbf{x}\>} && \text{$\mathbf{x} \mapsto P^{-1}\mathbf{x}$ and \cref{prop:covolume_of_dual_is_inverse}} \\
          &= \frac{1}{V_{\L}}\sum_{\mathbf{n} \in \Z^{n}}\int_{P[0,1]^{n}}f(\mathbf{x}+P\mathbf{n})e^{-2\pi i\<\mathbf{t},P^{-1}\mathbf{x}\>}\,d\mathbf{x} \\
          &= \frac{1}{V_{\L}}\sum_{\mathbf{n} \in \Z^{n}}\int_{P[0,1]^{n}}f(\mathbf{x}+P\mathbf{n})e^{-2\pi i\left\<(P^{-1})^{t}\mathbf{t},\mathbf{x}\right\>}\,d\mathbf{x} \\
          &= \frac{1}{V_{\L}}\int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\left\<(P^{-1})^{t}\mathbf{t},\mathbf{x}\right\>}\,d\mathbf{x} && \text{FTT} \\
          &= \frac{1}{V_{\L}}(\mc{F}f)\left((P^{-1})^{t}\mathbf{t}\right).
        \end{align*}
        Therefore the definition of $F_{P}(\mathbf{x})$ and its representation as a Fourier series together give
        \[
          \sum_{\l \in \L}f(P\mathbf{x}+\l) = \frac{1}{V_{\L}}\sum_{\mathbf{t} \in \Z^{n}}(\mc{F}f)\left((P^{-1})^{t}\mathbf{t}\right)e^{2\pi i\<\mathbf{t},\mathbf{x}\>}.
        \]
        Changing variables $\mathbf{x} \mapsto P^{-1}\mathbf{x}$ and using the fact $\L^{\vee} = (P^{-1})^{t}\Z^{n}$ results in
        \[
          \sum_{\l \in \L}f(\mathbf{x}+\l) = \frac{1}{V_{\L}}\sum_{\l^{\vee} \in \L^{\vee}}(\mc{F}f)(\l^{\vee})e^{2\pi i\<\l^{\vee},\mathbf{x}\>}.
        \]
        This proves the first statement. Setting $\mathbf{x} = \mathbf{0}$ proves the second statement.
      \end{proof}

      For convenience, we state the Poisson summation formula in the simplified case for the self-dual integral lattice $\L = \Z^{n}$ as a corollary since it is how the Poisson summation formula is usually applied:

      \begin{corollary}
        Suppose $f(\mathbf{x})$ is absolutely integrable on $\R^{n}$, and the function
        \[
          F(\mathbf{x}) = \sum_{\mathbf{n} \in \Z^{n}}f(\mathbf{x}+\mathbf{n}),
        \]
        is locally absolutely uniformly convergent and smooth. Then
        \[
          \sum_{\mathbf{n} \in \Z^{n}}f(\mathbf{x}+\mathbf{n}) = \sum_{\mathbf{t} \in \Z^{n}}(\mc{F}f)(\mathbf{t})e^{2\pi i\<\mathbf{t},\mathbf{x}\>},
        \]
        and
        \[
          \sum_{\mathbf{n} \in \Z^{n}}f(\mathbf{n}) = \sum_{\mathbf{t} \in \Z^{n}}(\mc{F}f)(\mathbf{t}).
        \]
      \end{corollary}
      \begin{proof}
        This is the Poisson summation formula when $\L = \Z^{n}$ since $\Z^{n}$ is self-dual.
      \end{proof}

      In practical settings, we need a class of functions $f(\mathbf{x})$ for which the assumptions of the Poisson summation formula hold. We say that $f(\mathbf{x})$ is of \textbf{Schwarz class}\index{Schwarz class} if $f \in C^{\infty}(\R^{n})$ and $f(\mathbf{x})$ along with all of its partial derivatives have rapid decay. If $f(\mathbf{x})$ is of Schwarz class, the rapid decay implies that $f(\mathbf{x})$ and all of its derivatives are absolutely integrable over $\R^{n}$. Moreover, this also implies that $F(\mathbf{x}) = \sum_{\mathbf{n} \in \Z^{n}}f(\mathbf{x}+\mathbf{n})$ and all of its derivatives are locally absolutely uniformly convergent by the Weierstrass $M$-test. The uniform limit theorem then implies $F(\mathbf{x})$ is smooth and thus the conditions of the Poisson summation formula are satisfied. We will now derive some properties of the Fourier transform including a case specific to Schwarz class functions:

      \begin{proposition}\label{prop:Fourier_transform_properties}
        Let $f(\mathbf{x})$ and $g(\mathbf{x})$ be absolutely integrable on $\R^{n}$. Then the following hold:
        \begin{enumerate}[label*=(\roman*)]
          \item For any $\a,\b \in \R$, we have
          \[
            (\mc{F}(\a f+\b g))(\boldsymbol{\xi}) = \a(\mc{F}f)(\boldsymbol{\xi})+\b(\mc{F}g)(\boldsymbol{\xi}).
          \]
          \item If $g(\mathbf{x}) = f(\mathbf{x}+\boldsymbol{\a})$ for any $\boldsymbol{\a} \in \R^{n}$ then
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = e^{2\pi i\<\boldsymbol{\a},\boldsymbol{\xi}\>}(\mc{F}f)(\boldsymbol{\xi}).
          \]
          \item If $g(\mathbf{x}) = f(A\mathbf{x})$ for any $A \in \GL_{n}(\R)$ then
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = \frac{1}{|\det(A)|}(\mc{F}f)((A^{-1})^{t}\boldsymbol{\xi}).
          \]
          \item If $f(\mathbf{x})$ is of Schwarz class and $g(\mathbf{x}) = \frac{\del}{\del \mathbf{x}}^{\mathbf{k}}f(\mathbf{x})$ for some $\mathbf{k} \in \Z_{\ge 0}^{n}$ then
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = (2\pi i\boldsymbol{\xi})^{\mathbf{k}}(\mc{F}f).
          \]
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        We will prove the statements separately:
        \begin{enumerate}[label*=(\roman*)]
          \item This is immediate from linearity of the integral.
          \item Applying the change of variables $\mathbf{x} \mapsto \mathbf{x}-\boldsymbol{\a}$ to
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = \int_{\R^{n}}f(\mathbf{x}+\boldsymbol{\a})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}\>}\,d\mathbf{x}
          \]
          gives
          \[
            \int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}-\boldsymbol{\a}\>}\,d\mathbf{x} = e^{2\pi i\<\boldsymbol{\xi},\boldsymbol{\a}\>}\int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}\>}\,d\mathbf{x} = e^{2\pi i\<\boldsymbol{\a},\boldsymbol{\xi}\>}(\mc{F}f)(\boldsymbol{\xi}).
          \]
          \item Applying the change of variables $\mathbf{x} \mapsto A^{-1}\mathbf{x}$ to
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = \int_{\R^{n}}f(A\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}\>}\,d\mathbf{x},
          \]
          gives
          \[
            \frac{1}{|\det(A)|}\int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},A^{-1}\mathbf{x}\>}\,d\mathbf{x} = \frac{1}{|\det(A)|}\int_{\R^{n}}f(\mathbf{x})e^{-2\pi i\left\<(A^{-1})^{t}\boldsymbol{\xi},\mathbf{x}\right\>}\,d\mathbf{x} = (\mc{F}f)((A^{-1})^{t}\boldsymbol{\xi}),
          \]
          since the Jacobian matrix is $A$.
          \item We may assume $\mathbf{k} \neq \mathbf{0}$ for otherwise there is nothing to prove. Since $f(\mathbf{x})$ is of Schwarz class, so is $g(\mathbf{x})$. First suppose $n = 1$ so that $f(\mathbf{x}) = f(x)$, $g(\mathbf{x}) = g(x)$, and $\mathbf{k} = k \ge 1$. Then 
          \[
            (\mc{F}g)(\z) = \int_{-\infty}^{\infty}\frac{d^{k}}{dx^{k}}f(x)e^{-2\pi i\z x}\,dx = 2\pi i\z\int_{-\infty}^{\infty}\frac{d^{k-1}}{dx^{k-1}}f(x)e^{-2\pi i\z x}\,dx,
          \]
          where the second equality follows by an application of integration by parts and that $f(x)$ is of Schwarz class. Repeating this procedure $k-1$ times results in
          \[
            (2\pi i\z)^{k}\int_{-\infty}^{\infty}f(x)e^{-2\pi i\z x}\,dx = (2\pi i\z)^{k}(\mc{F})(\z),
          \]
          proving the case when $n = 1$. The general case follows from what we have shown by applying repeated integration by parts in each variable to
          \[
            (\mc{F}g)(\boldsymbol{\xi}) = \int_{\R^{n}}\frac{\del}{\del \mathbf{x}}^{\mathbf{k}}f(\mathbf{x})e^{-2\pi i\<\boldsymbol{\xi},\mathbf{x}\>}\,d\mathbf{x}.
          \]
        \end{enumerate}
      \end{proof}

      The properties in \cref{prop:Fourier_transform_properties} are immensely useful when computing Fourier transforms. We now introduce examples of Schwarz class functions and compute their Fourier transforms. The classic example of a Schwarz class function is $e^{-2\pi x^{2}}$. This function is particularly important because it is essentially its own Fourier transform:

      \begin{proposition}\label{prop:Fourier_transform_of_exponential_single_variable}
        Let $\a > 0$ and set $f(x) = e^{-2\pi\a x^{2}}$. Then
        \[
          (\mc{F}f)(\z) = \frac{e^{-\frac{\pi\z^{2}}{2\a}}}{\sqrt{2\a}}.
        \]
        In particular, $e^{-\pi x^{2}}$ is its own Fourier transform.
      \end{proposition}
      \begin{proof}
        Note that $f(x)$ is absolutely integrable because it exhibits exponential decay. We compute its Fourier transform
        \[
          (\mc{F}f)(\z) = \int_{-\infty}^{\infty}e^{-2\pi\a x^{2}}e^{-2\pi i\z x}\,dx = \int_{-\infty}^{\infty}e^{-2\pi\a(x^{2}+i\z x)}\,dx.
        \]
        By performing the change of variables $x \mapsto \frac{x}{\sqrt{\a}}$, the last integral becomes
        \[
          \frac{1}{\sqrt{\a}}\int_{-\infty}^{\infty}e^{-2\pi\left(x^{2}+\frac{i\z x}{\sqrt{\a}}\right)}\,dx.
        \]
        Complete the square in the exponent by observing
        \[
          x^{2}+\frac{i\z x}{\sqrt{\a}} = \left(x+\frac{i\z}{2\sqrt{\a}}\right)^{2}+\frac{\z^{2}}{4\a},
        \]
        so that the previous integral is equal to
        \[
          \frac{e^{-\frac{\pi\z^{2}}{2\a}}}{\sqrt{\a}}\int_{-\infty}^{\infty}e^{-2\pi\left(x+\frac{i\z}{2\sqrt{\a}}\right)^{2}}\,dx.
        \]
        The change of variables $x \mapsto \frac{x}{\sqrt{2}}-\frac{i\z}{\sqrt{\a}}$ is permitted without affecting the line of integration by viewing the integral as a complex integral, noting that the integrand is entire as a complex function, and shifting the line of integration. Then the integral takes the form
        \[
          \frac{e^{-\frac{\pi\z^{2}}{2\a}}}{\sqrt{2\a}}\int_{-\infty}^{\infty}e^{-\pi x^{2}}\,dx = \frac{e^{-\frac{\pi\z^{2}}{2\a}}}{\sqrt{2\a}},
        \]
        where the equality holds because the integral is $1$ since it is the Gaussian integral (see \cref{append:Special_Integrals}). This proves the first statement. The second statement follows by taking $\a = \frac{1}{2}$.
      \end{proof}

      The analog of $e^{-2\pi x^{2}}$ on $\R^{n}$ is $e^{-2\pi\<\mathbf{x},\mathbf{x}\>}$ which is clearly Schwarz class because $e^{-2\pi x^{2}}$ is. We also obtain an analog of \cref{prop:Fourier_transform_of_exponential_single_variable} for this Schwarz class function as a corollary:

      \begin{corollary}\label{cor:Fourier_transform_of_exponential_single_variable}
        Let $\a > 0$ and set $f(\mathbf{x}) = e^{-2\pi\a\<\mathbf{x},\mathbf{x}\>}$. Then
        \[
          (\mc{F}f)(\boldsymbol{\xi}) = \frac{e^{-\frac{\pi\<\boldsymbol{\xi},\boldsymbol{\xi}\>}{2\a}}}{(2\a)^{\frac{n}{2}}}.
        \]
        In particular, $e^{-\pi\<\mathbf{x},\mathbf{x}\>}$ is its own Fourier transform.
      \end{corollary}
      \begin{proof}
        Applying \cref{prop:Fourier_transform_of_exponential_single_variable} to each variable separately proves the first statement. The second statement follows upon setting $\a = \frac{1}{2}$.
      \end{proof}
    \subsection*{The Mellin Transform}
      Like the Fourier transform, the Mellin transform is another type of integral transform. If $f(\mathbf{x})$ is a continuous function on $\R_{+}^{n}$ then the \textbf{Mellin transform}\index{Mellin transform} $(\mc{M}f)(\mathbf{s})$ of $f(\mathbf{x})$ is defined by
      \[
        (\mc{M}f)(\mathbf{s}) = \int_{\R_{+}^{n}}f(\mathbf{x})\mathbf{x}^{\mathbf{s}}\,\frac{d\mathbf{x}}{\mathbf{x}},
      \]
      for $\mathbf{s} \in \C^{n}$. However, this integral is not guaranteed to converge unless specific conditions upon $f(\mathbf{x})$ are imposed. For example, if $f(\mathbf{x})$ exhibits rapid decay and remains bounded as $\mathbf{x} \to 0$ then the integral is locally absolutely uniformly convergent for $\s > 0$. We will only be interested in the case when $\mathbf{s} = (s,\ldots,s)$ so that the Mellin transform is a function on $\C$. Moreover, most of the time we will take $n = 1$ so that the initial function we are taking the Mellin transform of is defined on $\R_{+}$. There is also an inverse transform in this case. If $g(s)$ holomorphic and tends to zero as $t \to \infty$ in a vertical strip $a < \s < b$ then the inverse \textbf{inverse Mellin transform}\index{inverse Mellin transform} $(\mc{M}^{-1}g)(x)$ of $g(s)$ is given by
      \[
        (\mc{M}^{-1}g)(x) = \frac{1}{2\pi i}\int_{(c)}g(s)x^{-s}\,ds,
      \]
      for any $a < c < b$.
      It is not immediately clear that this integral converges or is independent of $c$. The following theorem makes precise what properties $f(x)$ needs to satisfy so that the inverse Mellin transform recovers $f(x)$ (see \cite{debnath2002integral} for a proof):

      \begin{theorem*}[Mellin inversion formula]
        Let $a < b$ and suppose $g(s)$ is analytic in the strip vertical $a < \s < b$, tends to zero uniformly as $t \to \infty$ along any line $\s = c$ for $a < c < b$, and that the integral of $g(s)$ along this line is locally absolutely uniformly convergent. Then if
        \[
          f(x) = \frac{1}{2\pi i}\int_{(c)}g(s)x^{-s}\,ds,
        \]
        this integral is independent of $c$ and moreover $g(s) = (\mc{M}f)(s)$. Conversely, suppose $f(x)$ is piecewise continuous such that its value is halfway between the limit values at any jump discontinuity and
        \[
          g(s) = \int_{0}^{\infty}f(x)x^{s}\,\frac{dx}{x},
        \]
        is locally absolutely uniformly convergent in the vertical strip $a < \s < b$. Then $f(x) = (\mc{M}^{-1}g)(x)$.
      \end{theorem*}
  \section{The Gamma Function}
    The gamma function is ubiquitous in number theory and the better one understands this function the better one will be at seeing the forest for the trees. The \textbf{gamma function}\index{gamma function} $\G(s)$ is defined to be the Mellin transform of $e^{-x}$:
    \[
      \G(s) = \int_{0}^{\infty}e^{-x}x^{s-1}\,dx,
    \]
    for $\s > 0$. The integral is locally absolutely uniformly convergent in this region. Indeed, let $K$ is a compact subset in this region and set $\a = \min_{s \in K}(\s)$. Then we have to show that $\G(s)$ is absolutely uniformly convergent on $K$. Now split the integral by writing
    \[
      \G(s) = \int_{0}^{1}e^{-x}x^{s-1}\,dx+\int_{1}^{\infty}e^{-x}x^{s-1}\,dx.
    \]
    The second integral is absolutely uniformly convergent on $K$ since the integrand exhibits exponential decay. As for the first integral, we have
    \[
      \int_{0}^{1}e^{-x}x^{s-1}\,dx \ll \int_{0}^{1}x^{\s-1}\,dx \ll_{\a} 1,
    \]
    so that this integral is absolutely uniformly convergent on $K$ too. Thus $\G(s)$ is absolutely uniformly convergent on $K$. Also note that $\G(s)$ is real for $s > 0$. The most basic properties of $\G(s)$ are the following:

    \begin{proposition}\label{prop:Factorial_properties_of_gamma_function}
      $\G(s)$ satisfies the following properties:
      \begin{enumerate}[label*=(\roman*)]
        \item $\G(1) = 1$.
        \item $\G(s+1) = s\G(s)$.
        \item $\G(\conj{s}) = \conj{\G(s)}$.
      \end{enumerate}
    \end{proposition}
    \begin{proof}
      We obtain (i) by direct computation:
      \[
        \G(1) = \int_{0}^{\infty}e^{-x}\,dx = 1.
      \]
      An application of integration by parts gives (ii):
      \[
        \G(s+1) = \int_{0}^{\infty}e^{-x}x^{s}\,dx = s\int_{0}^{\infty}e^{-x}x^{s-1}\,dx = s\G(s).
      \]
      For (iii), since $\G(s)$ is real for $s >0$ we have $\G(\conj{s}) = \conj{\G(s)}$ on this half-line and then the identity theorem implies that this holds everywhere.
    \end{proof}

    From \cref{prop:Factorial_properties_of_gamma_function} we see that for $s = n$ a positive integer, $\G(n) = (n-1)!$. So $\G(s)$ can be thought of as a holomorphic extension of the factorial function. We can use property (ii) of \cref{prop:Factorial_properties_of_gamma_function} to extended $\G(s)$ to a meromorphic function on all of $\C$:

    \begin{theorem}\label{thm:continuation_of_gamma_function}
      $\G(s)$ admits meromorphic continuation to $\C$ with poles at $s = -n$ for $n \ge 0$. All of these poles are simple and with residue $\frac{(-1)^{n}}{n!}$ at $s = -n$.
    \end{theorem}
    \begin{proof}
      Using \cref{prop:Factorial_properties_of_gamma_function}, (ii) repeatedly, for any integer $n \ge 0$ we have
      \[
        \G(s) = \frac{\G(s+1+n)}{s(s+1) \cdots (s+n)}.
      \]
      The right-hand side defines an meromorphic function in the region $\s > -n$ and away from the points $0,-1,\ldots,-n$. Letting $n$ be arbitrary, we see that $\G(s)$ has meromorphic continuation to $\C$ with poles at $0,-1,-2,\ldots$. We now compute the residue at $s = -n$. Around this point $\G(s)$ admits meromorphic continuation with representation
      \[
        \frac{\G(s+1+n)}{s(s+1) \cdots (s+n)},
      \]
      where all of the factors except for $s+n$ are holomorphic at $s = -n$. Thus the pole is simple, and
      \[
        \Res_{s = -n}\G(s) = \lim_{z \to -n}\frac{\G(s+1+n)(s+n)}{s(s+1) \cdots (s+n)} = \frac{\G(1)}{(-n)(1-n) \cdots (-1)} = \frac{(-1)^{n}}{n!}.
      \]
    \end{proof}

    In particular, \cref{thm:continuation_of_gamma_function} implies $\Res_{s = 0}\G(s) = 1$ and $\Res_{s = 1}\G(s) = -1$. There are a few other properties of the gamma function that are famous and which we will use frequently. The first of which is the \textbf{Legendre duplication formula}\index{Legendre duplication formula} (see \cite{remmert1998classical} for a proof):

    \begin{theorem*}[Legendre duplication formula]
      For any $s \in \C-\{0,-1,-2,\ldots\}$,
      \[
        \G(s)\G\left(s+\frac{1}{2}\right) = 2^{1-2s}\sqrt{\pi}\G(2s).
      \]
    \end{theorem*}

    As a first application, we can use this formula to compute $\G\left(\frac{1}{2}\right)$. Letting $z = \frac{1}{2}$ in the Legendre duplication formula and recalling $\G(1) = 1$, we see that $\G\left(\frac{1}{2}\right) = \sqrt{\pi}$. There is also the important Hadamard factorization of the reciprocal of $\G(s)$ (see \cite{stein2003complex} for a proof):

    \begin{proposition}\label{prop:Hadamard_factorization_for_reciprocial_of_gamma}
      For all $s \in \C$,
      \[
        \frac{1}{\G(s)} = se^{\g s}\prod_{n \ge 1}\left(1+\frac{s}{n}\right)e^{-\frac{s}{n}},
      \]
      where $\g$ is the Euler-Mascheroni constant.
    \end{proposition}

    In particular, $\frac{1}{\G(s)}$ is entire so that $\G(s)$ is nowhere vanishing on $\C$. Also, $\frac{1}{\G(s)}$ is of order $1$ (see \cref{append:Factorizations_and_Finite_Order}). In particular, this means that $\G(s)$ is also order $1$ for $\s > 0$. We call $\frac{\G'}{\G}(s)$ the \textbf{digamma function}\index{digamma function}. Equivalently, the digamma function is the logarithmic derivative of the gamma function. Note that upon taking the logarithmic derivative of $\G(s+1) = s\G(s)$, we see that the digamma function satisfies the related formula
    \[
      \frac{\G'}{\G}(s+1) = \frac{\G'}{\G}(s)+\frac{1}{s}.
    \]
    If we take the logarithmic derivative of the Hadamard factorization for $\frac{1}{s\G(s)}$, we obtain a useful expression for the digamma function:

    \begin{corollary}\label{cor:logarithmic_derivative_of_gamma}
      For all $s \in \C$,
      \[
        \frac{\G'}{\G}(s+1) = -\g+\sum_{n \ge 1}\left(\frac{1}{n}-\frac{1}{s+n}\right),
      \]
      where $\g$ is the Euler-Mascheroni constant. In particular, the digamma function has simple poles of residue $-1$ at the poles of the gamma function.
    \end{corollary}
    \begin{proof}
      By \cref{prop:Factorial_properties_of_gamma_function} (ii), $\frac{1}{\G(s+1)} = \frac{1}{s\G(s)}$. Taking the logarithmic derivative using \cref{prop:Hadamard_factorization_for_reciprocial_of_gamma} we obtain
      \[
        -\frac{\G'}{\G}(s+1) = \g+\sum_{n \ge 1}\left(\frac{1}{s+n}-\frac{1}{n}\right),
      \]
      provided $s$ is distance $\e$ away from the poles of $\G(s)$. This is the desired formula and the statement regarding the poles follows immediately.
    \end{proof}
    
    We will also require a well-known approximation for the gamma function known as \textbf{Stirling's formula}\index{Stirling's formula} (see \cite{remmert1998classical} for a proof):

    \begin{theorem*}[Stirling's formula]
      \phantom{}
      \[
        \G(s) \sim \sqrt{2\pi}s^{s-\frac{1}{2}}e^{-s},
      \]
      provided $|\arg(s)| < \pi-\e$ and $|s| > \d$ for some $\e,\d > 0$.
    \end{theorem*}

    If $\s$ is bounded, Stirling's formula gives a useful asymptotic showing that $\G(s)$ decays as $s \to \infty$:
    
    \begin{corollary}\label{equ:weaker_Stirling_formula}
    Let $|\arg(s)| < \pi-\e$ and $|s| > \d$ for some $\e,\d > 0$. Then if $\s$ is bounded, we have
      \[
        \G(s) \sim \sqrt{2\pi}t^{\s-\frac{1}{2}}e^{-\frac{\pi}{2}|t|}.
      \]
    \end{corollary}
    \begin{proof}
      Stirling's formula can be equivalently expressed as
      \[
        \G(s) \sim \sqrt{2\pi}(\s+it)^{\s-\frac{1}{2}+it}e^{-\s-it}.
      \]
      Since $\s$ is bounded, $e^{-\s-it} \ll 1$ and we obtain the simplified asymptotic
      \[
        \G(s) \sim \sqrt{2\pi}(it)^{\s-\frac{1}{2}+it}.
      \]
      Similarly, $x$ being bounded implies $i^{\s-\frac{1}{2}} \ll 1$ and we compute
      \[
        (it)^{it} = e^{i|t|\log(i|t|)} = e^{i|t|(\log(i)+\log|t|)} = e^{-\frac{\pi}{2}|t|+i|t|\log|t|} \sim e^{-\frac{\pi}{2}|t|},
      \]
      where we have used the fact that $\log(i) = i\frac{\pi}{2}$. Together, we obtain the further simplified asymptotic
      \[
        \G(s) \sim \sqrt{2\pi}t^{\s-\frac{1}{2}}e^{-\frac{\pi}{2}|t|},
      \]
      which is the desired result
    \end{proof}
    Equivalent to Stirling's formula is the asymptotic
    \begin{equation}\label{equ:Stirlings_formula_equivalent_version}
        \G(s) = \sqrt{2\pi}s^{s-\frac{1}{2}}e^{-s}(1+O_{\e,\d}(1)),
    \end{equation}
    provided $|\arg(s)| < \pi-\e$ and $|s| > \d$ for some $\e,\d > 0$. Taking the logarithm (since $|\arg(s)| < \pi-\e$ the logarithm is defined) of this asymptotic gives
    \begin{equation}\label{equ:log_gamma_estimate}
      \log\G(s) = \frac{1}{2}\log(2\pi)+\left(s-\frac{1}{2}\right)\log(s)-s+O_{\e,\d}\left(1\right),
    \end{equation}
    which will be useful. In fact, from \cref{equ:log_gamma_estimate} we can obtain another useful estimate formula for the digamma function:

    \begin{proposition}\label{equ:approximtion_for_digamma}
    \[
      \frac{\G'}{\G}(s) = \log(s)+O_{\e,\d}(1),
    \]
    provided $|\arg(s)| < \pi-\e$ and $|s| > \d$ for some $\e,\d > 0$.
    \end{proposition}
    \begin{proof}
      \cref{equ:log_gamma_estimate} give the simplified estimate
      \[
        \log\G(s) = \frac{1}{2}\log(2\pi)+s\log(s)-s+O_{\e,\d}(1).
      \]
      Set $g(s) = \frac{1}{2}\log(2\pi)+s\log(s)-s$ so that $\log\G(s) = g(s)+O_{\e,\d}(1)$. Then $\log\G(s)-g(s) = O_{\e,\d}(1)$, and by Cauchy's integral formula, we have
      \begin{align*}
        \frac{\G'}{\G}(s) &= \frac{d}{ds}\left(g(s)+O_{\e,\d}(1)\right) \\
        &= g'(s)+\frac{d}{ds}(\log\G(s)-g(s)) \\
        &= \log(s)+\frac{1}{2\pi i}\int_{C}\frac{\log\G(u)-g(u)}{(u-s)^{2}}\,du,
      \end{align*}
      where $C$ is the circle about $s$ of sufficiently small radius $\eta$ depending upon $\e$ and $\d$. Therefore
      \[
        \left|\frac{\G'}{\G}(s)-\log(s)\right| \le \frac{1}{2\pi}\int_{C}\frac{|\log\G(u)-g(u)|}{\eta^{2}}\,|du| \ll_{\e,\d} 1,
      \]
      where the last estimate follows because $\log\G(s)-g(s) = O_{\e,\d}(1)$.
    \end{proof}

    Lastly, we introduce a useful function related to the Gamma function. We define the \textbf{beta function}\index{beta function} $B(s,u)$ by
    \[
        B(s,u) = \int_{0}^{1}t^{s-1}(1-t)^{u-1}\,dt,
    \]
    for $\s > 0$ and $\tau > 0$. The integral is locally absolutely uniformly convergent in this region. Indeed, let $K \x L$ be a compact subset in this region and set $\a = \min_{s \in K}(\s)$ and $\b = \min_{u \in L}(\tau)$. Then we have to show that $B(s,u)$ is absolutely uniformly convergent on $K \x L$. Observe
    \[
      \int_{0}^{1}t^{s-1}(1-t)^{u-1}\,dt \ll \int_{0}^{1}t^{\a-1}(1-t)^{\b-1}\,dt \ll_{\a,\b} 1,
    \]
    so that this integral is absolutely uniformly convergent on $K \x L$. Thus $B(s,u)$ is absolutely uniformly convergent on $K \x L$. Most importantly, the beta function is related to the gamma function in this region (see \cite{remmert1998classical} for a proof):
    
    \begin{proposition}\label{prop:integral_reprepsentation_for_beta_function}
      For $\Re(s) > 0$ and $\Re(u) > 0$,
      \[
        B(s,u) = \frac{\G(s)\G(u)}{\G(s+u)}.
      \]
    \end{proposition}

    In particular, \cref{prop:integral_reprepsentation_for_beta_function} shows that $B(s,u)$ has meromorphic continuation to $\C^{2}$ since the gamma function does by \cref{thm:continuation_of_gamma_function}.