\chapter{Conventional Preliminaries}\label{ch:Preliminaries}
  Here we setup notational conventions for the remainder of the text and define asymptotic notation.
  \section{Notations}
    Here we make some notational conventions that will be used throughout the rest of the text unless specified otherwise:
    \begin{itemize}
      \item The symbols $\subset$ and $\supset$ denote strict containment.
      \item Any ring is understood to be a commutative ring with $1$ and a subring is understood to contain $1$.
      \item The finite field with $p$ elements $\F_{p}$ stands for $\Z/p\Z$.
      \item Primes and divisors of integers are taken to be positive.
      \item For any $x \in \R$, $\lfloor x \rfloor$, $\lceil x \rceil$, and $\{x\}$ denotes the floor, ceiling, and fractional part of $x$ respectively.
      \item The symbol $\e$ denotes a small positive constant ($\e > 0$) that is not necessarily the same from line to line.
      \item If $a \in (\Z/m\Z)^{\ast}$, we will always let $\conj{a}$ denote the multiplicative inverse. That is, $a\conj{a} \equiv 1 \tmod{m}$.
      \item In a metric space $X$, a neighborhood about $x \in X$ is always taken to mean an open set about $x$.
      \item By analytic we mean real analytic or complex analytic accordingly.
      \item For the complex variables $z$, $s$, and $u$, we write
      \[
        z = x+iy, \quad s = \s+it, \quad \text{and} \quad u = \tau+ir,
      \]
      for the real and imaginary parts of these variables respectively unless specified otherwise. Moreover, in certain expressions we often write $\Im(z)$ for clarity.
      \item For $\mathbf{z} \in \C^{n}$, we write
      \[
        \mathbf{z} = \mathbf{x}+i\mathbf{y},
      \]
      for the real and imaginary parts of $\mathbf{z}$ respectively unless specified otherwise. Moreover,
      \[
        \mathbf{z}^{-1} = (z_{1}^{-1},\ldots,z_{n}^{-1}) = \frac{1}{\mathbf{z}},
      \]
      provided $z_{i}$ is nonzero for $1 \le i \le n$. Also,
      \[
        \a\mathbf{z} = (\a z_{1},\ldots,\a z_{n}) \quad \text{and} \quad \mathbf{z}^{\a} = (z_{1}^{\a},\ldots,z_{n}^{\a}),
      \]
      for all $\a,\b \in \C$. Lastly,
      \[
        \mathbf{z}^{\mathbf{w}} = z_{1}^{w_{1}} \cdots z_{n}^{w_{n}}, \quad \text{and} \quad \mathbf{z}\mathbf{w} = (z_{1}w_{1},\ldots,z_{n}w_{n}),
      \]
      for all $\mathbf{w} = (w_{1},\ldots,w_{n}) \in \C^{n}$.
      \item If $r \in \Z$ denotes the order of a possible pole of a complex function, $r \ge 0$ if it is a pole and $r \le 0$ if it is a zero.
      \item The nontrivial zeros of an $L$-function will be denoted by $\rho = \b+i\g$ unless specified otherwise.
      \item $\log$ will always stand for the principal branch of the logarithm while $\log_{0}$ will always stand for the branch cut of the logarithm along the positive real axis.
      \item For a sum $\sum$ over integers satisfying a congruence condition, $\sum^{'}$ will denote the sum restricted to relatively prime integers satisfying the same congruence.
      \item All integrals are taken with positive orientation.
      \item We will write $\int_{(a)}$ for the complex integral over the line whose real part is $a$ and with positive orientation.
      \item $\d_{a,b}$ will denote the indicator function for $a = b$. That is, $\d_{a,b} = 1,0$ according to if $a = b$ or not.
    \end{itemize}
  \section{Asymptotics}
    Throughout we assume $f,g:\R^{n} \to \C$. Much of the language of number theory is given in terms of asymptotics (or estimates or bounds) as they allows us to discuss approximate growth and dispense with superfluous constants. For this reason, asymptotics will be the first material that we will present. The asymptotics that we will cover are listed in the following table:
    \begin{center}
      \begin{stabular}[1.5]{|c|c|c|}
        \hline
        Asymptotics & Notation \\
        \hline
        Big O & $f(\mathbf{x}) = O(g(\mathbf{x}))$ \\
        \hline
        Vinogradov's symbol & $f(\mathbf{x}) \ll g(\mathbf{x})$ \\
        \hline
        Order of magnitude symbol & $f(\mathbf{x}) \asymp g(\mathbf{x})$ \\
        \hline
        Little o & $f(\mathbf{x}) = o(g(\mathbf{x}))$ \\
        \hline
        Asymptotic equivalence & $f(\mathbf{x}) \sim g(\mathbf{x})$ \\
        \hline
        Omega symbol & $f(\mathbf{x}) = \W(g(\mathbf{x}))$ \\
        \hline
      \end{stabular}
    \end{center}
    Implicit in all of these asymptotics is some limiting process $\mathbf{x} \to \mathbf{x}_{0}$ where $\mathbf{x}_{0}$ is finite or $\infty$. If $\mathbf{x}_{0}$ is finite then it is understood that the asymptotic is assumed to hold for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. If $\mathbf{x}_{0}$ is infinite then the asymptotic is assumed to hold for sufficiently large $\mathbf{x}$. If the limiting process is not explicitly mentioned, it is assumed to be as $\mathbf{x} \to \infty$. Often times, asymptotics will hold for all admissible values of $\mathbf{x}$ and this will be clear from context although we still might suppress the specific limiting process.

    \begin{remark}
      If $f,g: \C^{n} \to \C$ then the following theory still holds by identifying $\C^{n} \cong \R^{2n}$. Moreover, if $f,g:\Z_{+}^{n} \to \C$ then by extending $f(\mathbf{n})$ and $g(\mathbf{n})$ to $\R^{n}$ by making them piecewise linear, so that they are piecewise continuous, the following theory still holds with $\mathbf{n}$ in place of $\mathbf{x}$. In particular, we may take $f(\mathbf{x})$ or $g(\mathbf{x})$ to be a constant function.
    \end{remark}

    Implicit in some asymptotics will be a constant (such constants are in general not unique and any sufficiently large constant will do). Any such constant is called the \textbf{implicit constant}\index{implicit constant} of the asymptotic. The implicit constant may depend on one or more parameters, $\e$, $\s$, etc. If we wish to make these dependencies known, we use subscripts. If it is possible to choose the implicit constant independent of a certain parameter then we say that the asymptotic is \textbf{uniform}\index{uniform} with respect to that parameter. Moreover, we say that an implicit constant is \textbf{effective}\index{effective} if the constant is numerically computable and \textbf{ineffective}\index{ineffective} otherwise. Moreover, if we are interested in the dependence of an asymptotic on a certain parameter, say $p$, we will refer to the \textbf{$p$-aspect}\index{$p$-aspect} to mean the part of the asymptotic that is dependent upon $p$.
    \subsection*{\texorpdfstring{$O$}{O}-estimates and Symbols}
      We say $f(\mathbf{x})$ \textbf{is of order}\index{is of order} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = O(g(\mathbf{x}))$
      if there is some positive constant $c$ such that
      \[
        |f(\mathbf{x})| \le c|g(\mathbf{x})|,
      \]
      as $\mathbf{x} \to \mathbf{x}_{0}$. We call this an \textbf{$O$-estimate}\index{$O$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth at most}\index{growth at most} $g(\mathbf{x})$. The $O$-estimate says that for $\mathbf{x}$ close to $\mathbf{x}_{0}$, the size of $f(\mathbf{x})$ grows like $g(\mathbf{x})$.
      
      \begin{remark}
        Many authors assume that $g(\mathbf{x})$ is a nonnegative function so that the absolute value on $g(\mathbf{x})$ can be dropped. As we require asymptotics that will be used more generally, we do not make this assumption since one could very well replace $O(g(\mathbf{x}))$ with $O(|g(\mathbf{x})|)$. In practice this deviation causes no issue.
      \end{remark}
      
      As a symbol, let $O(g(\mathbf{x}))$ stand for a function $f(\mathbf{x})$ that is $O(g(\mathbf{x}))$. Then we may use the $O$-estimates in algebraic equations and inequalities. Note that this extends the definition of the symbol because $f(\mathbf{x}) = O(g(\mathbf{x}))$ means $f(\mathbf{x})$ is $O(g(\mathbf{x}))$. The symbol $\ll$ is known as \textbf{Vinogradov's symbol}\index{Vinogradov's symbol} and it is an alternative way to express $O$-estimates. We write $f(\mathbf{x}) \ll g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ if $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$. We also write $f(\mathbf{x}) \gg g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ to mean $g(\mathbf{x}) \ll f(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$. If there is a dependence of the implicit constant on parameters, we use subscripts to denote dependence on these parameters. If both $f(\mathbf{x}) \ll g(\mathbf{x})$ and $g(\mathbf{x}) \ll f(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ then we say $f(\mathbf{x})$ and $g(\mathbf{x})$ have the \textbf{same order of magnitude}\index{same order of magnitude} and write $f(\mathbf{x}) \asymp g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$. We also say $f(\mathbf{x})$ has \textbf{growth}\index{growth} $g(\mathbf{x})$. If there is a dependence of the implicit constant on parameters, we use subscripts to denote dependence on these parameters. From the definition of the $O$-estimate, this is equivalent to the existence of positive constants $c_{1}$ and $c_{2}$ such that
      \[
        c_{1}|g(\mathbf{x})| \le |f(\mathbf{x})| \le c_{2}|g(\mathbf{x})|.
      \]
      Equivalently, we can interchange $f(\mathbf{x})$ and $g(\mathbf{x})$ in the above equation.
    \subsection*{\texorpdfstring{$o$}{o}-estimates and Symbols}
      We say $f(\mathbf{x})$ \textbf{is of smaller order than}\index{is of smaller order than} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = o(g(\mathbf{x}))$ if
      \[
        \lim_{\mathbf{x} \to \mathbf{x}_{0}}\left|\frac{f(\mathbf{x})}{g(\mathbf{x})}\right| = 0,
      \]
      provided $g(\mathbf{x})$ is nonzero for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{$o$-estimate}\index{$o$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth less than}\index{growth less than} $g(\mathbf{x})$. The $o$-estimate says that for $\mathbf{x}$ close to $\mathbf{x}_{0}$, $g(\mathbf{x})$ dominates $f(\mathbf{x})$. If $f(\mathbf{x}) = o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ where the implicit constant can be taken arbitrarily small by definition of the $o$-estimate. Therefore, $o$-estimates are stronger than $O$-estimates. As a symbol, let $o(g(\mathbf{x}))$ stand for a function $f(\mathbf{x})$ that is $o(g(\mathbf{x}))$. Then we may use the $o$-estimates in algebraic equations and inequalities. Note that this extends the definition of the symbol because $f(\mathbf{x}) = o(g(\mathbf{x}))$ means $f(\mathbf{x})$ is $o(g(\mathbf{x}))$. Moreover, we say that $f(\mathbf{x})$ admits an \textbf{asymptotic formula}\index{asymptotic formula} as $\mathbf{x} \to \mathbf{x}_{0}$ if $f(\mathbf{x})$ is nonzero for all $\mathbf{x}$ sufficiently close $\mathbf{x}$ in norm and there exists functions $M(\mathbf{x})$, $E(\mathbf{x})$, and $R(\mathbf{x})$ such that
      \[
        f(\mathbf{x}) = M(\mathbf{x})+O(E(\mathbf{x})) \quad \text{or} \quad f(\mathbf{x}) = M(\mathbf{x})\left(1+O(R(\mathbf{x}))\right),
      \]
      as $\mathbf{x} \to \mathbf{x}_{0}$ where $E(\mathbf{x}) = o(M(\mathbf{x}))$ or $R(\mathbf{x}) = o(1)$ as $\mathbf{x} \to \mathbf{x}_{0}$ too. Accordingly, we call $M(\mathbf{x})$ the \textbf{main term}\index{main term}, $E(\mathbf{x})$ the \textbf{(absolute) error term}\index{(absolute) error term}, and $R(\mathbf{x})$ the \textbf{(relative) error term}\index{(relative) error term} respectively. These formulas are equivalent since $E(\mathbf{x})$ and $R(\mathbf{x})$ can be obtained from each other via the relation
      \[
        E(\mathbf{x}) = M(\mathbf{x})R(\mathbf{x}),
      \]
      as $M(\mathbf{x})$ is nonzero for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm because the same is assumed of $f(\mathbf{x})$. Therefore we only need to specify one of these formulas.
    
      \begin{remark}\label{rem:asymptotic_formula_erros}
        The asymptotic formulas
        \[
          f(\mathbf{x}) = M(\mathbf{x})+O(E(\mathbf{x})) \quad \text{and} \quad f(\mathbf{x}) = M(\mathbf{x})\left(1+O(R(\mathbf{x}))\right),
        \]
        are equivalent to
        \[
          f(\mathbf{x})-M(\mathbf{x}) = O(E(\mathbf{x})) \quad \text{and} \quad \frac{f(\mathbf{x})}{M(\mathbf{x})} = 1+O(R(\mathbf{x})),
        \]
        which shows that the absolute and relative errors between $f(\mathbf{x})$ and $M(\mathbf{x})$ are $O(E(\mathbf{x}))$ and $O(R(\mathbf{x}))$ respectively.
      \end{remark}
      
      We say $f(\mathbf{x})$ \textbf{is asymptotic to}\index{is asymptotic to} $g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) \sim g(\mathbf{x})$ if
      \[
        \lim_{\mathbf{x} \to \mathbf{x}_{0}}\frac{f(\mathbf{x})}{g(\mathbf{x})} = 1,
      \]
      provided $g(\mathbf{x})$ is nonzero for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{asymptotic equivalence}\index{asymptotic equivalence} and say that $f(\mathbf{x})$ and $g(\mathbf{x})$ are \textbf{asymptotically equivalent}\index{asymptotically equivalent}. It is useful to think of asymptotic equivalence as $f(\mathbf{x})$ and $g(\mathbf{x})$ being the same size in the limit as $\mathbf{x} \to \mathbf{x}_{0}$. Immediately from the definition, we see that this is an equivalence relation on functions. In particular, if $f(\mathbf{x}) \sim g(\mathbf{x})$ and $g(\mathbf{x}) \sim h(\mathbf{x})$ then $f(\mathbf{x}) \sim h(\mathbf{x})$. Also, if $f(\mathbf{x}) \sim g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $f(\mathbf{x}) \asymp g(\mathbf{x})$ as $\mathbf{x} \to \mathbf{x}_{0}$ with $c_{1} \le 1 \le c_{2}$. So asymptotic equivalence is stronger than being of the same order of magnitude. In addition, if we have an asymptotic formula for $f(\mathbf{x})$ with main term $M(\mathbf{x})$ then \cref{rem:asymptotic_formula_erros} shows $f(\mathbf{x}) \sim M(\mathbf{x})$. Therefore an asymptotic formula is stronger than asymptotic equality. Also note that $f(\mathbf{x}) \sim g(\mathbf{x})$ is equivalent to $f(\mathbf{x}) = g(\mathbf{x})(1+o(1))$ and hence implies $f(\mathbf{x}) = g(\mathbf{x})(1+O(1))$. We say $f(\mathbf{x})$ \textbf{is of larger order than}\index{is of larger order than} $g(\mathbf{x})$ or $f(\mathbf{x})$ is $\W(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and write $f(\mathbf{x}) = \W(g(\mathbf{x}))$ if
      \[
        \limsup_{\mathbf{x} \to \mathbf{x}_{0}}\left|\frac{f(\mathbf{x})}{g(\mathbf{x})}\right| > 0.
      \]
      provided $g(\mathbf{x})$ is nonzero for all $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm. We call this an \textbf{$\W$-estimate}\index{$\W$-estimate} and say that $f(\mathbf{x})$ has \textbf{growth at least}\index{growth at least} $g(\mathbf{x})$. Observe that $f(\mathbf{x}) = \W(g(\mathbf{x}))$ is precisely the negation of $f(\mathbf{x}) = o(g(\mathbf{x}))$, so that $f(\mathbf{x}) = \W(g(\mathbf{x}))$ means $f(\mathbf{x}) = o(g(\mathbf{x}))$ is false. This is weaker than $f(\mathbf{x}) \gg g(\mathbf{x})$ because $f(\mathbf{x}) = \W(g(\mathbf{x}))$ means $|f(\mathbf{x})| \ge c|g(\mathbf{x})|$ for some values of $\mathbf{x}$ arbitrarily close to $\mathbf{x}_{0}$ whereas $f(\mathbf{x}) \gg g(\mathbf{x})$ means $|f(\mathbf{x})| \ge c|g(\mathbf{x})|$ for all values of $\mathbf{x}$ sufficiently close to $\mathbf{x}_{0}$ in norm.
    \subsection*{Algebraic Manipulation for \texorpdfstring{$O$}{O}-estimates and \texorpdfstring{$o$}{o}-estimates}
      Asymptotics become increasingly more useful when we can use them in equations to represent approximations. We catalogue some of the most useful algebraic manipulations for $O$-estimates and $o$-estimates. Most importantly, if an algebraic equation involves a $O$-estimate or $o$-estimate then it is understood that the equation is not symmetric and is interpreted to be read from left to right. That is, any function of the form satisfying the estimate on the left-hand side also satisfies the estimate on the right-hand side too. We begin with $O$-estimates. The trivial algebraic manipulations are collected in the proposition below:

      \begin{proposition}\label{prop:Big_Oh_manipulations}
          The following $O$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
          \begin{enumerate}[label*=(\roman*)]
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ and $g(\mathbf{x}) = O(h(\mathbf{x}))$ then $f(\mathbf{x}) = O(h(\mathbf{x}))$. Equivalently, $O(O(h(\mathbf{x}))) = O(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = O(g_{i}(\mathbf{x}))$ for $i = 1,2$ then $f_{1}(\mathbf{x})f_{2}(\mathbf{x}) = O(g_{1}(\mathbf{x})g_{2}(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x})h(\mathbf{x}))$ then $f(\mathbf{x}) = g(\mathbf{x})O(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = O(g_{i}(\mathbf{x}))$ for $i = 1,2,\ldots,n$ then $\sum_{1 \le i \le n}f_{i}(\mathbf{x}) = O\left(\sum_{1 \le i \le n}|g_{i}(\mathbf{x})|\right)$.
            \item If $f_{n}(\mathbf{x}) = O(g_{n}(\mathbf{x}))$ for $n \ge 1$ then $\sum_{n \ge 1}f_{n}(\mathbf{x}) = O\left(\sum_{n \ge 1}|g_{n}(\mathbf{x})|\right)$ provided both $\sum_{n \ge 1}f_{n}(\mathbf{x})$ and $\sum_{n \ge 1}|g_{n}(\mathbf{x})|$ converge.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and $T(\mathbf{x})$ is such that $T(\mathbf{x}) \to \mathbf{x}_{0}$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $(f \circ T)(\mathbf{x}) = O((g \circ T)(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = O(g(\mathbf{x}))$ then $\Re(f(\mathbf{x})) = O(g(\mathbf{x}))$ and $\Im(f(\mathbf{x})) = O(g(\mathbf{x}))$.
          \end{enumerate}
      \end{proposition}
      \begin{proof}
        Statements (i)-(iii) and (vi) follow immediately from the definition of the $O$-estimate. Statement (iv) follows from the definition and the triangle inequality. Statement (v) follows in the same way as (iv) given that both sums converge. Statement (vii) follows from the definition the $O$-estimate and the bounds $|x| \le |z|$ and $|y| \le |z|$ for any complex $z$.
      \end{proof}

      The most common application of \cref{prop:Big_Oh_manipulations} (vi) will be in the single variable case when $z \asymp w$ or $|z| \sim |w|$ (the latter case implying the former) where $w$ is a function of $z$ (usually one that is more simple than $x$ itself). Taking $h(z) = w$, \cref{prop:Big_Oh_manipulations} (vi) says that if $f(z) = O(g(z))$ then $f(w) = O(g(w))$. In terms of Vinogradov's symbol, $f(z) \ll g(z)$ implies $f(w) \ll g(w)$. $O$-estimates also behave well with respect to integrals provided the functions are continuous and we are integrating over a compact region:

      \begin{proposition}
        Suppose $f(\mathbf{x}) = O(g(\mathbf{x}))$ as $\mathbf{x} \to \infty$ and $f(\mathbf{x})$ and $g(\mathbf{x})$ are continuous on a compact region $D$ where this estimate holds. Then
        \[
          \int_{D}f(\mathbf{x})\,d\mathbf{x} = O\left(\int_{D}|g(\mathbf{x})|\,d\mathbf{x}\right).
        \]
      \end{proposition}
      \begin{proof}
        This follows immediately from the definition of the $O$-estimate and that continuous functions are bounded on compact regions.
      \end{proof}

      The next proposition is a collection of some useful expressions for simplifying equations involving $O$-estimates:

      \begin{proposition}
        Let $f(\mathbf{x})$ be a function such that $f(\mathbf{x}) \to 0$ as $\mathbf{x} \to \mathbf{x}_{0}$. The following $O$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
        \begin{enumerate}[label*=(\roman*)]
          \item $\frac{1}{1+O(f(\mathbf{x}))} = 1+O(f(\mathbf{x}))$.
          \item $(1+O(f(\mathbf{x})))^{w} = 1+O(f(\mathbf{x}))$ for any complex $w$.
          \item $\log(1+O(f(\mathbf{x}))) = O(f(\mathbf{x}))$.
          \item $e^{O(f(\mathbf{x}))} = 1+O(f(\mathbf{x}))$.
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        Taking the Taylor series truncated after the first term and applying Taylor's theorem gives the following $O$-estimates as $z \to 0$:
        \begin{enumerate}[label*=(\roman*)]
          \item $\frac{1}{1+z} = 1+O(z)$.
          \item $(1+z)^{z} = 1+O(z)$.
          \item $\log(1+z) = O(z)$.
          \item $e^{z} = 1+O(z)$.
        \end{enumerate}
        Now apply \cref{prop:Big_Oh_manipulations} (v) to each of these $O$-estimates, and use \cref{prop:Big_Oh_manipulations} (i).
      \end{proof}

      For $o$-estimates, the following properties are useful:

      \begin{proposition}\label{prop:Little_Oh_manipulations}
          The following $o$-estimates hold as $\mathbf{x} \to \mathbf{x}_{0}$:
          \begin{enumerate}[label*=(\roman*)]
            \item If $f(\mathbf{x}) = o(g(\mathbf{x}))$ and $g(\mathbf{x}) = o(h(\mathbf{x}))$ then $f(\mathbf{x}) = o(h(\mathbf{x}))$. Equivalently, $o(o(h(\mathbf{x}))) = o(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = o(g_{i}(\mathbf{x}))$ for $i = 1,2$ then $f_{1}(\mathbf{x})f_{2}(\mathbf{x}) = o(g_{1}(\mathbf{x})g_{2}(\mathbf{x}))$.
            \item If $f(\mathbf{x}) = o(g(\mathbf{x})h(\mathbf{x}))$ then $f(\mathbf{x}) = g(\mathbf{x})o(h(\mathbf{x}))$.
            \item If $f_{i}(\mathbf{x}) = o(g_{i}(\mathbf{x}))$ for $i = 1,2,\ldots,n$ then $\sum_{1 \le i \le n}f_{i}(\mathbf{x}) = o\left(\sum_{1 \le i \le n}|g_{i}(\mathbf{x})|\right)$.
            \item If $f(\mathbf{x}) = o(g(\mathbf{x}))$ as $\mathbf{x} \to \mathbf{x}_{0}$ and $h(\mathbf{x})$ is such that $h(\mathbf{x}) \to \mathbf{x}_{0}$ as $\mathbf{x} \to \mathbf{x}_{0}$ then $(f \circ h)(\mathbf{x}) = o((g \circ h)(\mathbf{x}))$.
          \end{enumerate}
      \end{proposition}
      \begin{proof}
        Statements (i)-(iii) and (v) follow immediately from the definition of the $o$-estimate. Statement (iv) follows from the definition and that $\sum_{1 \le i \le n}|g_{i}(\mathbf{x})| \ge |g_{i}(\mathbf{x})|$.
      \end{proof}
    \subsection*{Growth and Decay of Functions}
      We will also be interested in the growth rate of functions. There are many types of growth rates, but we will only recall the ones that are standard. Throughout let $c \ge 1$. First suppose $\mathbf{x} \to \mathbf{x}_{0}$. If $f(\mathbf{x}) \asymp \log^{c}||\mathbf{x}||_{\infty}$, we say that $f(\mathbf{x})$ is of \textbf{logarithmic growth}\index{logarithmic growth}. If $f(\mathbf{x}) \asymp ||\mathbf{x}||_{\infty}^{c}$, we say that $f(\mathbf{x})$ is of \textbf{polynomial growth}\index{polynomial growth}. If $f(\mathbf{x}) \asymp e^{c||\mathbf{x}||_{\infty}}$, we say that $f(\mathbf{x})$ is of \textbf{exponential growth}\index{exponential growth}. Now suppose $\mathbf{x} \to \infty$. If $f(\mathbf{x}) \asymp \log^{-c}||\mathbf{x}||_{\infty}$, we say that $f(\mathbf{x})$ is of \textbf{logarithmic decay}\index{logarithmic decay}. If $f(\mathbf{x}) \asymp ||\mathbf{x}||_{\infty}^{-c}$ for some $c \ge 1$, we say that $f(\mathbf{x})$ is of \textbf{polynomial decay}\index{polynomial decay}. If $f(\mathbf{x}) \asymp e^{-c||\mathbf{x}||_{\infty}}$, we say that $f(\mathbf{x})$ is of \textbf{exponential decay}\index{exponential decay}. In all of these cases, we refer to the constant $c$ as the \textbf{order}\index{order} of growth or decay respectively. If $f(\mathbf{x}) = \W(||\mathbf{x}||_{\infty}^{n})$ for all $n \ge 0$ then we say $f(\mathbf{x})$ is of \textbf{rapid growth}\index{rapid growth}. Alternatively, if $f(\mathbf{x}) = o(||\mathbf{x}||_{\infty}^{-n})$ for all $n \ge 0$ then we say $f(\mathbf{x})$ is of \textbf{rapid decay}\index{rapid decay}.