\chapter{The Theory of Dirichlet Series}
  We start our discussion of number theory with Dirichlet series. Dirichlet series are essential tools because they are a way of analytically encoding arithmetic information. We discuss basic properties of Dirichlet series such as their abscissas of convergence and Euler products. Then we prove several variants of Perron's formula which connects coefficient sums to Dirichlet series via the inverse Mellin transform. Finally, we lightly introduce multiple Dirichlet series.
  \section{Abscissas of Convergence}
    A \textbf{Dirichlet series}\index{Dirichlet series} $D(s)$ is a sum of the form
    \[
      D(s) = \sum_{n \ge 1}\frac{a(n)}{n^{s}},
    \]
    with $a(n) \in \C$. We exclude the case $a(n) = 0$ for all $a(n)$ so that $D(s)$ is not identically zero. We would first like to understand where this series converges. It does not take much for $D(s)$ to converge uniformly in a sector:

    \begin{theorem}\label{thm:convergence_of_Dirichlet_series}
      Suppose $D(s)$ is a Dirichlet series that converges at $s_{0} = \s_{0}+it_{0}$. Then for any $H > 0$, $D(s)$ converges uniformly in the sector
      \[
        \{s \in \C:\text{$\s \ge \s_{0}$ and $|t-t_{0}| \le H(\s-\s_{0})$}\}.
      \]
    \end{theorem}
    \begin{proof}
      Set $R(u) = \sum_{n \ge u}\frac{a(n)}{n^{s_{0}}}$ so that $\frac{a(n)}{n^{s_{0}}} = (R(n)-R(n+1))$. Then for positive integers $N$ and $M$ with $1 \le M \le N$, classical summation by parts implies
      \begin{equation}\label{equ:convergence_of_Dirichlet_series_1}
        \sum_{M \le n \le N}\frac{a(n)}{n^{s}} = R(M)M^{s_{0}-s}-R(N+1)(N+1)^{s_{0}-s}-\sum_{M \le n \le N}R(n+1)(n^{s_{0}-s}-(n+1)^{s_{0}-s}).
      \end{equation}
      We will now express the sum on the right-hand side of \cref{equ:convergence_of_Dirichlet_series_1} as an integral. To do this, observe that
      \[
        n^{s_{0}-s}-(n+1)^{s_{0}-s} = -(s_{0}-s)\int_{n}^{n+1}u^{s_{0}-s-1}\,du.
      \]
      Therefore
      \begin{equation}\label{equ:convergence_of_Dirichlet_series_2}
        \begin{aligned}
          \sum_{M \le n \le N}R(n+1)(n^{s_{0}-s}-(n+1)^{s_{0}-s}) &= -(s_{0}-s)\sum_{M+1 \le n \le N}R(n+1)\int_{n}^{n+1}u^{s_{0}-s-1}\,du \\
          &= -(s_{0}-s)\sum_{M \le n \le N}\int_{n}^{n+1}R(u)u^{s_{0}-s-1}\,du \\
          &= -(s_{0}-s)\int_{M}^{N+1}R(u)u^{s_{0}-s-1}\,du,
        \end{aligned}
      \end{equation}
      where the second line follows because $R(u)$ is constant on the interval $(n,n+1]$. Combining \cref{equ:convergence_of_Dirichlet_series_1,equ:convergence_of_Dirichlet_series_2} gives
      \begin{equation}\label{equ:convergence_of_Dirichlet_series_3}
        \sum_{M \le n \le N}\frac{a(n)}{n^{s}} = R(M)M^{s_{0}-s}-R(N+1)(N+1)^{s_{0}-s}+(s_{0}-s)\int_{M}^{N+1}R(u)u^{s_{0}-s-1}\,du.
      \end{equation}
      We can choose $M$ such that $|R(u)| < \e$ for all $u \ge M$ because $D(s)$ converges at $s_{0}$. It follows that $|R(u)u^{s_{0}-s}| < \e$ for all $u \ge M$ because $\s \ge \s_{0}$. Moreover, for $s$ in the prescribed sector, we have
      \[
        |s-s_{0}| \le (\s-\s_{0})+|t-t_{0}| \le (H+1)(\s-\s_{0}).
      \]
      These estimates and \cref{equ:convergence_of_Dirichlet_series_3} together imply
      \[
        \left|\sum_{M \le n \le N}\frac{a(n)}{n^{s}}\right| \le 2\e+\e|s-s_{0}|\int_{M}^{N+1}u^{\s_{0}-\s-1}\,du \le 2\e+\e(H+1)(\s-\s_{0})\int_{M}^{N+1}u^{\s_{0}-\s-1}\,du.
      \]
      Since the last integral is finite, $\sum_{M \le n \le N}\frac{a(n)}{n^{s}}$ can be made uniformly arbitrarily small for $s$ in the desired sector. The claim follows by uniform Cauchy's criterion.
    \end{proof}
    
    By taking $H \to \infty$ in \cref{thm:convergence_of_Dirichlet_series} we see that $D(s)$ converges in the region $\s > \s_{0}$. Let $\s_{c}$ be the infimum of all $\s$ for which $D(s)$ converges. We call $\s_{c}$ the \textbf{abscissa of convergence}\index{abscissa of convergence} of $D(s)$. Similarly, let $\s_{a}$ be the infimum of all $\s$ for which $D(s)$ converges absolutely. Since the terms of $D(s)$ are holomorphic, the convergence is locally absolutely uniform (actually uniform in sectors) for $\s > \s_{a}$. It follows that $D(s)$ is holomorphic in the region $\s > \s_{a}$.  We call $\s_{a}$ the \textbf{abscissa of absolute convergence}\index{abscissa of absolute convergence} of $D(s)$. One should think of $\s_{c}$ and $\s_{a}$ as the boundaries of convergence and absolute convergence respectively. Of course, anything can happen at $\s = \s_{c}$ and $\s = \s_{a}$, but to the right of these lines we have convergence and absolute convergence of $D(s)$ respectively. It turns out that $\s_{a}$ is never far from $\s_{c}$ provided $\s_{c}$ is finite:

    \begin{theorem}\label{thm:abscissa_of_absolute_convergence_bound}
      If $D(s)$ is a Dirichlet series with finite abscissa of convergence $\s_{c}$ then
      \[
        \s_{c} \le \s_{a} \le \s_{c}+1.
      \]
    \end{theorem}
    \begin{proof}
      The first inequality is trivial since absolute convergence implies convergence. For the second inequality, since $D(s)$ converges at $\s_{c}+\e$, the terms $a(n)n^{-(\s_{c}+\e)}$ tend to zero as $n \to \infty$. Therefore $a(n) \ll_{\e} n^{\s_{c}+\e}$ where the implicit constant is independent of $n$. But then $a(n)n^{-(\s_{c}+\e)} \ll_{\e} 1$ which implies $\sum_{n \ge 1}a(n)n^{-(\s_{c}+1+2\e)}$ is absolutely convergent by the comparison test with respect to $\sum_{n \ge 1}n^{-(1+\e)}$. In terms of $D(s)$, this means $\s_{a} \le \s_{c}+1+2\e$ and taking $\e \to 0$ gives the second inequality.
    \end{proof}

    It turns out that the coefficients of Dirichlet series are uniquely determined:

    \begin{proposition}\label{prop:coefficients_of_Dirichlet_series_are_unique}
      Suppose $D(s)$ is a Dirichlet series with finite abscissa of convergence $\s_{c}$ such that
      \[
        D(s) = \sum_{n \ge 1}\frac{a(n)}{n^{s}} \quad \text{and} \quad D(s) = \sum_{n \ge 1}\frac{b(n)}{n^{s}}.
      \]
      Then $a(n) = b(n)$ for all $n \ge 1$.
    \end{proposition}
    \begin{proof}
      If $\s \ge \s_{c}+1$ then $D(s)$ converges absolutely by \cref{thm:abscissa_of_absolute_convergence_bound}. Letting $\s \to \infty$, the dominated convergence theorem implies that
      \[
        \lim_{\s \to \infty}D(\s) = a(1) \quad \text{and} \quad \lim_{\s \to \infty}D(\s) = b(1),
      \]
      because every term other than the first tends to zero as $\s \to \infty$. Therefore $a(1) = b(1)$. Now assume by induction that the claim holds for $n \le N$ and set $c(n) = a(n)-b(n)$ for all $n \ge 1$. Then by assumption and our induction hypothesis, we have
      \[
        \sum_{n > N}\frac{c(n)}{n^{s}} = 0.
      \]
      Moreover, the sum on the left-hand side has abscissa of convergence at most $\s_{c}$ since $D(s)$ does. It follows that
      \[
        c(N+1) = -\sum_{n > N}c(n)\left(\frac{N}{n}\right)^{\s},
      \]
      where the sum on the right-hand side has abscissa of convergence $\s_{c}$ as well. By \cref{thm:abscissa_of_absolute_convergence_bound} again, this sum converges absolutely for $\s \ge \s_{c}+1$. Letting $\s \to \infty$, the dominated convergence theorem implies that
      \[
        \lim_{\s \to \infty}\left(-\sum_{n > N}c(n)\left(\frac{N}{n}\right)^{\s}\right) = 0,
      \]
      because every term tends to zero as $\s \to \infty$ since $n > N$. Hence $c(N+1) = 0$ as well which completes the proof.
    \end{proof}

    We will now introduce several convergence theorems for Dirichlet series. It will be useful to setup some notation first. If $D(s)$ is a Dirichlet series with coefficients $a(n)$ then for $X > 0$, we set
    \[
      A(X) = \sum_{n \le X}a(n) \quad \text{and} \quad |A|(X) = \sum_{n \le X}|a(n)|.
    \]
    These are the partial sums of the coefficients $a(n)$ and $|a(n)|$ up to $X$ respectively. Our first convergence theorem relates boundedness of $A(X)$ to the value of $\s_{c}$: 
    
    \begin{proposition}\label{prop:Dirichlet_series_convergence_bounded_coefficient_sum}
      Suppose $D(s)$ is a Dirichlet series and that $A(X) \ll 1$. Then $\s_{c} \le 0$.
    \end{proposition}
    \begin{proof}
      Let $s$ be such that $\s > 0$. Since $A(X) \ll 1$, $A(X)X^{-s} \to 0$ as $X \to \infty$. \cref{cor:Abels_summation_formula_limit_version_specialization} then implies
      \[
        D(s) = s\int_{1}^{\infty}A(u)u^{-(s+1)}\,du.
      \]
      But because $A(u) \ll 1$, we have
      \[
        s\int_{1}^{\infty}A(u)u^{-(s+1)}\,du \ll s\int_{1}^{\infty}u^{-(\s+1)}\,du = \frac{s}{\s},
      \]
      and so the integral converges for $\s > 0$. Thus $D(s)$ converges for $\s > 0$ and therefore $\s_{c} \le 0$.
    \end{proof}

    Our next theorem states that if the coefficients of $D(s)$ are of polynomial growth, we can obtain an upper bound for the abscissa of absolute convergence:

    \begin{proposition}\label{prop:Dirichlet_series_convergence_polynomial_bound}
      Suppose $D(s)$ is a Dirichlet series whose coefficients satisfy $a(n) \ll_{\a} n^{\a}$ for some real $\a$. Then the abscissa of absolute convergence satisfies $\s_{a} \le 1+\a$.
    \end{proposition}
    \begin{proof}
      It suffices to show that $D(s)$ is absolutely convergent in the region $\s > 1+\a$. For $s$ is in this region, the polynomial bound gives
      \[
        D(s) \ll \sum_{n \ge 1}\left|\frac{a(n)}{n^{s}}\right| \ll_{\a} \sum_{n \ge 1}\frac{1}{n^{\s-\a}}.
      \]
      The latter series converges because $\s-\a > 1$. Therefore $D(s)$ is absolutely convergent.
    \end{proof}

    Obtaining polynomial bounds on coefficients of Dirichlet series are, in most cases, not hard to establish. So the assumption in \cref{prop:Dirichlet_series_convergence_polynomial_bound} is mild. Actually, there is a partial converse to \cref{prop:Dirichlet_series_convergence_polynomial_bound} which gives an approximate size to $A(X)$:

    \begin{proposition}\label{prop:Dirichlet_series_coefficient_size_on_average}
      Suppose $D(s)$ is a Dirichlet series with finite and nonnegative abscissa of absolute convergence $\s_{a}$. Then
      \[
        A(X) \ll_{\e} X^{\s_{a}+\e}.
      \]
    \end{proposition}
    \begin{proof}
      \cref{cor:Abels_summation_formula_zero_version} yields
      \begin{equation}\label{prop:Dirichlet_series_coefficient_size_on_average_1}
        \sum_{n \le X}\frac{a(n)}{n^{\s_{a}+\e}} = A(X)X^{-(\s_{a}+\e)}+(\s_{a}+\e)\int_{1}^{X}A(u)u^{-(\s_{a}+\e+1)}\,du.
      \end{equation}
      If we set $R(u) = \sum_{n \ge u}\frac{a(n)}{n^{\s_{a}+\e}}$ then $a(n) = (R(n)-R(n+1))n^{\s_{a}+\e}$ and it follows that
      \[
        A(u) = \sum_{n \le u}(R(n)-R(n+1))n^{\s_{a}+\e}.
      \]
      Substituting this into \cref{prop:Dirichlet_series_coefficient_size_on_average_1}, we obtain
      \[
        \int_{1}^{X}\sum_{n \le u}(R(n)-R(n+1))n^{\s_{a}+\e}u^{-(\s_{a}+\e+1)}\,du.
      \]
      As $R(n)$ is constant on the interval $[n,n+1)$, we have
      \[
        \int_{1}^{X}\sum_{n \le u}(R(n)-R(n+1))n^{\s_{a}+\e}u^{-(\s_{a}+\e+1)}\,du = \sum_{1 \le n \le X}(R(n)-R(n+1))n^{\s_{a}+\e}\int_{n}^{n+1}u^{-(\s_{a}+\e+1)}+O_{\e}(1),
      \]
      where the $O$-estimate is present as $X$ may not be an integer. Now $R(n) \ll_{\e} 1$ since it is the tail of $D(\s_{a}+\e)$ and moreover,
      \[
        \int_{n}^{n+1}u^{-(\s_{a}+\e+1)} = \frac{n^{-(\s_{a}+\e)}}{\s_{a}+\e}-\frac{(n+1)^{-(\s_{a}+\e)}}{\s_{a}+\e} \ll_{\e} 1,
      \]
      because $\s_{a}+\e > 0$. So
      \[
        \int_{1}^{X}A(u)u^{-(\s_{a}+\e+1)}\,du = \int_{1}^{X}\sum_{n \le u}(R(n)-R(n+1))n^{\s_{a}+\e}u^{-(\s_{a}+\e+1)}\,du \ll_{\e} 1.
      \]
      Also, $\sum_{n \le X}\frac{a(n)}{n^{\s_{a}+\e}} \ll_{\e} 1$ because $D(\s_{a}+\e)$ converges. We conclude
      \[
        A(X)X^{-(\s_{a}+\e)} = \sum_{n \le X}\frac{a(n)}{n^{\s_{a}+\e}}-(\s_{a}+\e)\int_{1}^{X}A(u)u^{-(\s_{a}+\e+1)}\,du. \ll_{\e} 1,
      \]
      which is equivalent to the desired estimate.
    \end{proof}

    A way to think about \cref{prop:Dirichlet_series_coefficient_size_on_average} is that if the abscissa of absolute convergence is $\s_{a} \ge 0$ then the size of the coefficients $a(n)$ is at most $n^{\s_{a}+\e}$ on average. Of corse, if $a(n) \ll_{\a} n^{\a}$ then \cref{prop:Dirichlet_series_convergence_polynomial_bound} implies that $\s_{a} \le 1+\a$ and so \cref{prop:Dirichlet_series_coefficient_size_on_average} gives the significantly weaker estimate $A(X) \ll_{\e} X^{1+\a+\e}$. However, if we have a bound of the form $|A|(X) \ll_{\a} X^{\a}$ we can still obtain an upper estimate for the abscissa of absolute convergence:

    \begin{proposition}\label{prop:Dirichlet_series_convergence_polynomial_bound_average}
      Suppose $D(s)$ is a Dirichlet series such that $|A|(X) \ll_{\a} X^{\a}$ for some $\a \ge 0$. Then the abscissa of absolute convergence satisfies $\s_{a} \le \a$.
    \end{proposition}
    \begin{proof}
      It suffices to show that $D(s)$ is absolutely convergent in the region $\s > \a$. Let $s$ be in this region. Then
      \[
        D(s) \ll \sum_{n \ge 1}\left|\frac{a(n)}{n^{s}}\right| = \sum_{n \ge 1}\frac{|a(n)|}{n^{\s}}.
      \]
      \cref{cor:Abels_summation_formula_zero_version} implies
      \[
        \sum_{n \le N}\frac{|a(n)|}{n^{\s}} = |a(N)|N^{-\s}+\s\int_{1}^{N}|A|(u)u^{-(\s+1)}\,du,
      \]
      By assumption $|A|(u) \ll_{\a} u^{\a}$ and so $a(N) \ll_{\a} N^{\a}$. Thus
      \[
        \sum_{n \le N}\frac{|a(n)|}{n^{\s}} \ll_{\a} |a(N)|N^{-\s}+\s\int_{1}^{N}u^{\a-(\s+1)}\,du.
      \]
      As $N \to \infty$, the left-hand side tends towards $\sum_{n \ge 1}\frac{|a(n)|}{n^{\s}}$. As for the right-hand side, the first term tends to zero since $\s > \a$. For the integral, we compute
      \[
        \int_{1}^{N}u^{\a-(\s+1)}\,du = \frac{N^{\a-\s}}{\a-\s}-\frac{1}{\a-\s},
      \]
      which is also bounded as $N \to \infty$. This finishes the proof.
    \end{proof}

    In general, \cref{prop:Dirichlet_series_convergence_polynomial_bound_average} is weaker than \cref{prop:Dirichlet_series_convergence_polynomial_bound}. For example, from our comments following \cref{prop:Dirichlet_series_coefficient_size_on_average}, if $D(s)$ is a Dirichlet series with coefficients $a(n)$ and we have the estimate $|A|(X) \ll_{\b} X^{\b}$ for some real $\b$ then \cref{prop:Dirichlet_series_convergence_polynomial_bound_average} only says $\s_{a} \le \b$. This is a significantly worse upper bound for the abscissa of absolute convergence than what \cref{prop:Dirichlet_series_convergence_polynomial_bound} would imply if $a(n) \ll_{\a} n^{\a}$ and $\a$ is very small compared to $\b$. Actually, the question of sharp polynomial bounds for these coefficients can be very deep. However, if the coefficients $a(n)$ are always nonnegative then \textbf{Landau's theorem}\index{Landau's theorem} provides a way of obtaining a lower bound for their growth as well as locating a singularity of $D(s)$:

    \begin{theorem*}[Landau's theorem]
      Suppose $D(s)$ is a Dirichlet series with nonnegative coefficients $a(n)$ and finite abscissa of absolute convergence $\s_{a}$. Then $\s_{a}$ is a singularity of $D(s)$.
    \end{theorem*}
    \begin{proof}
      If we replace $a(n)$ by $a(n)n^{-\s_{a}}$ then we may assume $\s_{a} = 0$. Now suppose to the contrary that $D(s)$ was holomorphic at $s = 0$. Therefore for some $\d > 0$, $D(s)$ is holomorphic in the domain
      \[
        \mc{D} = \{s \in \C:\s_{a} > 0\} \cup \{s \in \C:|s| < \d\}.
      \]
      Let $P(s)$ be the power series expansion of $D(s)$ at $s = 1$ so that
      \[
        P(s) = \sum_{k \ge 0}c_{k}(s-1)^{k},
      \]
      where
      \[
        c_{k} = \frac{D^{(k)}(1)}{k!} = \frac{1}{k!}\sum_{n \ge 1}\frac{a(n)(-\log(n))^{k}}{n},
      \]
      because $D(s)$ is holomorphic and so we can differentiate termwise. The radius of convergence of $P(s)$ is the distance from $s = 1$ to the nearest singularity of $P(s)$. Since $P(s)$ is holomorphic on $\mc{D}$, the closest points are $\pm i\d$. Therefore, the radius of convergence is at least $|1\pm\d| = \sqrt{1+\d^{2}}$. We can write $\sqrt{1+\d^{2}} = 1+\d'$ for some $\d' > 0$. Then for $|s-1| < 1+\d'$, write $P(s)$ as
      \[
        P(s) = \sum_{k \ge 0}\frac{(s-1)^{k}}{k!}\sum_{n \ge 1}\frac{a(n)(-\log(n))^{k}}{n} = \sum_{k \ge 0}\frac{(1-s)^{k}}{k!}\sum_{n \ge 1}\frac{a(n)(\log(n))^{k}}{n}.
      \]
      If $s$ is real with $s < 1$ then this last double sum is a sum of positive terms because $a(n) \ge 0$. Moreover, since $P(s)$ is absolutely convergent the two sums can be interchanged by the Fubini–Tonelli theorem. Interchanging sums we see that
      \[
        P(s) = \sum_{n \ge 1}\frac{a(n)}{n}\sum_{k \ge 0}\frac{(1-s)^{k}(\log(n))^{k}}{k!} = \sum_{n \ge 1}\frac{a(n)}{n}e^{(1-s)\log(n)} = \sum_{n \ge 1}\frac{a(n)}{n^{s}} = D(s),
      \]
      for $-\d' < s < 1$. As $\d' > 0$, this implies that $D(s)$ converges absolutely (since $a(n)$ is nonnegative) for some $s < 0$ (say $s = -\frac{\d'}{2}$) which contradicts $\s_{a} = 0$.
    \end{proof}
  \section{Euler Products and Dirichlet Convolutions}
    Generally speaking, if the coefficients $a(n)$ are chosen at random, $D(s)$ will not possess any good properties outside of convergence in some region (it might not even possess that). However, many Dirichlet series of interest will have coefficients that exhibit polynomial growth and are multiplicative. These Dirichlet series admits infinite product expressions:

    \begin{proposition}\label{prop:Dirichlet_series_Euler_product}
        Suppose the coefficients $a(n)$ of a Dirichlet series $D(s)$ are multiplicative and satisfy $a(n) \ll_{\a} n^{\a}$ for some $\a \ge 0$. Then $D(s)$ converges absolutely for $\s > 1+\a$ and admits the infinite product expression
        \[
          D(s) = \prod_{p}\left(\sum_{k \ge 0}\frac{a(p^{k})}{p^{ks}}\right),
        \]
        in this region. Conversely, suppose that there are coefficients $a(n)$ such that
        \[
          \prod_{p}\left(\sum_{k \ge 0}\left|\frac{a(p^{k})}{p^{ks}}\right|\right),
        \]
        converges for $\s > 1+\a$. Then the Dirichlet series $D(s)$ converges absolutely in this region and satisfies the former infinite product expression. Moreover, if the coefficients $a(n)$ are completely multiplicative in either case then
        \[
          D(s) = \prod_{p}(1-a(p)p^{-s})^{-1},
        \]
        for $\s > 1+\a$.
    \end{proposition}
    \begin{proof}
        Since $a(n) \ll_{\a} n^{\a}$, \cref{prop:Dirichlet_series_convergence_polynomial_bound} implies that $D(s)$ converges absolutely for $\s > 1+\a$. Let $s$ be such that $\s > 1+\a$. Since
        \[
          \sum_{k \ge 0}\left|\frac{a(p^{k})}{p^{ks}}\right| < \sum_{n \ge 1}\left|\frac{a(n)}{n^{s}}\right|,
        \]
        the infinite series on the left converges because the right does by the absolute convergence of $D(s)$. Now let $N \ge 1$. By the fundamental theorem of arithmetic
        \begin{equation}\label{equ:Dirichlet_series_Euler_product_1}
          \prod_{p \le N}\left(\sum_{k \ge 0}\frac{a(p^{k})}{p^{ks}}\right) = \sum_{n \le N}\frac{a(n)}{n^{s}}+\asum_{n > N}\frac{a(n)}{n^{s}},
        \end{equation}
        where the $\ast$ denotes that we are summing over only those additional terms $\frac{a(n)}{n^{s}}$ that appear in the expanded product on the left-hand side with $n > N$. As $N \to \infty$, the first sum on the right-hand side tends to $D(s)$ and the second sum tends to zero because it is part of the tail of $D(s)$ (which tends to zero by convergence). This proves that the product converges, and is equal to $D(s)$. \cref{equ:Dirichlet_series_Euler_product_1} also holds absolutely in the sense that
        \begin{equation}\label{equ:Dirichlet_series_Euler_product_2}
          \prod_{p \le N}\left(\sum_{k \ge 0}\left|\frac{a(p^{k})}{p^{ks}}\right|\right) = \sum_{n \le N}\left|\frac{a(n)}{n^{s}}\right|+\asum_{n > N}\left|\frac{a(n)}{n^{s}}\right|,
        \end{equation}
        since $D(s)$ converges absolutely. For the converse statement, since the product
        \[
          \prod_{p}\left(\sum_{k \ge 0}\left|\frac{a(p^{k})}{p^{ks}}\right|\right),
        \]
        converges for $\s > 1+\a$ each factor is necessarily finite. That is, for each prime $p$ the series $\sum_{k \ge 0}\frac{a(p^{k})}{p^{ks}}$ converges absolutely in this region. Now fix an $N \ge 1$. Then \cref{equ:Dirichlet_series_Euler_product_2} holds. Taking $N \to \infty$ in \cref{equ:Dirichlet_series_Euler_product_2}, the left-hand side converges by assumption. Therefore the right-hand sides does too. But the first sum on the right-hand side tends to
        \[
          \sum_{n \ge 1}\left|\frac{a(n)}{n^{s}}\right|,
        \]
        and the second sum is part of its tail. So the first sum must convergence hence defining an absolutely convergent Dirichlet series in $\s > 1+\a$, and the second sum must tend to zero. Lastly, if the $a(n)$ are completely multiplicative we have
        \[
          \prod_{p}\left(\sum_{k \ge 0}\frac{a(p^{k})}{p^{ks}}\right) = \prod_{p}\left(\sum_{k \ge 0}\left(\frac{a(p)}{p^{s}}\right)^{k}\right) = \prod_{p}(1-a(p)p^{-s})^{-1}.
        \]
    \end{proof}

    Now suppose $D(s)$ is a Dirichlet series that has a product expression
    \[
      D(s) = \prod_{p}\prod_{1 \le i \le d}(1-\a_{i}(p)p^{-s})^{-1},
    \]
    for $\s > \s_{a}$ and some $\a_{i}(p) \in \C$ for all $i$ and all primes $p$. We call this product the \textbf{Euler product}\index{Euler product} of $D(s)$, and it is said to be of \textbf{degree}\index{degree} $d$. In \cref{prop:Dirichlet_series_Euler_product}, complete multiplicativity of the coefficients is enough to guarantee that $D(s)$ has an Euler product of degree $1$, but in general $D(s)$ will admit an Euler product of degree $d > 1$ if the coefficients are merely multiplicative but satisfy additional properties like a recurrence relation. If $D(s)$ has an Euler product then for any $N \ge 1$ we let $D_{(N)}(s)$ denote the Dirichlet series with the factors $p \mid N$ in the Euler product removed. Then
    \[
      D_{(N)}(s) = D(s)\prod_{p \mid N}\prod_{1 \le i \le d}(1-\a_{i}(p)p^{-s}).
    \]
    Dually, for any $N \ge 1$ we let $D_{N}(s)$ denote the Dirichlet series only consisting of the factors $p \mid N$ in the Euler product. That is
    \[
      D_{N}(s) = \prod_{p \mid N}\prod_{1 \le i \le d}(1-\a_{i}(p)p^{-s})^{-1}.
    \]
    With this notation, we have the relationship
    \[
      D(s) = D_{(N)}(s)D_{N}(s).
    \]
    We now turn to discussing how Dirichlet series behave with respect to products. Let $D_{f}(s)$ and $D_{g}(s)$ be the Dirichlet series defined by
    \[
      D_{f}(s) = \sum_{n \ge 1}\frac{f(n)}{n^{s}} \quad \text{and} \quad D_{g}(s) = \sum_{n \ge 1}\frac{g(n)}{n^{s}},
    \]
    for some arithmetic functions $f$ and $g$. We also assume $f(n) \ll_{\a} n^{\a}$ and $g(n) \ll_{\a} n^{\a}$ for some $\a \ge 0$. This ensures $D_{f}(s)$ and $D_{g}(s)$ both converge absolutely for $\s > 1+\a$. As every $n \ge 1$ is of the form $n = d\frac{n}{d}$ for some divisor $d$ of $n$, we see that
    \begin{equation}\label{equ:Dirichlet_convolution_of_Dirichlet_series}
      D_{f}(s)D_{g}(s) = \sum_{n \ge 1}\frac{\sum_{d \mid n}f(d)g\left(\frac{n}{d}\right)}{n^{s}} = \sum_{n \ge 1}\frac{(f \ast g)(n)}{n^{s}} = D_{f \ast g}(s),
    \end{equation}
    for $\s > 1+\a$. In other words, $D_{f}(s)D_{g}(s)$ is again a Dirichlet series whose coefficients are given by $(f \ast g)(n)$. In particular, $D_{f \ast g}(s)$ converges absolutely for $\s > 1+\a$ as well since $D_{f}(s)$ and $D_{g}(s)$ both do. That is, if both $f(n)$ and $g(n)$ are multiplicative. Also, \cref{prop:Dirichlet_convolution_of_multiplicative_functions} shows that $D_{f \ast g}(s)$ has multiplicative coefficients if both $D_{f}(s)$ and $D_{g}(s)$ do. Moreover, from the M\"obius inversion formula we immediately find that
    \[
      D_{g}(s) = D_{f \ast \mathbf{1}}(s),
    \]
    if and only if
    \[
      D_{f}(s) = D_{g \ast \mu}(s).
    \]
  \section{Perron Formulas}
    With the Mellin inversion formula, it is not hard to prove very useful integral representations for the sum of coefficients of a Dirichlet series. Such formulas are desirable because it allows for the examination of a sum of coefficients of a Dirichlet series, a discrete object, by means of a complex integral where analytic techniques are available. This idea is a cornerstone of analytic number theory because it makes the methods of complex analysis readily available to applications in number theory. First, we setup some general notation. If $D(s)$ is a Dirichlet series with coefficients $a(n)$ then for $X > 0$, we set
    \[
      A^{\ast}(X) = \asum_{n \le X}a(n),
    \]
    where the $\ast$ indicates that the last term is multiplied by $\frac{1}{2}$ if $X$ is an integer. We would like to relate $A^{\ast}(X)$ to an integral involving the Dirichlet series $D(s)$ via an inverse Mellin transform. We will prove a few of these types of representations, the first being (classical) \textbf{Perron's formula}\index{Perron's formula} which is a consequence of Abel's summation formula and the Mellin inversion formula applied to Dirichlet series:

    \begin{theorem*}[Perron's formula, classical]
      Let $D(s)$ be a Dirichlet series with coefficient $a(n)$ and finite and nonnegative abscissa of absolute convergence $\s_{a}$. Then for any $c > \s_{a}$,
      \[
        A^{\ast}(X) = \frac{1}{2\pi i}\int_{(c)}D(s)X^{s}\,\frac{ds}{s}.
      \]
    \end{theorem*}
    \begin{proof}
      Let $s$ be such that $\s > \s_{a}$. \cref{cor:Abels_summation_formula_limit_version_specialization} gives
      \[
        \sum_{n \ge 1}\frac{a(n)}{n^{s}} = \lim_{Y \to \infty}A^{\ast}(Y)Y^{-s}+s\int_{1}^{\infty}A^{\ast}(u)u^{-(s+1)}\,du.
      \]
      Now $A^{\ast}(Y) \le A(Y)$ and $A(Y) \ll_{\e} Y^{\s_{a}+\e}$ by \cref{prop:Dirichlet_series_coefficient_size_on_average} so that $A^{\ast}(Y)Y^{-s} \ll_{\e} Y^{\s_{a}+\e-\s}$. Choosing $\e < \s-\s_{a}$, this latter term tends to zero as $Y \to \infty$, which implies that $A^{\ast}(Y)Y^{-s} \to 0$ as $Y \to \infty$. Therefore we can write the equation above as
      \begin{equation}\label{equ:integral_rep_for_Dirichlet_series}
        \frac{D(s)}{s} = \int_{1}^{\infty}A^{\ast}(u)u^{-(s+1)}\,du = \int_{0}^{\infty}A^{\ast}(u)u^{-(s+1)}\,du,
      \end{equation}
      where the second equality follows because $A(u) = 0$ in the interval $[0,1)$. The Mellin inversion formula immediately gives the result.
    \end{proof}

    As a corollary of classical Perron's formula, we obtain a useful integral representation for Dirichlet series:

    \begin{corollary}\label{cor:integral_rep_for_Dirichlet_series}
      Let $D(s)$ be a Dirichlet series with coefficient $a(n)$ and finite and nonnegative abscissa of absolute convergence $\s_{a}$. Then for $\s > \s_{a}$,
      \[
        D(s) = s\int_{1}^{\infty}A^{\ast}(u)u^{-(s+1)}\,du.
      \]
    \end{corollary}
    \begin{proof}
      The identity follows from \cref{equ:integral_rep_for_Dirichlet_series}.
    \end{proof}

    Classical Perron's formula is not always useful for applications because the integral need not be absolutely bounded. In effect, this can make estimating the integral difficult even if we have alternative representations for the Dirichlet series. There are two ways to correct for this defect each of which leads to a different variant of Perron's formula. The first is to truncate the integral while the latter is to introduce a smoothing function. Both variants can be equally useful to classical Perron's formula. The choice of which variant, if any, to use depends on how one is approaching the particular application and how much analytic information is known about the Dirichlet series under investigation. Let us first focus on the truncated variant. To state it, we will need to setup some notation and prove a lemma. For any $c > 0$ and $T > 0$, consider the integrals
    \[
      \d(y) = \frac{1}{2\pi i}\int_{(c)}y^{s}\,\frac{ds}{s} \quad \text{and} \quad I(y,T) = \frac{1}{2\pi i}\int_{c-iT}^{c+iT}y^{s}\,\frac{ds}{s}.
    \]
    Note that $I(y,T)$ is just $d(y)$ truncated at height $T$. The lemma we require gives an approximation for how close $I(y,T)$ is to $\d(y)$ and also computes $\d(y)$ exactly:

    \begin{lemma}\label{lem:delta_truncation_estimate}
      For any $c > 0$, $y > 0$, and $T > 0$, we have
      \[
        \d(y) = \begin{cases} 0 & \text{if $y < 1$}, \\ \frac{1}{2} & \text{if $y = 1$}, \\ 1 & \text{if $y > 1$}. \end{cases}
      \]
      Moreover,
      \[
        I(y,T)-\d(y) = \begin{cases} O\left(y^{c}\min\left(1,\frac{1}{T\log(y)}\right)\right) & \text{if $y \neq 1$}, \\ O\left(\frac{c}{T}\right) & \text{if $y = 1$}. \end{cases}
      \]
    \end{lemma}
    \begin{proof}
      Since
      \[
        \lim_{T \to \infty}I(y,T) = \d(y),
      \]
      it suffices to estimate $I(y,T)$ and then take the limit as $T \to \infty$. First consider the case $y = 1$. We compute
      \begin{align*}
        I(1,T) &= \frac{1}{2\pi i}\int_{c-iT}^{c+iT}\frac{ds}{s} \\
        &= \frac{1}{2\pi i}\int_{-T}^{T}\frac{dt}{c+it} \\
        &= \frac{1}{2\pi}\int_{0}^{T}\left(\frac{1}{c+it}+\frac{1}{c-it}\right)\,dt \\
        &= \frac{1}{\pi}\int_{0}^{T}\frac{c}{c^{2}+t^{2}}\,dt \\
        &= \frac{1}{\pi}\tan^{-1}\left(\frac{T}{c}\right).
      \end{align*}
      Truncating the Laurent series of the inverse tangent after the first term gives $\tan^{-1}(t) = \frac{\pi}{2}+O\left(\frac{1}{t}\right)$. Thus $I(1,T) = \frac{1}{2}+O\left(\frac{c}{T}\right)$ and $\d(y) = \frac{1}{2}$. This proves everything when $y = 1$. Now suppose $y < 1$ and let $d > c$. Let $\eta = \sum_{1 \le i \le 4}\eta_{i}$ be the contour in \cref{fig:discontinuous_integral_contour_1}.

      \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[scale=3]
          \def\xmin{-0.5} \def\xmax{3}
          \def\ymin{-1} \def\ymax{1}
          \draw[thick] (\xmin,0) -- (\xmax,0);
          \draw[thick] (0,\ymin) -- (0,\ymax);

          \draw[->-] (0.25,-0.75) -- (0.25,0.75);
          \draw[->-] (0.25,0.75) -- (2.5,0.75);
          \draw[->-] (2.5,0.75) -- (2.5,-0.75);
          \draw[->-] (2.5,-0.75) -- (0.25,-0.75);

          \node at (0.25,0) [above left] {\tiny{$\eta_{1}$}};
          \node at (1.375,0.75) [above] {\tiny{$\eta_{2}$}};
          \node at (2.5,0) [above right] {\tiny{$\eta_{3}$}};
          \node at (1.375,-0.75) [below] {\tiny{$\eta_{4}$}};

          \node at (0.25,-0.75) [circle,fill,inner sep=1pt]{};
          \node at (0.25,0.75) [circle,fill,inner sep=1pt]{};
          \node at (2.5,0.75) [circle,fill,inner sep=1pt]{};
          \node at (2.5,-0.75) [circle,fill,inner sep=1pt]{};

          \node at (0.25,-0.75) [below] {\tiny{$c-iT$}};
          \node at (0.25,0.75) [above] {\tiny{$c+iT$}};
          \node at (2.5,0.75) [above] {\tiny{$d+iT$}};
          \node at (2.5,-0.75) [below] {\tiny{$d-iT$}};
        \end{tikzpicture}
        \caption{A rectangular contour.}
        \label{fig:discontinuous_integral_contour_1}
      \end{figure}

      Observe that
      \[
        I(y,T) = \frac{1}{2\pi i}\int_{\eta_{1}}y^{s}\,\frac{ds}{s}.
      \]
      As the integrand only has a simple pole at $s = 0$, the residue theorem implies
      \begin{equation}\label{lem:delta_truncation_estimate_1}
        I(y,T) = -\frac{1}{2\pi i}\int_{\eta_{2}}y^{s}\,\frac{ds}{s}-\frac{1}{2\pi i}\int_{\eta_{3}}y^{s}\,\frac{ds}{s}-\frac{1}{2\pi i}\int_{\eta_{4}}y^{s}\,\frac{ds}{s}.
      \end{equation}
      We will estimate each of these integrals. For the integral over $\eta_{2}$, we have
      \begin{equation}\label{lem:delta_truncation_estimate_2}
        \frac{1}{2\pi i}\int_{\eta_{2}}y^{s}\,\frac{ds}{s} = O\left(\int_{c}^{d}\frac{y^{\s}}{T}\,d\s\right) = O\left(\frac{y^{c}}{\log(y)T}\right),
      \end{equation}
      because $y < 1$. For the integral over $\eta_{4}$, we similarly have
      \begin{equation}\label{lem:delta_truncation_estimate_3}
        \frac{1}{2\pi i}\int_{\eta_{4}}y^{s}\,\frac{ds}{s} = O\left(\int_{c}^{d}\frac{y^{\s}}{T}\,d\s\right) = O\left(\frac{y^{c}}{\log(y)T}\right).
      \end{equation}
      For the integral over $\eta_{3}$, observe that
      \begin{equation}\label{lem:delta_truncation_estimate_4}
        \frac{1}{2\pi i}\int_{\eta_{3}}y^{s}\,\frac{ds}{s} = O\left(\int_{-T}^{T}\frac{y^{d}}{1+|t|}\,dt\right) = O\left(y^{d}\log(T)\right).
      \end{equation}
      Plugging \cref{lem:delta_truncation_estimate_2,lem:delta_truncation_estimate_3,lem:delta_truncation_estimate_4} into \cref{lem:delta_truncation_estimate_1} gives
      \[
        I(y,T) = O\left(\frac{y^{c}}{\log(y)T}\right)+O\left(y^{d}\log(T)\right).
      \]
      Since $y < 1$, taking the limit as $d \to \infty$ implies $I(y,T) = O\left(\frac{y^{c}}{\log(y)T}\right)$ and thus $\d(y) = 0$. Now let $\g = \g_{1}+\g_{2}$ be the contour in \cref{fig:discontinuous_integral_contour_2}. 
      
      \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[scale=3]
          \def\xmin{-0.5} \def\xmax{1.5}
          \def\ymin{-1} \def\ymax{1}
          \draw[thick] (\xmin,0) -- (\xmax,0);
          \draw[thick] (0,\ymin) -- (0,\ymax);

          \draw[->-] (0.25,0.75) -- (0.25,-0.75);
          \draw[->-] (0.25,-0.75) arc (270:450:3/4);

          \node at (0.25,0) [above left] {\tiny{$\eta_{1}$}};
          \node at (1,0) [above right] {\tiny{$\eta_{2}$}};

          \node at (0.25,-0.75) [circle,fill,inner sep=1pt]{};
          \node at (0.25,0.75) [circle,fill,inner sep=1pt]{};

          \node at (0.25,-0.75) [below] {\tiny{$c-iT$}};
          \node at (0.25,0.75) [above] {\tiny{$c+iT$}};
        \end{tikzpicture}
        \caption{A semicircular contour.}
        \label{fig:discontinuous_integral_contour_2}
      \end{figure}

     This time
      \[
        I(y,T) = -\frac{1}{2\pi i}\int_{\g_{1}}y^{s}\,\frac{ds}{s}.
      \]
      As the integrand only has a simple pole at $s = 0$, the residue theorem implies
      \[
        I(y,T) = \frac{1}{2\pi i}\int_{\g_{2}}y^{s}\,\frac{ds}{s}.
      \]
      The change of variables $s \mapsto \sqrt{(c^{2}+T^{2})}e^{i\t}$ shows that
      \[
        \frac{1}{2\pi i}\int_{\g_{2}}y^{s}\,\frac{ds}{s} = O\left(\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}y^{c}\,d\theta\right) = O(y^{c}).
      \]
      Hence $I(y,T) = O(y^{c})$. Combining these two cases results in $I(y,T) = O\left(y^{c}\min\left(1,\frac{1}{T\log(y)}\right)\right)$. This proves everything when $y < 1$. Now suppose $y > 1$ and let $d < 0$. Let $\eta = \sum_{1 \le i \le 4}\eta_{i}$ be the contour in \cref{fig:discontinuous_integral_contour_3}.

      \begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=3]
          \def\xmin{-2.5} \def\xmax{1}
          \def\ymin{-1} \def\ymax{1}
          \draw[thick] (\xmin,0) -- (\xmax,0);
          \draw[thick] (0,\ymin) -- (0,\ymax);

          \draw[->-] (0.25,-0.75) -- (0.25,0.75);
          \draw[->-] (0.25,0.75) -- (-2,0.75);
          \draw[->-] (-2,0.75) -- (-2,-0.75);
          \draw[->-] (-2,-0.75) -- (0.25,-0.75);

          \node at (0.25,0) [above right] {\tiny{$\eta_{1}$}};
          \node at (-0.875,0.75) [above] {\tiny{$\eta_{2}$}};
          \node at (-2,0) [above left] {\tiny{$\eta_{3}$}};
          \node at (-0.875,-0.75) [below] {\tiny{$\eta_{4}$}};

          \node at (0.25,-0.75) [circle,fill,inner sep=1pt]{};
          \node at (0.25,0.75) [circle,fill,inner sep=1pt]{};
          \node at (-2,0.75) [circle,fill,inner sep=1pt]{};
          \node at (-2,-0.75) [circle,fill,inner sep=1pt]{};

          \node at (0.25,-0.75) [below] {\tiny{$c-iT$}};
          \node at (0.25,0.75) [above] {\tiny{$c+iT$}};
          \node at (-2,0.75) [above] {\tiny{$-d+iT$}};
          \node at (-2,-0.75) [below] {\tiny{$-d-iT$}};
        \end{tikzpicture}
        \caption{A rectangular contour.}
        \label{fig:discontinuous_integral_contour_3}
      \end{figure}

      As before,
      \[
        I(y,T) = \frac{1}{2\pi i}\int_{\eta_{1}}y^{s}\,\frac{ds}{s}.
      \]
      The integrand has a simple a pole at $s = 0$ and its residue is $1$. So the residue theorem implies
      \begin{equation}\label{lem:delta_truncation_estimate_5}
        I(y,T) = 1-\frac{1}{2\pi i}\int_{\eta_{2}}y^{s}\,\frac{ds}{s}-\frac{1}{2\pi i}\int_{\eta_{3}}y^{s}\,\frac{ds}{s}-\frac{1}{2\pi i}\int_{\eta_{4}}y^{s}\,\frac{ds}{s}.
      \end{equation}
      We will estimate each of these integrals. For the integral over $\eta_{2}$, we have
      \begin{equation}\label{lem:delta_truncation_estimate_6}
        \frac{1}{2\pi i}\int_{\eta_{2}}y^{s}\,\frac{ds}{s} = O\left(\int_{-d}^{c}\frac{y^{\s}}{T}\,d\s\right) = O\left(\frac{y^{c}}{\log(y)T}\right),
      \end{equation}
      because $y > 1$. For the integral over $\eta_{4}$, we similarly have
      \begin{equation}\label{lem:delta_truncation_estimate_7}
        \frac{1}{2\pi i}\int_{\eta_{4}}y^{s}\,\frac{ds}{s} = O\left(\int_{-d}^{c}\frac{y^{\s}}{T}\,d\s\right) = O\left(\frac{y^{c}}{\log(y)T}\right).
      \end{equation}
      For the integral over $\eta_{3}$, observe that
      \begin{equation}\label{lem:delta_truncation_estimate_8}
        \frac{1}{2\pi i}\int_{\eta_{3}}y^{s}\,\frac{ds}{s} = O\left(\int_{-T}^{T}\frac{y^{-d}}{1+|t|}\,dt\right) = O\left(y^{-d}\log(T)\right).
      \end{equation}
      Plugging \cref{lem:delta_truncation_estimate_6,lem:delta_truncation_estimate_7,lem:delta_truncation_estimate_8} into \cref{lem:delta_truncation_estimate_5} gives
      \[
        I(y,T) = 1+O\left(\frac{y^{c}}{\log(y)T}\right)+O\left(y^{-d}\log(T)\right).
      \]
      Since $y > 1$, taking the limit as $d \to \infty$ implies $I(y,T) = 1+O\left(\frac{y^{c}}{\log(y)T}\right)$ and thus $\d(y) = 1$. Now let $\g = \g_{1}+\g_{2}$ be the contour in \cref{fig:discontinuous_integral_contour_4}. 
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=3]
          \def\xmin{-1.25} \def\xmax{0.75}
          \def\ymin{-1} \def\ymax{1}
          \draw[thick] (\xmin,0) -- (\xmax,0);
          \draw[thick] (0,\ymin) -- (0,\ymax);

          \draw[->-] (0.25,-0.75) -- (0.25,0.75);
          \draw[->-] (0.25,0.75) arc (90:270:3/4);

          \node at (0.25,0) [above right] {\tiny{$\eta_{1}$}};
          \node at (-0.25,0) [above left] {\tiny{$\eta_{2}$}};

          \node at (0.25,-0.75) [circle,fill,inner sep=1pt]{};
          \node at (0.25,0.75) [circle,fill,inner sep=1pt]{};

          \node at (0.25,-0.75) [below] {\tiny{$c-iT$}};
          \node at (0.25,0.75) [above] {\tiny{$c+iT$}};
        \end{tikzpicture}
        \caption{A semicircular contour.}
        \label{fig:discontinuous_integral_contour_4}
      \end{figure}

      This time
      \[
        I(y,T) = \frac{1}{2\pi i}\int_{\g_{1}}y^{s}\,\frac{ds}{s}.
      \]
      The integrand has a simple a pole at $s = 0$ and its residue is $1$. So the residue theorem implies
      \[
        I(y,T) = 1-\frac{1}{2\pi i}\int_{\g_{2}}y^{s}\,\frac{ds}{s}.
      \]
      The change of variables $s \mapsto \sqrt{(c^{2}+T^{2})}e^{i\t}$ shows that
      \[
        \frac{1}{2\pi i}\int_{\g_{2}}y^{s}\,\frac{ds}{s} = O\left(\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}y^{c}\,d\theta\right) = O(y^{c}).
      \]
      Hence $I(y,T) = 1+O(y^{c})$. Combining these two cases results in $I(y,T) = 1+O\left(y^{c}\min\left(1,\frac{1}{T\log(y)}\right)\right)$. This proves everything when $y < 1$ and completing the proof.
    \end{proof}

    We can now state and prove (truncated) \textbf{Perron's formula}\index{Perron's formula}:

    \begin{theorem*}[Perron's formula, truncated]
      Let $D(s)$ be a Dirichlet series with coefficient $a(n)$ and finite and nonnegative abscissa of absolute convergence $\s_{a}$. Then for any $c > \s_{a}$ and $T > 0$,
      \[
        A^{\ast}(X) = \frac{1}{2\pi i}\int_{c-iT}^{c+iT}D(s)X^{s}\,\frac{ds}{s}+O\left(X^{c}\sum_{\substack{n \ge 1 \\ n \neq X}}\frac{a(n)}{n^{c}}\min\left(1,\frac{1}{T\log\left(\frac{X}{n}\right)}\right)+\d_{X}a(X)\frac{c}{T}\right),
      \]
      where $\d_{X} = 1,0$ according to if $X$ is an integer or not.
    \end{theorem*}
    \begin{proof}
      By \cref{append:Special_Integrals}, we have
      \[
        A^{\ast}(X) = \sum_{n \ge 1}a(n)\d\left(\frac{X}{n}\right).
      \]
      Using \cref{lem:delta_truncation_estimate}, we may replace $\d\left(\frac{X}{n}\right)$ and obtain
      \[
        A^{\ast}(X) = \sum_{n \ge 1}a(n)\frac{1}{2\pi i}\int_{c-iT}^{c+iT}\frac{X^{s}}{n^{s}}\,\frac{ds}{s}+\sum_{\substack{n \ge 1 \\ n \neq X}}a(n)O\left(\frac{X^{c}}{n^{c}}\min\left(1,\frac{1}{T\log\left(\frac{X}{n}\right)}\right)\right)+\d_{X}a(X)O\left(\frac{c}{T}\right).
      \]
      Since $D(s)$ converges absolutely, the sum may be moved inside of the first $O$-estimate and then we may combine the resulting two $O$-estimates. By the Fubini–Tonelli theorem, we may interchange the sum and the integral and the statement of the lemma follows.
    \end{proof}

    Since the integral in truncated Perron's formula is over a finite vertical line, the integral is automatically absolutely bounded. There is a slightly weaker variant of truncated Perron's formula that follows as a corollary:

    \begin{corollary}
      Let $D(s)$ be a Dirichlet series with coefficient $a(n)$ and finite and nonnegative abscissa of absolute convergence $\s_{a}$. Then for any $c > \s_{a}$ and $T > 0$,
      \[
        A^{\ast}(X) = \frac{1}{2\pi i}\int_{c-iT}^{c+iT}D(s)X^{s}\,\frac{ds}{s}+O\left(\frac{X^{c}}{T}\right),
      \]
    \end{corollary}
    \begin{proof}
      For sufficiently large $X$, we have
      \[
        \min\left(1,\frac{1}{T\log\left(\frac{X}{n}\right)}\right) \ll \frac{X^{c}}{T}.
      \]
      The statement now follows from truncated Perron's formula.
    \end{proof}

    Having discussed truncated Perron's formula, we turn to the variant where we instead introduce a smoothing function. For any $X > 0$, we set
    \[
      A_{\psi}(X) = \sum_{n \ge 1}a(n)\psi\left(\frac{n}{X}\right),
    \]
    where $\psi(y)$ is smooth function such that $\psi(y) \to 0$ as $y \to \infty$. This is most useful in two cases. The first is when we choose $\psi(y)$ to be a bump function. In this setting, the bump function can be chosen such that it assigns weight $1$ or $0$ to the coefficients $a(n)$ and we can estimate sums like
    \[
      \sum_{\frac{X}{2} \le n < X}a(n) \quad \text{or} \quad \sum_{X \le n < X+Y}a(n),
    \]
    for some $X$ and $Y$ with $Y < X$. Sums of this type are called \textbf{unweighted}\index{unweighted}. As an example of an unweighted sum, let $\psi(y)$ be a bump function that is identically $1$ on $[0,1]$ and is supported in $\left[1,\frac{X+1}{X}\right]$. For example,
    \[
      \psi(y) = \begin{cases} 1 & \text{if $0 \le y \le 1$}, \\ e^{\frac{1-y}{\frac{X+1}{X}-y}} & \text{if $1 \le y < \frac{X+1}{X}$}, \\ 0 & \text{if $y \ge \frac{X+1}{X}$}. \end{cases}
    \]
    Then
    \[
      A_{\psi}(X) = \sum_{n \ge 1}a(n)\psi\left(\frac{n}{X}\right) = \sum_{n \le X}a(n).
    \]
    In the second case, we want $\psi(y)$ to dampen the $a(n)$ with a weight other than $1$ or $0$. Sums of this type are called \textbf{weighted}\index{weighted}. In any case, suppose the support of the smooth function $\psi(y)$ is contained in $[0,M]$. These conditions will force the Mellin transform $\Psi(s)$ of $\psi(y)$ to exist and have nice properties. To see that $\Psi(s)$ exists, let $K$ be a compact set in $\C$ and let $\a = \max_{s \in K}(\s)$ and $\b = \min_{s \in K}\{\s\}$. Note that $\psi(y)$ is bounded because it is compactly supported. Then for $s \in K$,
    \[
      \Psi(s) = \int_{0}^{\infty}\psi(y)y^{s}\,\frac{dy}{y} \ll \int_{0}^{M}y^{\s-1}\,dy \ll_{\a,\b} 1.
    \]
    Therefore $\Psi(s)$ is locally absolutely uniformly convergent for $s \in \C$. In particular, the Mellin inversion formula implies that $\psi(y)$ is the Mellin inverse of $\Psi(s)$. As for nice properties, $\Psi(s)$ exhibit polynomial decay of arbitrarily large order in vertical strips:

    \begin{proposition}\label{prop:smoothing_function_Mellin_inverse_vertical_strips}
      Suppose $\psi(y)$ is a bump function and let $\Psi(s)$ denote its Mellin transform. Then for any $N \ge 1$,
      \[
        \Psi(s) \ll (|s|+1)^{-N},
      \]
      provided $s$ is contained in the vertical strip $a < \s < b$ for any $a$ and $b$ with $0 < a < b$.
    \end{proposition}
    \begin{proof}
      Fix $a$ and $b$ with $a < b$. Also, let the support of $\psi(y)$ be contained in $[0,M]$. Now consider
      \[
        \Psi(s) = \int_{0}^{\infty}\psi(y)y^{s}\,\frac{dy}{y}.
      \]
      Since $\psi(y)$ is compactly supported, integrating by parts yields
      \[
        \Psi(s) = \frac{1}{s}\int_{0}^{\infty}\psi'(y)y^{s+1}\,\frac{dy}{y}.
      \]
      Repeatedly integrating by parts $N \ge 1$ times, we arrive at
      \[
        \Psi(s) = \frac{1}{s(s+1) \cdots (s+N-1)}\int_{0}^{\infty}\psi^{(N)}(y)y^{s+N}\,\frac{dy}{y}.
      \]
      Therefore
      \[
        \Psi(s) \ll (|s|+1)^{-N}\int_{0}^{\infty}\psi^{(N)}(y)y^{\s+N}\,\frac{dy}{y}.
      \]
      The claim will follow if we can show that the integral is bounded. Since $\psi(y)$ is compactly supported in $[0,M]$ so is $\psi^{(N)}(y)$. In particular, $\psi^{(N)}(y)$ is bounded. Therefore
      \[
        \int_{0}^{\infty}\psi^{(N)}(y)y^{\s+N}\,\frac{dy}{y} \ll \int_{0}^{M}y^{\s+N}\,\frac{dy}{y} = \frac{M^{\s+N}}{\s+N} \ll \frac{M^{b+N}}{N} \ll 1,
      \]
      where the second to last estimate follows because $a < \s < b$ with $0 < a < b$. So the integral is bounded and the claim follows.
    \end{proof}
    
    The following theorem is (smoothed) \textbf{Perron's formula}\index{Perron's formula}:

    \begin{theorem*}[Perron's formula, smoothed]
      Let $D(s)$ be a Dirichlet series with coefficient $a(n)$ and finite and nonnegative abscissa of absolute convergence $\s_{a}$. Let $\psi(y)$ be a bump function and denote its Mellin transform by $\Psi(s)$. Then for any $c > \s_{a}$,
      \[
        A_{\psi}(X) = \frac{1}{2\pi i}\int_{(c)}D(s)\Psi(s)X^{s}\,ds.
      \]
      In particular,
      \[
        \sum_{n \ge 1}a(n)\psi(n) = \frac{1}{2\pi i}\int_{(c)}D(s)\Psi(s)\,ds.
      \]
    \end{theorem*}
    \begin{proof}
      The first statement is just a computation:
      \begin{align*}
        A_{\psi}(X) &= \sum_{n \ge 1}a(n)\psi\left(\frac{n}{X}\right) \\
        &= \sum_{n \ge 1}\frac{a(n)}{2\pi i}\int_{(c)}\Psi(s)\left(\frac{n}{X}\right)^{-s}\,ds & \text{Mellin inversion formula} \\
        &= \frac{1}{2\pi i}\int_{(c)}\sum_{n \ge 1}a(n)\Psi(s)\left(\frac{n}{X}\right)^{-s}\,ds & \text{FTT} \\
        &= \frac{1}{2\pi i}\int_{(c)}D(s)\Psi(s)X^{s}\,ds. \\
      \end{align*}
      This proves the first statement. For the second statement, take $X = 1$.
    \end{proof}

    In light of \cref{prop:smoothing_function_Mellin_inverse_vertical_strips} and that the Dirichlet series is absolute bounded for $c > \s_{a}$, the integral in smoothed Perron's formula is also absolutely bounded. The compensation for the convergence is that we have introduced a weighting factor to the sum of coefficients of the Dirichlet series.
